1.13.1 

[2023-10-22 19:13:40,675] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-10-22 19:13:41,502] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.147.1, master_port=6000
[2023-10-22 19:13:41,503] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-10-22 19:13:44,792] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 128, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x128x384, b=2048): 0.0001
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x128x384, b=2048): 13.675
Elapsed time for attention_key_query_prob (512x2048x1x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x1x2048): 0.957
Elapsed time for attention_prob_times_values (512x2048x2048x1): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x1): 1.434
Elapsed time for attention_linear_projection (4x128x128, b=2048): 0.0001
Throughput (in TFLOP/s) for attention_linear_projection (4x128x128, b=2048): 3.656
Elapsed time for mlp_h_to_4h (4x128x512, b=2048): 0.0001
Throughput (in TFLOP/s) for mlp_h_to_4h (4x128x512, b=2048): 18.687
Elapsed time for mlp_4h_to_h (4x512x128, b=2048): 0.0001
Throughput (in TFLOP/s) for mlp_4h_to_h (4x512x128, b=2048): 19.666

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 1.269
MLP duration (in seconds): 0.0001
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x256x768, b=2048): 0.0001
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x256x768, b=2048): 49.130
Elapsed time for attention_key_query_prob (512x2048x2x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x2x2048): 1.233
Elapsed time for attention_prob_times_values (512x2048x2048x2): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x2): 2.813
Elapsed time for attention_linear_projection (4x256x256, b=2048): 0.0001
Throughput (in TFLOP/s) for attention_linear_projection (4x256x256, b=2048): 17.524
Elapsed time for mlp_h_to_4h (4x256x1024, b=2048): 0.0001
Throughput (in TFLOP/s) for mlp_h_to_4h (4x256x1024, b=2048): 67.218
Elapsed time for mlp_4h_to_h (4x1024x256, b=2048): 0.0001
Throughput (in TFLOP/s) for mlp_4h_to_h (4x1024x256, b=2048): 50.888

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 2.117
MLP duration (in seconds): 0.0001
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x384x1152, b=2048): 0.0001
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x384x1152, b=2048): 103.399
Elapsed time for attention_key_query_prob (512x2048x3x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x3x2048): 2.811
Elapsed time for attention_prob_times_values (512x2048x2048x3): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x3): 3.181
Elapsed time for attention_linear_projection (4x384x384, b=2048): 0.0001
Throughput (in TFLOP/s) for attention_linear_projection (4x384x384, b=2048): 40.211
Elapsed time for mlp_h_to_4h (4x384x1536, b=2048): 0.0001
Throughput (in TFLOP/s) for mlp_h_to_4h (4x384x1536, b=2048): 108.086
Elapsed time for mlp_4h_to_h (4x1536x384, b=2048): 0.0001
Throughput (in TFLOP/s) for mlp_4h_to_h (4x1536x384, b=2048): 119.565

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 4.043
MLP duration (in seconds): 0.0002
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x512x1536, b=2048): 0.0001
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x512x1536, b=2048): 108.739
Elapsed time for attention_key_query_prob (512x2048x4x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x4x2048): 2.456
Elapsed time for attention_prob_times_values (512x2048x2048x4): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x4): 5.630
Elapsed time for attention_linear_projection (4x512x512, b=2048): 0.0001
Throughput (in TFLOP/s) for attention_linear_projection (4x512x512, b=2048): 68.236
Elapsed time for mlp_h_to_4h (4x512x2048, b=2048): 0.0001
Throughput (in TFLOP/s) for mlp_h_to_4h (4x512x2048, b=2048): 118.907
Elapsed time for mlp_4h_to_h (4x2048x512, b=2048): 0.0001
Throughput (in TFLOP/s) for mlp_4h_to_h (4x2048x512, b=2048): 129.600

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 5.039
MLP duration (in seconds): 0.0003
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x640x1920, b=2048): 0.0001
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x640x1920, b=2048): 147.112
Elapsed time for attention_key_query_prob (512x2048x5x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x5x2048): 4.660
Elapsed time for attention_prob_times_values (512x2048x2048x5): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x5): 5.232
Elapsed time for attention_linear_projection (4x640x640, b=2048): 0.0001
Throughput (in TFLOP/s) for attention_linear_projection (4x640x640, b=2048): 91.388
Elapsed time for mlp_h_to_4h (4x640x2560, b=2048): 0.0002
Throughput (in TFLOP/s) for mlp_h_to_4h (4x640x2560, b=2048): 163.886
Elapsed time for mlp_4h_to_h (4x2560x640, b=2048): 0.0001
Throughput (in TFLOP/s) for mlp_4h_to_h (4x2560x640, b=2048): 188.593

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 7.821
MLP duration (in seconds): 0.0003
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x768x2304, b=2048): 0.0002
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x768x2304, b=2048): 166.571
Elapsed time for attention_key_query_prob (512x2048x6x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x6x2048): 3.685
Elapsed time for attention_prob_times_values (512x2048x2048x6): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x6): 8.355
Elapsed time for attention_linear_projection (4x768x768, b=2048): 0.0001
Throughput (in TFLOP/s) for attention_linear_projection (4x768x768, b=2048): 135.108
Elapsed time for mlp_h_to_4h (4x768x3072, b=2048): 0.0002
Throughput (in TFLOP/s) for mlp_h_to_4h (4x768x3072, b=2048): 180.948
Elapsed time for mlp_4h_to_h (4x3072x768, b=2048): 0.0002
Throughput (in TFLOP/s) for mlp_4h_to_h (4x3072x768, b=2048): 169.592

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 8.737
MLP duration (in seconds): 0.0004
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x896x2688, b=2048): 0.0002
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x896x2688, b=2048): 175.511
Elapsed time for attention_key_query_prob (512x2048x7x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x7x2048): 6.507
Elapsed time for attention_prob_times_values (512x2048x2048x7): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x7): 7.231
Elapsed time for attention_linear_projection (4x896x896, b=2048): 0.0001
Throughput (in TFLOP/s) for attention_linear_projection (4x896x896, b=2048): 108.175
Elapsed time for mlp_h_to_4h (4x896x3584, b=2048): 0.0003
Throughput (in TFLOP/s) for mlp_h_to_4h (4x896x3584, b=2048): 195.809
Elapsed time for mlp_4h_to_h (4x3584x896, b=2048): 0.0003
Throughput (in TFLOP/s) for mlp_4h_to_h (4x3584x896, b=2048): 184.358

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 12.357
MLP duration (in seconds): 0.0006
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x1024x3072, b=2048): 0.0003
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x1024x3072, b=2048): 203.361
Elapsed time for attention_key_query_prob (512x2048x8x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x8x2048): 10.235
Elapsed time for attention_prob_times_values (512x2048x2048x8): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x8): 11.198
Elapsed time for attention_linear_projection (4x1024x1024, b=2048): 0.0001
Throughput (in TFLOP/s) for attention_linear_projection (4x1024x1024, b=2048): 147.357
Elapsed time for mlp_h_to_4h (4x1024x4096, b=2048): 0.0003
Throughput (in TFLOP/s) for mlp_h_to_4h (4x1024x4096, b=2048): 204.855
Elapsed time for mlp_4h_to_h (4x4096x1024, b=2048): 0.0003
Throughput (in TFLOP/s) for mlp_4h_to_h (4x4096x1024, b=2048): 211.313

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 20.225
MLP duration (in seconds): 0.0007
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x1152x3456, b=2048): 0.0003
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x1152x3456, b=2048): 206.486
Elapsed time for attention_key_query_prob (512x2048x9x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x9x2048): 6.318
Elapsed time for attention_prob_times_values (512x2048x2048x9): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x9): 8.904
Elapsed time for attention_linear_projection (4x1152x1152, b=2048): 0.0002
Throughput (in TFLOP/s) for attention_linear_projection (4x1152x1152, b=2048): 132.942
Elapsed time for mlp_h_to_4h (4x1152x4608, b=2048): 0.0004
Throughput (in TFLOP/s) for mlp_h_to_4h (4x1152x4608, b=2048): 214.583
Elapsed time for mlp_4h_to_h (4x4608x1152, b=2048): 0.0005
Throughput (in TFLOP/s) for mlp_4h_to_h (4x4608x1152, b=2048): 192.198

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 15.019
MLP duration (in seconds): 0.0009
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x1280x3840, b=2048): 0.0004
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x1280x3840, b=2048): 209.275
Elapsed time for attention_key_query_prob (512x2048x10x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x10x2048): 6.103
Elapsed time for attention_prob_times_values (512x2048x2048x10): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x10): 13.606
Elapsed time for attention_linear_projection (4x1280x1280, b=2048): 0.0002
Throughput (in TFLOP/s) for attention_linear_projection (4x1280x1280, b=2048): 167.794
Elapsed time for mlp_h_to_4h (4x1280x5120, b=2048): 0.0005
Throughput (in TFLOP/s) for mlp_h_to_4h (4x1280x5120, b=2048): 221.198
Elapsed time for mlp_4h_to_h (4x5120x1280, b=2048): 0.0005
Throughput (in TFLOP/s) for mlp_4h_to_h (4x5120x1280, b=2048): 236.782

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 17.998
MLP duration (in seconds): 0.0009
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x1408x4224, b=2048): 0.0004
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x1408x4224, b=2048): 219.850
Elapsed time for attention_key_query_prob (512x2048x11x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x11x2048): 7.686
Elapsed time for attention_prob_times_values (512x2048x2048x11): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x11): 10.925
Elapsed time for attention_linear_projection (4x1408x1408, b=2048): 0.0002
Throughput (in TFLOP/s) for attention_linear_projection (4x1408x1408, b=2048): 181.645
Elapsed time for mlp_h_to_4h (4x1408x5632, b=2048): 0.0007
Throughput (in TFLOP/s) for mlp_h_to_4h (4x1408x5632, b=2048): 197.369
Elapsed time for mlp_4h_to_h (4x5632x1408, b=2048): 0.0006
Throughput (in TFLOP/s) for mlp_4h_to_h (4x5632x1408, b=2048): 223.517

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 20.229
MLP duration (in seconds): 0.0012
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x1536x4608, b=2048): 0.0006
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x1536x4608, b=2048): 201.487
Elapsed time for attention_key_query_prob (512x2048x12x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x12x2048): 7.370
Elapsed time for attention_prob_times_values (512x2048x2048x12): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x12): 16.066
Elapsed time for attention_linear_projection (4x1536x1536, b=2048): 0.0002
Throughput (in TFLOP/s) for attention_linear_projection (4x1536x1536, b=2048): 166.800
Elapsed time for mlp_h_to_4h (4x1536x6144, b=2048): 0.0008
Throughput (in TFLOP/s) for mlp_h_to_4h (4x1536x6144, b=2048): 203.937
Elapsed time for mlp_4h_to_h (4x6144x1536, b=2048): 0.0007
Throughput (in TFLOP/s) for mlp_4h_to_h (4x6144x1536, b=2048): 223.550

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 23.410
MLP duration (in seconds): 0.0014
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x1664x4992, b=2048): 0.0006
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x1664x4992, b=2048): 227.968
Elapsed time for attention_key_query_prob (512x2048x13x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x13x2048): 8.964
Elapsed time for attention_prob_times_values (512x2048x2048x13): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x13): 12.279
Elapsed time for attention_linear_projection (4x1664x1664, b=2048): 0.0003
Throughput (in TFLOP/s) for attention_linear_projection (4x1664x1664, b=2048): 177.995
Elapsed time for mlp_h_to_4h (4x1664x6656, b=2048): 0.0009
Throughput (in TFLOP/s) for mlp_h_to_4h (4x1664x6656, b=2048): 209.903
Elapsed time for mlp_4h_to_h (4x6656x1664, b=2048): 0.0008
Throughput (in TFLOP/s) for mlp_4h_to_h (4x6656x1664, b=2048): 235.929

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 25.210
MLP duration (in seconds): 0.0016
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x1792x5376, b=2048): 0.0008
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x1792x5376, b=2048): 208.316
Elapsed time for attention_key_query_prob (512x2048x14x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x14x2048): 8.559
Elapsed time for attention_prob_times_values (512x2048x2048x14): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x14): 18.286
Elapsed time for attention_linear_projection (4x1792x1792, b=2048): 0.0003
Throughput (in TFLOP/s) for attention_linear_projection (4x1792x1792, b=2048): 184.976
Elapsed time for mlp_h_to_4h (4x1792x7168, b=2048): 0.0010
Throughput (in TFLOP/s) for mlp_h_to_4h (4x1792x7168, b=2048): 214.301
Elapsed time for mlp_4h_to_h (4x7168x1792, b=2048): 0.0009
Throughput (in TFLOP/s) for mlp_4h_to_h (4x7168x1792, b=2048): 230.712

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 29.123
MLP duration (in seconds): 0.0019
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x1920x5760, b=2048): 0.0009
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x1920x5760, b=2048): 211.165
Elapsed time for attention_key_query_prob (512x2048x15x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x15x2048): 10.444
Elapsed time for attention_prob_times_values (512x2048x2048x15): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x15): 15.160
Elapsed time for attention_linear_projection (4x1920x1920, b=2048): 0.0003
Throughput (in TFLOP/s) for attention_linear_projection (4x1920x1920, b=2048): 186.682
Elapsed time for mlp_h_to_4h (4x1920x7680, b=2048): 0.0011
Throughput (in TFLOP/s) for mlp_h_to_4h (4x1920x7680, b=2048): 216.242
Elapsed time for mlp_4h_to_h (4x7680x1920, b=2048): 0.0010
Throughput (in TFLOP/s) for mlp_4h_to_h (4x7680x1920, b=2048): 233.805

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 31.935
MLP duration (in seconds): 0.0022
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x2048x6144, b=2048): 0.0009
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x2048x6144, b=2048): 219.242
Elapsed time for attention_key_query_prob (512x2048x16x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x16x2048): 20.058
Elapsed time for attention_prob_times_values (512x2048x2048x16): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x16): 21.917
Elapsed time for attention_linear_projection (4x2048x2048, b=2048): 0.0004
Throughput (in TFLOP/s) for attention_linear_projection (4x2048x2048, b=2048): 180.031
Elapsed time for mlp_h_to_4h (4x2048x8192, b=2048): 0.0013
Throughput (in TFLOP/s) for mlp_h_to_4h (4x2048x8192, b=2048): 208.259
Elapsed time for mlp_4h_to_h (4x8192x2048, b=2048): 0.0011
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8192x2048, b=2048): 240.895

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 52.301
MLP duration (in seconds): 0.0025
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x2176x6528, b=2048): 0.0011
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x2176x6528, b=2048): 204.003
Elapsed time for attention_key_query_prob (512x2048x17x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x17x2048): 11.811
Elapsed time for attention_prob_times_values (512x2048x2048x17): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x17): 16.488
Elapsed time for attention_linear_projection (4x2176x2176, b=2048): 0.0004
Throughput (in TFLOP/s) for attention_linear_projection (4x2176x2176, b=2048): 192.878
Elapsed time for mlp_h_to_4h (4x2176x8704, b=2048): 0.0017
Throughput (in TFLOP/s) for mlp_h_to_4h (4x2176x8704, b=2048): 178.489
Elapsed time for mlp_4h_to_h (4x8704x2176, b=2048): 0.0020
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8704x2176, b=2048): 155.594

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 37.549
MLP duration (in seconds): 0.0037
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x2304x6912, b=2048): 0.0012
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x2304x6912, b=2048): 215.810
Elapsed time for attention_key_query_prob (512x2048x18x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x18x2048): 10.940
Elapsed time for attention_prob_times_values (512x2048x2048x18): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x18): 22.978
Elapsed time for attention_linear_projection (4x2304x2304, b=2048): 0.0004
Throughput (in TFLOP/s) for attention_linear_projection (4x2304x2304, b=2048): 200.655
Elapsed time for mlp_h_to_4h (4x2304x9216, b=2048): 0.0016
Throughput (in TFLOP/s) for mlp_h_to_4h (4x2304x9216, b=2048): 219.556
Elapsed time for mlp_4h_to_h (4x9216x2304, b=2048): 0.0015
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9216x2304, b=2048): 237.379

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 41.619
MLP duration (in seconds): 0.0031
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x2432x7296, b=2048): 0.0013
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x2432x7296, b=2048): 218.521
Elapsed time for attention_key_query_prob (512x2048x19x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x19x2048): 13.041
Elapsed time for attention_prob_times_values (512x2048x2048x19): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x19): 17.018
Elapsed time for attention_linear_projection (4x2432x2432, b=2048): 0.0005
Throughput (in TFLOP/s) for attention_linear_projection (4x2432x2432, b=2048): 200.025
Elapsed time for mlp_h_to_4h (4x2432x9728, b=2048): 0.0018
Throughput (in TFLOP/s) for mlp_h_to_4h (4x2432x9728, b=2048): 215.967
Elapsed time for mlp_4h_to_h (4x9728x2432, b=2048): 0.0017
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9728x2432, b=2048): 230.055

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 42.809
MLP duration (in seconds): 0.0035
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x2560x7680, b=2048): 0.0016
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x2560x7680, b=2048): 204.091
Elapsed time for attention_key_query_prob (512x2048x20x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x20x2048): 12.137
Elapsed time for attention_prob_times_values (512x2048x2048x20): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x20): 25.221
Elapsed time for attention_linear_projection (4x2560x2560, b=2048): 0.0005
Throughput (in TFLOP/s) for attention_linear_projection (4x2560x2560, b=2048): 209.178
Elapsed time for mlp_h_to_4h (4x2560x10240, b=2048): 0.0020
Throughput (in TFLOP/s) for mlp_h_to_4h (4x2560x10240, b=2048): 211.288
Elapsed time for mlp_4h_to_h (4x10240x2560, b=2048): 0.0018
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10240x2560, b=2048): 244.495

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 47.816
MLP duration (in seconds): 0.0038
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x2688x8064, b=2048): 0.0017
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x2688x8064, b=2048): 211.887
Elapsed time for attention_key_query_prob (512x2048x21x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x21x2048): 14.404
Elapsed time for attention_prob_times_values (512x2048x2048x21): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x21): 19.271
Elapsed time for attention_linear_projection (4x2688x2688, b=2048): 0.0006
Throughput (in TFLOP/s) for attention_linear_projection (4x2688x2688, b=2048): 212.008
Elapsed time for mlp_h_to_4h (4x2688x10752, b=2048): 0.0022
Throughput (in TFLOP/s) for mlp_h_to_4h (4x2688x10752, b=2048): 220.089
Elapsed time for mlp_4h_to_h (4x10752x2688, b=2048): 0.0019
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10752x2688, b=2048): 246.138

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 49.626
MLP duration (in seconds): 0.0041
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x2816x8448, b=2048): 0.0018
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x2816x8448, b=2048): 219.614
Elapsed time for attention_key_query_prob (512x2048x22x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x22x2048): 13.439
Elapsed time for attention_prob_times_values (512x2048x2048x22): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x22): 28.151
Elapsed time for attention_linear_projection (4x2816x2816, b=2048): 0.0006
Throughput (in TFLOP/s) for attention_linear_projection (4x2816x2816, b=2048): 217.539
Elapsed time for mlp_h_to_4h (4x2816x11264, b=2048): 0.0023
Throughput (in TFLOP/s) for mlp_h_to_4h (4x2816x11264, b=2048): 222.809
Elapsed time for mlp_4h_to_h (4x11264x2816, b=2048): 0.0021
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11264x2816, b=2048): 242.086

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 55.542
MLP duration (in seconds): 0.0045
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x2944x8832, b=2048): 0.0019
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x2944x8832, b=2048): 222.655
Elapsed time for attention_key_query_prob (512x2048x23x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x23x2048): 15.830
Elapsed time for attention_prob_times_values (512x2048x2048x23): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x23): 22.533
Elapsed time for attention_linear_projection (4x2944x2944, b=2048): 0.0007
Throughput (in TFLOP/s) for attention_linear_projection (4x2944x2944, b=2048): 207.382
Elapsed time for mlp_h_to_4h (4x2944x11776, b=2048): 0.0025
Throughput (in TFLOP/s) for mlp_h_to_4h (4x2944x11776, b=2048): 225.286
Elapsed time for mlp_4h_to_h (4x11776x2944, b=2048): 0.0023
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11776x2944, b=2048): 246.320

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 57.900
MLP duration (in seconds): 0.0048
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x3072x9216, b=2048): 0.0021
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x3072x9216, b=2048): 219.391
Elapsed time for attention_key_query_prob (512x2048x24x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x24x2048): 29.821
Elapsed time for attention_prob_times_values (512x2048x2048x24): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x24): 31.767
Elapsed time for attention_linear_projection (4x3072x3072, b=2048): 0.0007
Throughput (in TFLOP/s) for attention_linear_projection (4x3072x3072, b=2048): 210.969
Elapsed time for mlp_h_to_4h (4x3072x12288, b=2048): 0.0027
Throughput (in TFLOP/s) for mlp_h_to_4h (4x3072x12288, b=2048): 225.827
Elapsed time for mlp_4h_to_h (4x12288x3072, b=2048): 0.0026
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12288x3072, b=2048): 237.683

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 86.361
MLP duration (in seconds): 0.0053
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x3200x9600, b=2048): 0.0022
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x3200x9600, b=2048): 224.964
Elapsed time for attention_key_query_prob (512x2048x25x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x25x2048): 17.078
Elapsed time for attention_prob_times_values (512x2048x2048x25): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x25): 23.614
Elapsed time for attention_linear_projection (4x3200x3200, b=2048): 0.0008
Throughput (in TFLOP/s) for attention_linear_projection (4x3200x3200, b=2048): 209.556
Elapsed time for mlp_h_to_4h (4x3200x12800, b=2048): 0.0030
Throughput (in TFLOP/s) for mlp_h_to_4h (4x3200x12800, b=2048): 225.270
Elapsed time for mlp_4h_to_h (4x12800x3200, b=2048): 0.0029
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12800x3200, b=2048): 233.841

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 63.856
MLP duration (in seconds): 0.0058
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x3328x9984, b=2048): 0.0024
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x3328x9984, b=2048): 228.836
Elapsed time for attention_key_query_prob (512x2048x26x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x26x2048): 15.855
Elapsed time for attention_prob_times_values (512x2048x2048x26): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x26): 29.783
Elapsed time for attention_linear_projection (4x3328x3328, b=2048): 0.0008
Throughput (in TFLOP/s) for attention_linear_projection (4x3328x3328, b=2048): 215.428
Elapsed time for mlp_h_to_4h (4x3328x13312, b=2048): 0.0032
Throughput (in TFLOP/s) for mlp_h_to_4h (4x3328x13312, b=2048): 228.939
Elapsed time for mlp_4h_to_h (4x13312x3328, b=2048): 0.0029
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13312x3328, b=2048): 250.860

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 67.733
MLP duration (in seconds): 0.0061
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x3456x10368, b=2048): 0.0026
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x3456x10368, b=2048): 229.140
Elapsed time for attention_key_query_prob (512x2048x27x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x27x2048): 18.267
Elapsed time for attention_prob_times_values (512x2048x2048x27): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x27): 24.124
Elapsed time for attention_linear_projection (4x3456x3456, b=2048): 0.0009
Throughput (in TFLOP/s) for attention_linear_projection (4x3456x3456, b=2048): 214.752
Elapsed time for mlp_h_to_4h (4x3456x13824, b=2048): 0.0034
Throughput (in TFLOP/s) for mlp_h_to_4h (4x3456x13824, b=2048): 231.190
Elapsed time for mlp_4h_to_h (4x13824x3456, b=2048): 0.0030
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13824x3456, b=2048): 257.439

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 69.363
MLP duration (in seconds): 0.0064
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x3584x10752, b=2048): 0.0027
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x3584x10752, b=2048): 231.115
Elapsed time for attention_key_query_prob (512x2048x28x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x28x2048): 16.366
Elapsed time for attention_prob_times_values (512x2048x2048x28): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x28): 31.894
Elapsed time for attention_linear_projection (4x3584x3584, b=2048): 0.0009
Throughput (in TFLOP/s) for attention_linear_projection (4x3584x3584, b=2048): 221.674
Elapsed time for mlp_h_to_4h (4x3584x14336, b=2048): 0.0036
Throughput (in TFLOP/s) for mlp_h_to_4h (4x3584x14336, b=2048): 232.628
Elapsed time for mlp_4h_to_h (4x14336x3584, b=2048): 0.0035
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14336x3584, b=2048): 242.368

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 73.131
MLP duration (in seconds): 0.0071
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x3712x11136, b=2048): 0.0029
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x3712x11136, b=2048): 231.644
Elapsed time for attention_key_query_prob (512x2048x29x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x29x2048): 19.557
Elapsed time for attention_prob_times_values (512x2048x2048x29): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x29): 25.796
Elapsed time for attention_linear_projection (4x3712x3712, b=2048): 0.0010
Throughput (in TFLOP/s) for attention_linear_projection (4x3712x3712, b=2048): 223.479
Elapsed time for mlp_h_to_4h (4x3712x14848, b=2048): 0.0039
Throughput (in TFLOP/s) for mlp_h_to_4h (4x3712x14848, b=2048): 232.563
Elapsed time for mlp_4h_to_h (4x14848x3712, b=2048): 0.0036
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14848x3712, b=2048): 249.573

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 76.142
MLP duration (in seconds): 0.0075
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x3840x11520, b=2048): 0.0031
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x3840x11520, b=2048): 233.553
Elapsed time for attention_key_query_prob (512x2048x30x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x30x2048): 17.790
Elapsed time for attention_prob_times_values (512x2048x2048x30): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x30): 34.039
Elapsed time for attention_linear_projection (4x3840x3840, b=2048): 0.0011
Throughput (in TFLOP/s) for attention_linear_projection (4x3840x3840, b=2048): 220.909
Elapsed time for mlp_h_to_4h (4x3840x15360, b=2048): 0.0041
Throughput (in TFLOP/s) for mlp_h_to_4h (4x3840x15360, b=2048): 235.134
Elapsed time for mlp_4h_to_h (4x15360x3840, b=2048): 0.0038
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15360x3840, b=2048): 255.242

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 80.399
MLP duration (in seconds): 0.0079
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x3968x11904, b=2048): 0.0033
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x3968x11904, b=2048): 234.180
Elapsed time for attention_key_query_prob (512x2048x31x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x31x2048): 20.735
Elapsed time for attention_prob_times_values (512x2048x2048x31): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x31): 28.663
Elapsed time for attention_linear_projection (4x3968x3968, b=2048): 0.0012
Throughput (in TFLOP/s) for attention_linear_projection (4x3968x3968, b=2048): 223.737
Elapsed time for mlp_h_to_4h (4x3968x15872, b=2048): 0.0044
Throughput (in TFLOP/s) for mlp_h_to_4h (4x3968x15872, b=2048): 235.908
Elapsed time for mlp_4h_to_h (4x15872x3968, b=2048): 0.0042
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15872x3968, b=2048): 244.711

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 83.622
MLP duration (in seconds): 0.0086
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x4096x12288, b=2048): 0.0035
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x4096x12288, b=2048): 237.422
Elapsed time for attention_key_query_prob (512x2048x32x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x32x2048): 39.543
Elapsed time for attention_prob_times_values (512x2048x2048x32): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x32): 42.537
Elapsed time for attention_linear_projection (4x4096x4096, b=2048): 0.0012
Throughput (in TFLOP/s) for attention_linear_projection (4x4096x4096, b=2048): 229.255
Elapsed time for mlp_h_to_4h (4x4096x16384, b=2048): 0.0046
Throughput (in TFLOP/s) for mlp_h_to_4h (4x4096x16384, b=2048): 237.422
Elapsed time for mlp_4h_to_h (4x16384x4096, b=2048): 0.0043
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16384x4096, b=2048): 252.834

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 120.783
MLP duration (in seconds): 0.0090
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x4224x12672, b=2048): 0.0035
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x4224x12672, b=2048): 253.153
Elapsed time for attention_key_query_prob (512x2048x33x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x33x2048): 18.959
Elapsed time for attention_prob_times_values (512x2048x2048x33): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x33): 24.332
Elapsed time for attention_linear_projection (4x4224x4224, b=2048): 0.0012
Throughput (in TFLOP/s) for attention_linear_projection (4x4224x4224, b=2048): 249.310
Elapsed time for mlp_h_to_4h (4x4224x16896, b=2048): 0.0047
Throughput (in TFLOP/s) for mlp_h_to_4h (4x4224x16896, b=2048): 247.136
Elapsed time for mlp_4h_to_h (4x16896x4224, b=2048): 0.0046
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16896x4224, b=2048): 254.089

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 80.990
MLP duration (in seconds): 0.0093
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x4352x13056, b=2048): 0.0037
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x4352x13056, b=2048): 253.662
Elapsed time for attention_key_query_prob (512x2048x34x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x34x2048): 15.038
Elapsed time for attention_prob_times_values (512x2048x2048x34): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x34): 38.357
Elapsed time for attention_linear_projection (4x4352x4352, b=2048): 0.0013
Throughput (in TFLOP/s) for attention_linear_projection (4x4352x4352, b=2048): 235.317
Elapsed time for mlp_h_to_4h (4x4352x17408, b=2048): 0.0049
Throughput (in TFLOP/s) for mlp_h_to_4h (4x4352x17408, b=2048): 251.226
Elapsed time for mlp_4h_to_h (4x17408x4352, b=2048): 0.0051
Throughput (in TFLOP/s) for mlp_4h_to_h (4x17408x4352, b=2048): 244.627

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 82.853
MLP duration (in seconds): 0.0100
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x4480x13440, b=2048): 0.0040
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x4480x13440, b=2048): 246.672
Elapsed time for attention_key_query_prob (512x2048x35x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x35x2048): 19.707
Elapsed time for attention_prob_times_values (512x2048x2048x35): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x35): 25.316
Elapsed time for attention_linear_projection (4x4480x4480, b=2048): 0.0014
Throughput (in TFLOP/s) for attention_linear_projection (4x4480x4480, b=2048): 239.408
Elapsed time for mlp_h_to_4h (4x4480x17920, b=2048): 0.0051
Throughput (in TFLOP/s) for mlp_h_to_4h (4x4480x17920, b=2048): 255.886
Elapsed time for mlp_4h_to_h (4x17920x4480, b=2048): 0.0053
Throughput (in TFLOP/s) for mlp_4h_to_h (4x17920x4480, b=2048): 249.149

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 85.328
MLP duration (in seconds): 0.0104
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0294
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x4608x13824, b=2048): 0.0042
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x4608x13824, b=2048): 250.114
Elapsed time for attention_key_query_prob (512x2048x36x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x36x2048): 15.233
Elapsed time for attention_prob_times_values (512x2048x2048x36): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x36): 40.404
Elapsed time for attention_linear_projection (4x4608x4608, b=2048): 0.0014
Throughput (in TFLOP/s) for attention_linear_projection (4x4608x4608, b=2048): 243.235
Elapsed time for mlp_h_to_4h (4x4608x18432, b=2048): 0.0054
Throughput (in TFLOP/s) for mlp_h_to_4h (4x4608x18432, b=2048): 256.895
Elapsed time for mlp_4h_to_h (4x18432x4608, b=2048): 0.0055
Throughput (in TFLOP/s) for mlp_4h_to_h (4x18432x4608, b=2048): 250.834

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 86.864
MLP duration (in seconds): 0.0110
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x4736x14208, b=2048): 0.0044
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x4736x14208, b=2048): 250.085
Elapsed time for attention_key_query_prob (512x2048x37x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x37x2048): 20.733
Elapsed time for attention_prob_times_values (512x2048x2048x37): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x37): 26.667
Elapsed time for attention_linear_projection (4x4736x4736, b=2048): 0.0015
Throughput (in TFLOP/s) for attention_linear_projection (4x4736x4736, b=2048): 251.199
Elapsed time for mlp_h_to_4h (4x4736x18944, b=2048): 0.0057
Throughput (in TFLOP/s) for mlp_h_to_4h (4x4736x18944, b=2048): 256.957
Elapsed time for mlp_4h_to_h (4x18944x4736, b=2048): 0.0057
Throughput (in TFLOP/s) for mlp_4h_to_h (4x18944x4736, b=2048): 257.171

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 91.703
MLP duration (in seconds): 0.0114
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x4864x14592, b=2048): 0.0047
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x4864x14592, b=2048): 249.931
Elapsed time for attention_key_query_prob (512x2048x38x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x38x2048): 16.414
Elapsed time for attention_prob_times_values (512x2048x2048x38): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x38): 42.529
Elapsed time for attention_linear_projection (4x4864x4864, b=2048): 0.0016
Throughput (in TFLOP/s) for attention_linear_projection (4x4864x4864, b=2048): 238.562
Elapsed time for mlp_h_to_4h (4x4864x19456, b=2048): 0.0064
Throughput (in TFLOP/s) for mlp_h_to_4h (4x4864x19456, b=2048): 243.183
Elapsed time for mlp_4h_to_h (4x19456x4864, b=2048): 0.0065
Throughput (in TFLOP/s) for mlp_4h_to_h (4x19456x4864, b=2048): 237.412

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 93.572
MLP duration (in seconds): 0.0129
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0330
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x4992x14976, b=2048): 0.0048
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x4992x14976, b=2048): 252.767
Elapsed time for attention_key_query_prob (512x2048x39x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x39x2048): 21.281
Elapsed time for attention_prob_times_values (512x2048x2048x39): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x39): 27.083
Elapsed time for attention_linear_projection (4x4992x4992, b=2048): 0.0017
Throughput (in TFLOP/s) for attention_linear_projection (4x4992x4992, b=2048): 244.223
Elapsed time for mlp_h_to_4h (4x4992x19968, b=2048): 0.0068
Throughput (in TFLOP/s) for mlp_h_to_4h (4x4992x19968, b=2048): 241.417
Elapsed time for mlp_4h_to_h (4x19968x4992, b=2048): 0.0069
Throughput (in TFLOP/s) for mlp_4h_to_h (4x19968x4992, b=2048): 237.451

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 95.665
MLP duration (in seconds): 0.0136
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0342
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x5120x15360, b=2048): 0.0051
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x5120x15360, b=2048): 251.785
Elapsed time for attention_key_query_prob (512x2048x40x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x40x2048): 39.784
Elapsed time for attention_prob_times_values (512x2048x2048x40): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x40): 52.204
Elapsed time for attention_linear_projection (4x5120x5120, b=2048): 0.0017
Throughput (in TFLOP/s) for attention_linear_projection (4x5120x5120, b=2048): 250.235
Elapsed time for mlp_h_to_4h (4x5120x20480, b=2048): 0.0072
Throughput (in TFLOP/s) for mlp_h_to_4h (4x5120x20480, b=2048): 239.936
Elapsed time for mlp_4h_to_h (4x20480x5120, b=2048): 0.0072
Throughput (in TFLOP/s) for mlp_4h_to_h (4x20480x5120, b=2048): 239.864

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 142.740
MLP duration (in seconds): 0.0143
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0288
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x5248x15744, b=2048): 0.0054
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x5248x15744, b=2048): 248.857
Elapsed time for attention_key_query_prob (512x2048x41x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x41x2048): 22.274
Elapsed time for attention_prob_times_values (512x2048x2048x41): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x41): 28.381
Elapsed time for attention_linear_projection (4x5248x5248, b=2048): 0.0019
Throughput (in TFLOP/s) for attention_linear_projection (4x5248x5248, b=2048): 240.000
Elapsed time for mlp_h_to_4h (4x5248x20992, b=2048): 0.0077
Throughput (in TFLOP/s) for mlp_h_to_4h (4x5248x20992, b=2048): 235.139
Elapsed time for mlp_4h_to_h (4x20992x5248, b=2048): 0.0074
Throughput (in TFLOP/s) for mlp_4h_to_h (4x20992x5248, b=2048): 242.553

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 100.658
MLP duration (in seconds): 0.0151
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0365
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x5376x16128, b=2048): 0.0057
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x5376x16128, b=2048): 249.373
Elapsed time for attention_key_query_prob (512x2048x42x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x42x2048): 17.809
Elapsed time for attention_prob_times_values (512x2048x2048x42): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x42): 46.727
Elapsed time for attention_linear_projection (4x5376x5376, b=2048): 0.0019
Throughput (in TFLOP/s) for attention_linear_projection (4x5376x5376, b=2048): 244.924
Elapsed time for mlp_h_to_4h (4x5376x21504, b=2048): 0.0079
Throughput (in TFLOP/s) for mlp_h_to_4h (4x5376x21504, b=2048): 239.050
Elapsed time for mlp_4h_to_h (4x21504x5376, b=2048): 0.0080
Throughput (in TFLOP/s) for mlp_4h_to_h (4x21504x5376, b=2048): 236.566

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 104.298
MLP duration (in seconds): 0.0159
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0375
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x5504x16512, b=2048): 0.0059
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x5504x16512, b=2048): 250.456
Elapsed time for attention_key_query_prob (512x2048x43x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x43x2048): 22.814
Elapsed time for attention_prob_times_values (512x2048x2048x43): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x43): 30.611
Elapsed time for attention_linear_projection (4x5504x5504, b=2048): 0.0020
Throughput (in TFLOP/s) for attention_linear_projection (4x5504x5504, b=2048): 247.744
Elapsed time for mlp_h_to_4h (4x5504x22016, b=2048): 0.0081
Throughput (in TFLOP/s) for mlp_h_to_4h (4x5504x22016, b=2048): 246.060
Elapsed time for mlp_4h_to_h (4x22016x5504, b=2048): 0.0082
Throughput (in TFLOP/s) for mlp_4h_to_h (4x22016x5504, b=2048): 242.076

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 106.660
MLP duration (in seconds): 0.0163
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0383
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x5632x16896, b=2048): 0.0061
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x5632x16896, b=2048): 256.089
Elapsed time for attention_key_query_prob (512x2048x44x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x44x2048): 17.948
Elapsed time for attention_prob_times_values (512x2048x2048x44): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x44): 48.847
Elapsed time for attention_linear_projection (4x5632x5632, b=2048): 0.0021
Throughput (in TFLOP/s) for attention_linear_projection (4x5632x5632, b=2048): 243.874
Elapsed time for mlp_h_to_4h (4x5632x22528, b=2048): 0.0086
Throughput (in TFLOP/s) for mlp_h_to_4h (4x5632x22528, b=2048): 240.517
Elapsed time for mlp_4h_to_h (4x22528x5632, b=2048): 0.0088
Throughput (in TFLOP/s) for mlp_4h_to_h (4x22528x5632, b=2048): 236.646

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 108.623
MLP duration (in seconds): 0.0174
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0400
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x5760x17280, b=2048): 0.0068
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x5760x17280, b=2048): 238.696
Elapsed time for attention_key_query_prob (512x2048x45x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x45x2048): 23.988
Elapsed time for attention_prob_times_values (512x2048x2048x45): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x45): 31.930
Elapsed time for attention_linear_projection (4x5760x5760, b=2048): 0.0022
Throughput (in TFLOP/s) for attention_linear_projection (4x5760x5760, b=2048): 244.446
Elapsed time for mlp_h_to_4h (4x5760x23040, b=2048): 0.0089
Throughput (in TFLOP/s) for mlp_h_to_4h (4x5760x23040, b=2048): 244.367
Elapsed time for mlp_4h_to_h (4x23040x5760, b=2048): 0.0089
Throughput (in TFLOP/s) for mlp_4h_to_h (4x23040x5760, b=2048): 243.610

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 110.546
MLP duration (in seconds): 0.0178
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0410
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x5888x17664, b=2048): 0.0069
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x5888x17664, b=2048): 246.040
Elapsed time for attention_key_query_prob (512x2048x46x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x46x2048): 18.606
Elapsed time for attention_prob_times_values (512x2048x2048x46): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x46): 50.823
Elapsed time for attention_linear_projection (4x5888x5888, b=2048): 0.0023
Throughput (in TFLOP/s) for attention_linear_projection (4x5888x5888, b=2048): 248.296
Elapsed time for mlp_h_to_4h (4x5888x23552, b=2048): 0.0092
Throughput (in TFLOP/s) for mlp_h_to_4h (4x5888x23552, b=2048): 247.754
Elapsed time for mlp_4h_to_h (4x23552x5888, b=2048): 0.0093
Throughput (in TFLOP/s) for mlp_4h_to_h (4x23552x5888, b=2048): 244.362

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 112.447
MLP duration (in seconds): 0.0185
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0422
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x6016x18048, b=2048): 0.0075
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x6016x18048, b=2048): 237.903
Elapsed time for attention_key_query_prob (512x2048x47x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x47x2048): 24.726
Elapsed time for attention_prob_times_values (512x2048x2048x47): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x47): 32.262
Elapsed time for attention_linear_projection (4x6016x6016, b=2048): 0.0024
Throughput (in TFLOP/s) for attention_linear_projection (4x6016x6016, b=2048): 250.717
Elapsed time for mlp_h_to_4h (4x6016x24064, b=2048): 0.0096
Throughput (in TFLOP/s) for mlp_h_to_4h (4x6016x24064, b=2048): 246.420
Elapsed time for mlp_4h_to_h (4x24064x6016, b=2048): 0.0097
Throughput (in TFLOP/s) for mlp_4h_to_h (4x24064x6016, b=2048): 244.350

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 114.394
MLP duration (in seconds): 0.0193
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0436
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x6144x18432, b=2048): 0.0075
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x6144x18432, b=2048): 248.506
Elapsed time for attention_key_query_prob (512x2048x48x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x48x2048): 46.854
Elapsed time for attention_prob_times_values (512x2048x2048x48): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x48): 62.266
Elapsed time for attention_linear_projection (4x6144x6144, b=2048): 0.0025
Throughput (in TFLOP/s) for attention_linear_projection (4x6144x6144, b=2048): 245.418
Elapsed time for mlp_h_to_4h (4x6144x24576, b=2048): 0.0100
Throughput (in TFLOP/s) for mlp_h_to_4h (4x6144x24576, b=2048): 246.908
Elapsed time for mlp_4h_to_h (4x24576x6144, b=2048): 0.0100
Throughput (in TFLOP/s) for mlp_4h_to_h (4x24576x6144, b=2048): 247.508

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 163.088
MLP duration (in seconds): 0.0200
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0377
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x6272x18816, b=2048): 0.0079
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x6272x18816, b=2048): 244.804
Elapsed time for attention_key_query_prob (512x2048x49x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x49x2048): 25.507
Elapsed time for attention_prob_times_values (512x2048x2048x49): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x49): 33.465
Elapsed time for attention_linear_projection (4x6272x6272, b=2048): 0.0026
Throughput (in TFLOP/s) for attention_linear_projection (4x6272x6272, b=2048): 248.372
Elapsed time for mlp_h_to_4h (4x6272x25088, b=2048): 0.0105
Throughput (in TFLOP/s) for mlp_h_to_4h (4x6272x25088, b=2048): 244.630
Elapsed time for mlp_4h_to_h (4x25088x6272, b=2048): 0.0109
Throughput (in TFLOP/s) for mlp_4h_to_h (4x25088x6272, b=2048): 236.368

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 119.800
MLP duration (in seconds): 0.0214
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0465
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x6400x19200, b=2048): 0.0081
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x6400x19200, b=2048): 248.127
Elapsed time for attention_key_query_prob (512x2048x50x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x50x2048): 19.624
Elapsed time for attention_prob_times_values (512x2048x2048x50): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x50): 54.952
Elapsed time for attention_linear_projection (4x6400x6400, b=2048): 0.0027
Throughput (in TFLOP/s) for attention_linear_projection (4x6400x6400, b=2048): 251.992
Elapsed time for mlp_h_to_4h (4x6400x25600, b=2048): 0.0109
Throughput (in TFLOP/s) for mlp_h_to_4h (4x6400x25600, b=2048): 245.529
Elapsed time for mlp_4h_to_h (4x25600x6400, b=2048): 0.0111
Throughput (in TFLOP/s) for mlp_4h_to_h (4x25600x6400, b=2048): 240.973

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 121.500
MLP duration (in seconds): 0.0221
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0477
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x6528x19584, b=2048): 0.0086
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x6528x19584, b=2048): 244.705
Elapsed time for attention_key_query_prob (512x2048x51x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x51x2048): 26.367
Elapsed time for attention_prob_times_values (512x2048x2048x51): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x51): 35.870
Elapsed time for attention_linear_projection (4x6528x6528, b=2048): 0.0028
Throughput (in TFLOP/s) for attention_linear_projection (4x6528x6528, b=2048): 245.327
Elapsed time for mlp_h_to_4h (4x6528x26112, b=2048): 0.0112
Throughput (in TFLOP/s) for mlp_h_to_4h (4x6528x26112, b=2048): 249.417
Elapsed time for mlp_4h_to_h (4x26112x6528, b=2048): 0.0118
Throughput (in TFLOP/s) for mlp_4h_to_h (4x26112x6528, b=2048): 237.272

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 125.132
MLP duration (in seconds): 0.0230
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0488
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x6656x19968, b=2048): 0.0090
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x6656x19968, b=2048): 241.577
Elapsed time for attention_key_query_prob (512x2048x52x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x52x2048): 19.997
Elapsed time for attention_prob_times_values (512x2048x2048x52): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x52): 56.935
Elapsed time for attention_linear_projection (4x6656x6656, b=2048): 0.0029
Throughput (in TFLOP/s) for attention_linear_projection (4x6656x6656, b=2048): 250.674
Elapsed time for mlp_h_to_4h (4x6656x26624, b=2048): 0.0116
Throughput (in TFLOP/s) for mlp_h_to_4h (4x6656x26624, b=2048): 250.056
Elapsed time for mlp_4h_to_h (4x26624x6656, b=2048): 0.0122
Throughput (in TFLOP/s) for mlp_4h_to_h (4x26624x6656, b=2048): 237.990

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 124.073
MLP duration (in seconds): 0.0238
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0508
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x6784x20352, b=2048): 0.0092
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x6784x20352, b=2048): 247.063
Elapsed time for attention_key_query_prob (512x2048x53x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x53x2048): 27.284
Elapsed time for attention_prob_times_values (512x2048x2048x53): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x53): 36.999
Elapsed time for attention_linear_projection (4x6784x6784, b=2048): 0.0030
Throughput (in TFLOP/s) for attention_linear_projection (4x6784x6784, b=2048): 253.886
Elapsed time for mlp_h_to_4h (4x6784x27136, b=2048): 0.0120
Throughput (in TFLOP/s) for mlp_h_to_4h (4x6784x27136, b=2048): 250.745
Elapsed time for mlp_4h_to_h (4x27136x6784, b=2048): 0.0126
Throughput (in TFLOP/s) for mlp_4h_to_h (4x27136x6784, b=2048): 239.097

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 130.398
MLP duration (in seconds): 0.0246
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0513
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x6912x20736, b=2048): 0.0096
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x6912x20736, b=2048): 245.113
Elapsed time for attention_key_query_prob (512x2048x54x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x54x2048): 21.047
Elapsed time for attention_prob_times_values (512x2048x2048x54): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x54): 59.074
Elapsed time for attention_linear_projection (4x6912x6912, b=2048): 0.0030
Throughput (in TFLOP/s) for attention_linear_projection (4x6912x6912, b=2048): 257.278
Elapsed time for mlp_h_to_4h (4x6912x27648, b=2048): 0.0126
Throughput (in TFLOP/s) for mlp_h_to_4h (4x6912x27648, b=2048): 248.547
Elapsed time for mlp_4h_to_h (4x27648x6912, b=2048): 0.0126
Throughput (in TFLOP/s) for mlp_4h_to_h (4x27648x6912, b=2048): 249.359

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 130.398
MLP duration (in seconds): 0.0252
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0527
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x7040x21120, b=2048): 0.0098
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x7040x21120, b=2048): 248.083
Elapsed time for attention_key_query_prob (512x2048x55x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x55x2048): 27.350
Elapsed time for attention_prob_times_values (512x2048x2048x55): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x55): 37.024
Elapsed time for attention_linear_projection (4x7040x7040, b=2048): 0.0032
Throughput (in TFLOP/s) for attention_linear_projection (4x7040x7040, b=2048): 250.817
Elapsed time for mlp_h_to_4h (4x7040x28160, b=2048): 0.0130
Throughput (in TFLOP/s) for mlp_h_to_4h (4x7040x28160, b=2048): 249.440
Elapsed time for mlp_4h_to_h (4x28160x7040, b=2048): 0.0139
Throughput (in TFLOP/s) for mlp_4h_to_h (4x28160x7040, b=2048): 233.802

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 132.524
MLP duration (in seconds): 0.0269
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0550
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x7168x21504, b=2048): 0.0103
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x7168x21504, b=2048): 244.562
Elapsed time for attention_key_query_prob (512x2048x56x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x56x2048): 54.205
Elapsed time for attention_prob_times_values (512x2048x2048x56): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x56): 71.038
Elapsed time for attention_linear_projection (4x7168x7168, b=2048): 0.0033
Throughput (in TFLOP/s) for attention_linear_projection (4x7168x7168, b=2048): 253.251
Elapsed time for mlp_h_to_4h (4x7168x28672, b=2048): 0.0136
Throughput (in TFLOP/s) for mlp_h_to_4h (4x7168x28672, b=2048): 247.720
Elapsed time for mlp_4h_to_h (4x28672x7168, b=2048): 0.0137
Throughput (in TFLOP/s) for mlp_4h_to_h (4x28672x7168, b=2048): 246.626

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 179.212
MLP duration (in seconds): 0.0272
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0487
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x7296x21888, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x7296x21888, b=2048): 239.647
Elapsed time for attention_key_query_prob (512x2048x57x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x57x2048): 28.036
Elapsed time for attention_prob_times_values (512x2048x2048x57): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x57): 38.254
Elapsed time for attention_linear_projection (4x7296x7296, b=2048): 0.0034
Throughput (in TFLOP/s) for attention_linear_projection (4x7296x7296, b=2048): 257.500
Elapsed time for mlp_h_to_4h (4x7296x29184, b=2048): 0.0142
Throughput (in TFLOP/s) for mlp_h_to_4h (4x7296x29184, b=2048): 245.618
Elapsed time for mlp_4h_to_h (4x29184x7296, b=2048): 0.0151
Throughput (in TFLOP/s) for mlp_4h_to_h (4x29184x7296, b=2048): 230.410

Attention duration (in seconds): 0.0294
Attention throughput (in TFLOP/s): 135.145
MLP duration (in seconds): 0.0293
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0588
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x7424x22272, b=2048): 0.0113
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x7424x22272, b=2048): 239.389
Elapsed time for attention_key_query_prob (512x2048x58x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x58x2048): 22.137
Elapsed time for attention_prob_times_values (512x2048x2048x58): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x58): 62.961
Elapsed time for attention_linear_projection (4x7424x7424, b=2048): 0.0036
Throughput (in TFLOP/s) for attention_linear_projection (4x7424x7424, b=2048): 253.380
Elapsed time for mlp_h_to_4h (4x7424x29696, b=2048): 0.0145
Throughput (in TFLOP/s) for mlp_h_to_4h (4x7424x29696, b=2048): 249.491
Elapsed time for mlp_4h_to_h (4x29696x7424, b=2048): 0.0148
Throughput (in TFLOP/s) for mlp_4h_to_h (4x29696x7424, b=2048): 243.395

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 136.600
MLP duration (in seconds): 0.0293
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0594
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x7552x22656, b=2048): 0.0116
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x7552x22656, b=2048): 241.497
Elapsed time for attention_key_query_prob (512x2048x59x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x59x2048): 29.607
Elapsed time for attention_prob_times_values (512x2048x2048x59): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x59): 40.816
Elapsed time for attention_linear_projection (4x7552x7552, b=2048): 0.0037
Throughput (in TFLOP/s) for attention_linear_projection (4x7552x7552, b=2048): 253.411
Elapsed time for mlp_h_to_4h (4x7552x30208, b=2048): 0.0157
Throughput (in TFLOP/s) for mlp_h_to_4h (4x7552x30208, b=2048): 237.819
Elapsed time for mlp_4h_to_h (4x30208x7552, b=2048): 0.0165
Throughput (in TFLOP/s) for mlp_4h_to_h (4x30208x7552, b=2048): 227.022

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 141.190
MLP duration (in seconds): 0.0322
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0622
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x7680x23040, b=2048): 0.0117
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x7680x23040, b=2048): 246.893
Elapsed time for attention_key_query_prob (512x2048x60x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x60x2048): 21.279
Elapsed time for attention_prob_times_values (512x2048x2048x60): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x60): 65.050
Elapsed time for attention_linear_projection (4x7680x7680, b=2048): 0.0037
Throughput (in TFLOP/s) for attention_linear_projection (4x7680x7680, b=2048): 260.257
Elapsed time for mlp_h_to_4h (4x7680x30720, b=2048): 0.0159
Throughput (in TFLOP/s) for mlp_h_to_4h (4x7680x30720, b=2048): 243.852
Elapsed time for mlp_4h_to_h (4x30720x7680, b=2048): 0.0161
Throughput (in TFLOP/s) for mlp_4h_to_h (4x30720x7680, b=2048): 239.982

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 138.954
MLP duration (in seconds): 0.0320
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0635
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x7808x23424, b=2048): 0.0120
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x7808x23424, b=2048): 249.631
Elapsed time for attention_key_query_prob (512x2048x61x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x61x2048): 30.086
Elapsed time for attention_prob_times_values (512x2048x2048x61): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x61): 42.070
Elapsed time for attention_linear_projection (4x7808x7808, b=2048): 0.0039
Throughput (in TFLOP/s) for attention_linear_projection (4x7808x7808, b=2048): 253.095
Elapsed time for mlp_h_to_4h (4x7808x31232, b=2048): 0.0159
Throughput (in TFLOP/s) for mlp_h_to_4h (4x7808x31232, b=2048): 250.863
Elapsed time for mlp_4h_to_h (4x31232x7808, b=2048): 0.0172
Throughput (in TFLOP/s) for mlp_4h_to_h (4x31232x7808, b=2048): 232.036

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 146.325
MLP duration (in seconds): 0.0331
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0640
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x7936x23808, b=2048): 0.0126
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x7936x23808, b=2048): 246.462
Elapsed time for attention_key_query_prob (512x2048x62x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x62x2048): 22.970
Elapsed time for attention_prob_times_values (512x2048x2048x62): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x62): 66.840
Elapsed time for attention_linear_projection (4x7936x7936, b=2048): 0.0040
Throughput (in TFLOP/s) for attention_linear_projection (4x7936x7936, b=2048): 257.310
Elapsed time for mlp_h_to_4h (4x7936x31744, b=2048): 0.0166
Throughput (in TFLOP/s) for mlp_h_to_4h (4x7936x31744, b=2048): 248.473
Elapsed time for mlp_4h_to_h (4x31744x7936, b=2048): 0.0170
Throughput (in TFLOP/s) for mlp_4h_to_h (4x31744x7936, b=2048): 242.510

Attention duration (in seconds): 0.0321
Attention throughput (in TFLOP/s): 144.961
MLP duration (in seconds): 0.0336
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0658
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8064x24192, b=2048): 0.0130
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8064x24192, b=2048): 245.983
Elapsed time for attention_key_query_prob (512x2048x63x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x63x2048): 30.285
Elapsed time for attention_prob_times_values (512x2048x2048x63): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x63): 41.708
Elapsed time for attention_linear_projection (4x8064x8064, b=2048): 0.0041
Throughput (in TFLOP/s) for attention_linear_projection (4x8064x8064, b=2048): 258.321
Elapsed time for mlp_h_to_4h (4x8064x32256, b=2048): 0.0177
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8064x32256, b=2048): 240.673
Elapsed time for mlp_4h_to_h (4x32256x8064, b=2048): 0.0184
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32256x8064, b=2048): 231.911

Attention duration (in seconds): 0.0325
Attention throughput (in TFLOP/s): 147.597
MLP duration (in seconds): 0.0361
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0686
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8192x24576, b=2048): 0.0135
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8192x24576, b=2048): 244.444
Elapsed time for attention_key_query_prob (512x2048x64x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x64x2048): 62.611
Elapsed time for attention_prob_times_values (512x2048x2048x64): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x64): 81.318
Elapsed time for attention_linear_projection (4x8192x8192, b=2048): 0.0042
Throughput (in TFLOP/s) for attention_linear_projection (4x8192x8192, b=2048): 260.297
Elapsed time for mlp_h_to_4h (4x8192x32768, b=2048): 0.0178
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8192x32768, b=2048): 247.083
Elapsed time for mlp_4h_to_h (4x32768x8192, b=2048): 0.0195
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32768x8192, b=2048): 225.163

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 194.118
MLP duration (in seconds): 0.0373
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0628
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8320x24960, b=2048): 0.0138
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8320x24960, b=2048): 245.688
Elapsed time for attention_key_query_prob (512x2048x65x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x65x2048): 27.153
Elapsed time for attention_prob_times_values (512x2048x2048x65): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x65): 41.618
Elapsed time for attention_linear_projection (4x8320x8320, b=2048): 0.0045
Throughput (in TFLOP/s) for attention_linear_projection (4x8320x8320, b=2048): 254.014
Elapsed time for mlp_h_to_4h (4x8320x33280, b=2048): 0.0180
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8320x33280, b=2048): 251.347
Elapsed time for mlp_4h_to_h (4x33280x8320, b=2048): 0.0195
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33280x8320, b=2048): 233.040

Attention duration (in seconds): 0.0353
Attention throughput (in TFLOP/s): 144.320
MLP duration (in seconds): 0.0375
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0728
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8448x25344, b=2048): 0.0142
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8448x25344, b=2048): 246.433
Elapsed time for attention_key_query_prob (512x2048x66x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x66x2048): 19.744
Elapsed time for attention_prob_times_values (512x2048x2048x66): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x66): 71.084
Elapsed time for attention_linear_projection (4x8448x8448, b=2048): 0.0045
Throughput (in TFLOP/s) for attention_linear_projection (4x8448x8448, b=2048): 258.182
Elapsed time for mlp_h_to_4h (4x8448x33792, b=2048): 0.0188
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8448x33792, b=2048): 249.316
Elapsed time for mlp_4h_to_h (4x33792x8448, b=2048): 0.0206
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33792x8448, b=2048): 226.907

Attention duration (in seconds): 0.0371
Attention throughput (in TFLOP/s): 141.318
MLP duration (in seconds): 0.0394
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0765
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8576x25728, b=2048): 0.0155
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8576x25728, b=2048): 233.863
Elapsed time for attention_key_query_prob (512x2048x67x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x67x2048): 28.029
Elapsed time for attention_prob_times_values (512x2048x2048x67): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x67): 42.949
Elapsed time for attention_linear_projection (4x8576x8576, b=2048): 0.0046
Throughput (in TFLOP/s) for attention_linear_projection (4x8576x8576, b=2048): 262.105
Elapsed time for mlp_h_to_4h (4x8576x34304, b=2048): 0.0200
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8576x34304, b=2048): 241.335
Elapsed time for mlp_4h_to_h (4x34304x8576, b=2048): 0.0206
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34304x8576, b=2048): 234.450

Attention duration (in seconds): 0.0370
Attention throughput (in TFLOP/s): 145.739
MLP duration (in seconds): 0.0405
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0776
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8704x26112, b=2048): 0.0153
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8704x26112, b=2048): 242.764
Elapsed time for attention_key_query_prob (512x2048x68x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x68x2048): 19.312
Elapsed time for attention_prob_times_values (512x2048x2048x68): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x68): 72.617
Elapsed time for attention_linear_projection (4x8704x8704, b=2048): 0.0049
Throughput (in TFLOP/s) for attention_linear_projection (4x8704x8704, b=2048): 254.356
Elapsed time for mlp_h_to_4h (4x8704x34816, b=2048): 0.0204
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8704x34816, b=2048): 243.834
Elapsed time for mlp_4h_to_h (4x34816x8704, b=2048): 0.0206
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34816x8704, b=2048): 241.115

Attention duration (in seconds): 0.0394
Attention throughput (in TFLOP/s): 140.968
MLP duration (in seconds): 0.0410
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0803
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8832x26496, b=2048): 0.0160
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8832x26496, b=2048): 240.302
Elapsed time for attention_key_query_prob (512x2048x69x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x69x2048): 28.379
Elapsed time for attention_prob_times_values (512x2048x2048x69): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x69): 43.752
Elapsed time for attention_linear_projection (4x8832x8832, b=2048): 0.0050
Throughput (in TFLOP/s) for attention_linear_projection (4x8832x8832, b=2048): 255.221
Elapsed time for mlp_h_to_4h (4x8832x35328, b=2048): 0.0206
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8832x35328, b=2048): 247.909
Elapsed time for mlp_4h_to_h (4x35328x8832, b=2048): 0.0212
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35328x8832, b=2048): 240.585

Attention duration (in seconds): 0.0382
Attention throughput (in TFLOP/s): 149.423
MLP duration (in seconds): 0.0419
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0800
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8960x26880, b=2048): 0.0163
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8960x26880, b=2048): 242.207
Elapsed time for attention_key_query_prob (512x2048x70x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x70x2048): 20.288
Elapsed time for attention_prob_times_values (512x2048x2048x70): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x70): 73.985
Elapsed time for attention_linear_projection (4x8960x8960, b=2048): 0.0051
Throughput (in TFLOP/s) for attention_linear_projection (4x8960x8960, b=2048): 259.558
Elapsed time for mlp_h_to_4h (4x8960x35840, b=2048): 0.0207
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8960x35840, b=2048): 254.004
Elapsed time for mlp_4h_to_h (4x35840x8960, b=2048): 0.0210
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35840x8960, b=2048): 250.396

Attention duration (in seconds): 0.0402
Attention throughput (in TFLOP/s): 145.685
MLP duration (in seconds): 0.0417
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0820
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9088x27264, b=2048): 0.0167
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9088x27264, b=2048): 242.539
Elapsed time for attention_key_query_prob (512x2048x71x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x71x2048): 28.059
Elapsed time for attention_prob_times_values (512x2048x2048x71): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x71): 44.730
Elapsed time for attention_linear_projection (4x9088x9088, b=2048): 0.0053
Throughput (in TFLOP/s) for attention_linear_projection (4x9088x9088, b=2048): 253.763
Elapsed time for mlp_h_to_4h (4x9088x36352, b=2048): 0.0212
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9088x36352, b=2048): 255.689
Elapsed time for mlp_4h_to_h (4x36352x9088, b=2048): 0.0215
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36352x9088, b=2048): 252.148

Attention duration (in seconds): 0.0398
Attention throughput (in TFLOP/s): 151.492
MLP duration (in seconds): 0.0426
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0824
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9216x27648, b=2048): 0.0175
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9216x27648, b=2048): 238.634
Elapsed time for attention_key_query_prob (512x2048x72x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x72x2048): 68.366
Elapsed time for attention_prob_times_values (512x2048x2048x72): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x72): 85.410
Elapsed time for attention_linear_projection (4x9216x9216, b=2048): 0.0054
Throughput (in TFLOP/s) for attention_linear_projection (4x9216x9216, b=2048): 257.337
Elapsed time for mlp_h_to_4h (4x9216x36864, b=2048): 0.0220
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9216x36864, b=2048): 252.727
Elapsed time for mlp_4h_to_h (4x36864x9216, b=2048): 0.0232
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36864x9216, b=2048): 239.824

Attention duration (in seconds): 0.0310
Attention throughput (in TFLOP/s): 199.215
MLP duration (in seconds): 0.0452
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0763
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9344x28032, b=2048): 0.0176
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9344x28032, b=2048): 244.439
Elapsed time for attention_key_query_prob (512x2048x73x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x73x2048): 28.490
Elapsed time for attention_prob_times_values (512x2048x2048x73): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x73): 45.825
Elapsed time for attention_linear_projection (4x9344x9344, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_linear_projection (4x9344x9344, b=2048): 260.628
Elapsed time for mlp_h_to_4h (4x9344x37376, b=2048): 0.0226
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9344x37376, b=2048): 253.340
Elapsed time for mlp_4h_to_h (4x37376x9344, b=2048): 0.0239
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37376x9344, b=2048): 239.872

Attention duration (in seconds): 0.0409
Attention throughput (in TFLOP/s): 155.263
MLP duration (in seconds): 0.0464
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0873
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9472x28416, b=2048): 0.0183
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9472x28416, b=2048): 240.324
Elapsed time for attention_key_query_prob (512x2048x74x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x74x2048): 51.126
Elapsed time for attention_prob_times_values (512x2048x2048x74): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x74): 77.199
Elapsed time for attention_linear_projection (4x9472x9472, b=2048): 0.0060
Throughput (in TFLOP/s) for attention_linear_projection (4x9472x9472, b=2048): 245.644
Elapsed time for mlp_h_to_4h (4x9472x37888, b=2048): 0.0236
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9472x37888, b=2048): 249.605
Elapsed time for mlp_4h_to_h (4x37888x9472, b=2048): 0.0242
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37888x9472, b=2048): 242.530

Attention duration (in seconds): 0.0347
Attention throughput (in TFLOP/s): 187.943
MLP duration (in seconds): 0.0478
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0825
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9600x28800, b=2048): 0.0184
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9600x28800, b=2048): 246.587
Elapsed time for attention_key_query_prob (512x2048x75x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x75x2048): 29.930
Elapsed time for attention_prob_times_values (512x2048x2048x75): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x75): 47.333
Elapsed time for attention_linear_projection (4x9600x9600, b=2048): 0.0062
Throughput (in TFLOP/s) for attention_linear_projection (4x9600x9600, b=2048): 244.223
Elapsed time for mlp_h_to_4h (4x9600x38400, b=2048): 0.0243
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9600x38400, b=2048): 248.183
Elapsed time for mlp_4h_to_h (4x38400x9600, b=2048): 0.0249
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38400x9600, b=2048): 242.363

Attention duration (in seconds): 0.0421
Attention throughput (in TFLOP/s): 158.687
MLP duration (in seconds): 0.0493
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0914
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9728x29184, b=2048): 0.0193
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9728x29184, b=2048): 240.922
Elapsed time for attention_key_query_prob (512x2048x76x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x76x2048): 52.061
Elapsed time for attention_prob_times_values (512x2048x2048x76): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x76): 79.290
Elapsed time for attention_linear_projection (4x9728x9728, b=2048): 0.0065
Throughput (in TFLOP/s) for attention_linear_projection (4x9728x9728, b=2048): 237.404
Elapsed time for mlp_h_to_4h (4x9728x38912, b=2048): 0.0251
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9728x38912, b=2048): 247.451
Elapsed time for mlp_4h_to_h (4x38912x9728, b=2048): 0.0246
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38912x9728, b=2048): 252.020

Attention duration (in seconds): 0.0362
Attention throughput (in TFLOP/s): 189.230
MLP duration (in seconds): 0.0497
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0859
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9856x29568, b=2048): 0.0195
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9856x29568, b=2048): 244.439
Elapsed time for attention_key_query_prob (512x2048x77x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x77x2048): 30.075
Elapsed time for attention_prob_times_values (512x2048x2048x77): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x77): 48.505
Elapsed time for attention_linear_projection (4x9856x9856, b=2048): 0.0066
Throughput (in TFLOP/s) for attention_linear_projection (4x9856x9856, b=2048): 242.638
Elapsed time for mlp_h_to_4h (4x9856x39424, b=2048): 0.0257
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9856x39424, b=2048): 247.386
Elapsed time for mlp_4h_to_h (4x39424x9856, b=2048): 0.0263
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39424x9856, b=2048): 242.297

Attention duration (in seconds): 0.0439
Attention throughput (in TFLOP/s): 160.058
MLP duration (in seconds): 0.0520
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0959
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9984x29952, b=2048): 0.0198
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9984x29952, b=2048): 247.098
Elapsed time for attention_key_query_prob (512x2048x78x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x78x2048): 53.657
Elapsed time for attention_prob_times_values (512x2048x2048x78): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x78): 80.898
Elapsed time for attention_linear_projection (4x9984x9984, b=2048): 0.0068
Throughput (in TFLOP/s) for attention_linear_projection (4x9984x9984, b=2048): 239.627
Elapsed time for mlp_h_to_4h (4x9984x39936, b=2048): 0.0264
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9984x39936, b=2048): 247.754
Elapsed time for mlp_4h_to_h (4x39936x9984, b=2048): 0.0259
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39936x9984, b=2048): 252.208

Attention duration (in seconds): 0.0370
Attention throughput (in TFLOP/s): 194.519
MLP duration (in seconds): 0.0523
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0893
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10112x30336, b=2048): 0.0203
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10112x30336, b=2048): 247.342
Elapsed time for attention_key_query_prob (512x2048x79x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x79x2048): 30.173
Elapsed time for attention_prob_times_values (512x2048x2048x79): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x79): 49.389
Elapsed time for attention_linear_projection (4x10112x10112, b=2048): 0.0071
Throughput (in TFLOP/s) for attention_linear_projection (4x10112x10112, b=2048): 237.615
Elapsed time for mlp_h_to_4h (4x10112x40448, b=2048): 0.0270
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10112x40448, b=2048): 247.765
Elapsed time for mlp_4h_to_h (4x40448x10112, b=2048): 0.0274
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40448x10112, b=2048): 244.281

Attention duration (in seconds): 0.0455
Attention throughput (in TFLOP/s): 162.246
MLP duration (in seconds): 0.0545
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10240x30720, b=2048): 0.0206
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10240x30720, b=2048): 250.612
Elapsed time for attention_key_query_prob (512x2048x80x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x80x2048): 78.362
Elapsed time for attention_prob_times_values (512x2048x2048x80): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x80): 93.935
Elapsed time for attention_linear_projection (4x10240x10240, b=2048): 0.0072
Throughput (in TFLOP/s) for attention_linear_projection (4x10240x10240, b=2048): 239.840
Elapsed time for mlp_h_to_4h (4x10240x40960, b=2048): 0.0274
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10240x40960, b=2048): 251.002
Elapsed time for mlp_4h_to_h (4x40960x10240, b=2048): 0.0273
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40960x10240, b=2048): 251.416

Attention duration (in seconds): 0.0358
Attention throughput (in TFLOP/s): 211.320
MLP duration (in seconds): 0.0547
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0905
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10368x31104, b=2048): 0.0211
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10368x31104, b=2048): 250.351
Elapsed time for attention_key_query_prob (512x2048x81x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x81x2048): 30.701
Elapsed time for attention_prob_times_values (512x2048x2048x81): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x81): 50.344
Elapsed time for attention_linear_projection (4x10368x10368, b=2048): 0.0075
Throughput (in TFLOP/s) for attention_linear_projection (4x10368x10368, b=2048): 235.158
Elapsed time for mlp_h_to_4h (4x10368x41472, b=2048): 0.0284
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10368x41472, b=2048): 247.922
Elapsed time for mlp_4h_to_h (4x41472x10368, b=2048): 0.0289
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41472x10368, b=2048): 243.515

Attention duration (in seconds): 0.0468
Attention throughput (in TFLOP/s): 165.270
MLP duration (in seconds): 0.0573
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10496x31488, b=2048): 0.0218
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10496x31488, b=2048): 247.857
Elapsed time for attention_key_query_prob (512x2048x82x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x82x2048): 56.079
Elapsed time for attention_prob_times_values (512x2048x2048x82): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x82): 84.954
Elapsed time for attention_linear_projection (4x10496x10496, b=2048): 0.0076
Throughput (in TFLOP/s) for attention_linear_projection (4x10496x10496, b=2048): 237.597
Elapsed time for mlp_h_to_4h (4x10496x41984, b=2048): 0.0291
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10496x41984, b=2048): 248.286
Elapsed time for mlp_4h_to_h (4x41984x10496, b=2048): 0.0300
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41984x10496, b=2048): 240.788

Attention duration (in seconds): 0.0399
Attention throughput (in TFLOP/s): 198.755
MLP duration (in seconds): 0.0591
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0989
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10624x31872, b=2048): 0.0224
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10624x31872, b=2048): 248.049
Elapsed time for attention_key_query_prob (512x2048x83x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x83x2048): 31.364
Elapsed time for attention_prob_times_values (512x2048x2048x83): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x83): 51.742
Elapsed time for attention_linear_projection (4x10624x10624, b=2048): 0.0076
Throughput (in TFLOP/s) for attention_linear_projection (4x10624x10624, b=2048): 242.559
Elapsed time for mlp_h_to_4h (4x10624x42496, b=2048): 0.0302
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10624x42496, b=2048): 245.085
Elapsed time for mlp_4h_to_h (4x42496x10624, b=2048): 0.0307
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42496x10624, b=2048): 240.846

Attention duration (in seconds): 0.0482
Attention throughput (in TFLOP/s): 168.099
MLP duration (in seconds): 0.0609
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10752x32256, b=2048): 0.0229
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10752x32256, b=2048): 247.840
Elapsed time for attention_key_query_prob (512x2048x84x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x84x2048): 56.413
Elapsed time for attention_prob_times_values (512x2048x2048x84): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x84): 86.876
Elapsed time for attention_linear_projection (4x10752x10752, b=2048): 0.0079
Throughput (in TFLOP/s) for attention_linear_projection (4x10752x10752, b=2048): 238.619
Elapsed time for mlp_h_to_4h (4x10752x43008, b=2048): 0.0305
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10752x43008, b=2048): 248.395
Elapsed time for mlp_4h_to_h (4x43008x10752, b=2048): 0.0296
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43008x10752, b=2048): 256.315

Attention duration (in seconds): 0.0414
Attention throughput (in TFLOP/s): 200.370
MLP duration (in seconds): 0.0601
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10880x32640, b=2048): 0.0237
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10880x32640, b=2048): 245.226
Elapsed time for attention_key_query_prob (512x2048x85x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x85x2048): 31.729
Elapsed time for attention_prob_times_values (512x2048x2048x85): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x85): 53.085
Elapsed time for attention_linear_projection (4x10880x10880, b=2048): 0.0082
Throughput (in TFLOP/s) for attention_linear_projection (4x10880x10880, b=2048): 237.521
Elapsed time for mlp_h_to_4h (4x10880x43520, b=2048): 0.0315
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10880x43520, b=2048): 246.246
Elapsed time for mlp_4h_to_h (4x43520x10880, b=2048): 0.0323
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43520x10880, b=2048): 240.339

Attention duration (in seconds): 0.0503
Attention throughput (in TFLOP/s): 168.830
MLP duration (in seconds): 0.0638
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11008x33024, b=2048): 0.0243
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11008x33024, b=2048): 245.359
Elapsed time for attention_key_query_prob (512x2048x86x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x86x2048): 58.319
Elapsed time for attention_prob_times_values (512x2048x2048x86): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x86): 88.614
Elapsed time for attention_linear_projection (4x11008x11008, b=2048): 0.0085
Throughput (in TFLOP/s) for attention_linear_projection (4x11008x11008, b=2048): 233.496
Elapsed time for mlp_h_to_4h (4x11008x44032, b=2048): 0.0319
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11008x44032, b=2048): 248.668
Elapsed time for mlp_4h_to_h (4x44032x11008, b=2048): 0.0313
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44032x11008, b=2048): 253.494

Attention duration (in seconds): 0.0433
Attention throughput (in TFLOP/s): 200.560
MLP duration (in seconds): 0.0633
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11136x33408, b=2048): 0.0248
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11136x33408, b=2048): 245.723
Elapsed time for attention_key_query_prob (512x2048x87x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x87x2048): 31.947
Elapsed time for attention_prob_times_values (512x2048x2048x87): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x87): 53.556
Elapsed time for attention_linear_projection (4x11136x11136, b=2048): 0.0084
Throughput (in TFLOP/s) for attention_linear_projection (4x11136x11136, b=2048): 241.682
Elapsed time for mlp_h_to_4h (4x11136x44544, b=2048): 0.0326
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11136x44544, b=2048): 249.050
Elapsed time for mlp_4h_to_h (4x44544x11136, b=2048): 0.0339
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44544x11136, b=2048): 239.975

Attention duration (in seconds): 0.0519
Attention throughput (in TFLOP/s): 171.038
MLP duration (in seconds): 0.0665
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11264x33792, b=2048): 0.0254
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11264x33792, b=2048): 245.736
Elapsed time for attention_key_query_prob (512x2048x88x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x88x2048): 77.168
Elapsed time for attention_prob_times_values (512x2048x2048x88): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x88): 101.887
Elapsed time for attention_linear_projection (4x11264x11264, b=2048): 0.0086
Throughput (in TFLOP/s) for attention_linear_projection (4x11264x11264, b=2048): 240.649
Elapsed time for mlp_h_to_4h (4x11264x45056, b=2048): 0.0333
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11264x45056, b=2048): 249.457
Elapsed time for mlp_4h_to_h (4x45056x11264, b=2048): 0.0325
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45056x11264, b=2048): 255.810

Attention duration (in seconds): 0.0426
Attention throughput (in TFLOP/s): 212.816
MLP duration (in seconds): 0.0658
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11392x34176, b=2048): 0.0256
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11392x34176, b=2048): 249.002
Elapsed time for attention_key_query_prob (512x2048x89x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x89x2048): 32.578
Elapsed time for attention_prob_times_values (512x2048x2048x89): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x89): 54.875
Elapsed time for attention_linear_projection (4x11392x11392, b=2048): 0.0090
Throughput (in TFLOP/s) for attention_linear_projection (4x11392x11392, b=2048): 236.847
Elapsed time for mlp_h_to_4h (4x11392x45568, b=2048): 0.0346
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11392x45568, b=2048): 246.082
Elapsed time for mlp_4h_to_h (4x45568x11392, b=2048): 0.0338
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45568x11392, b=2048): 251.310

Attention duration (in seconds): 0.0533
Attention throughput (in TFLOP/s): 173.933
MLP duration (in seconds): 0.0684
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11520x34560, b=2048): 0.0264
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11520x34560, b=2048): 246.763
Elapsed time for attention_key_query_prob (512x2048x90x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x90x2048): 59.219
Elapsed time for attention_prob_times_values (512x2048x2048x90): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x90): 92.503
Elapsed time for attention_linear_projection (4x11520x11520, b=2048): 0.0091
Throughput (in TFLOP/s) for attention_linear_projection (4x11520x11520, b=2048): 238.719
Elapsed time for mlp_h_to_4h (4x11520x46080, b=2048): 0.0352
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11520x46080, b=2048): 247.186
Elapsed time for mlp_4h_to_h (4x46080x11520, b=2048): 0.0360
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46080x11520, b=2048): 241.422

Attention duration (in seconds): 0.0462
Attention throughput (in TFLOP/s): 204.771
MLP duration (in seconds): 0.0712
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11648x34944, b=2048): 0.0268
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11648x34944, b=2048): 249.278
Elapsed time for attention_key_query_prob (512x2048x91x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x91x2048): 33.198
Elapsed time for attention_prob_times_values (512x2048x2048x91): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x91): 56.462
Elapsed time for attention_linear_projection (4x11648x11648, b=2048): 0.0094
Throughput (in TFLOP/s) for attention_linear_projection (4x11648x11648, b=2048): 237.145
Elapsed time for mlp_h_to_4h (4x11648x46592, b=2048): 0.0356
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11648x46592, b=2048): 249.525
Elapsed time for mlp_4h_to_h (4x46592x11648, b=2048): 0.0369
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46592x11648, b=2048): 241.245

Attention duration (in seconds): 0.0548
Attention throughput (in TFLOP/s): 176.452
MLP duration (in seconds): 0.0725
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11776x35328, b=2048): 0.0275
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11776x35328, b=2048): 248.014
Elapsed time for attention_key_query_prob (512x2048x92x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x92x2048): 61.253
Elapsed time for attention_prob_times_values (512x2048x2048x92): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x92): 94.461
Elapsed time for attention_linear_projection (4x11776x11776, b=2048): 0.0095
Throughput (in TFLOP/s) for attention_linear_projection (4x11776x11776, b=2048): 239.697
Elapsed time for mlp_h_to_4h (4x11776x47104, b=2048): 0.0364
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11776x47104, b=2048): 249.746
Elapsed time for mlp_4h_to_h (4x47104x11776, b=2048): 0.0358
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47104x11776, b=2048): 254.064

Attention duration (in seconds): 0.0476
Attention throughput (in TFLOP/s): 207.550
MLP duration (in seconds): 0.0722
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11904x35712, b=2048): 0.0285
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11904x35712, b=2048): 244.352
Elapsed time for attention_key_query_prob (512x2048x93x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x93x2048): 33.787
Elapsed time for attention_prob_times_values (512x2048x2048x93): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x93): 57.611
Elapsed time for attention_linear_projection (4x11904x11904, b=2048): 0.0097
Throughput (in TFLOP/s) for attention_linear_projection (4x11904x11904, b=2048): 240.407
Elapsed time for mlp_h_to_4h (4x11904x47616, b=2048): 0.0376
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11904x47616, b=2048): 246.729
Elapsed time for mlp_4h_to_h (4x47616x11904, b=2048): 0.0381
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47616x11904, b=2048): 243.629

Attention duration (in seconds): 0.0569
Attention throughput (in TFLOP/s): 177.199
MLP duration (in seconds): 0.0758
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1327
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12032x36096, b=2048): 0.0279
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12032x36096, b=2048): 255.152
Elapsed time for attention_key_query_prob (512x2048x94x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x94x2048): 63.701
Elapsed time for attention_prob_times_values (512x2048x2048x94): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x94): 96.088
Elapsed time for attention_linear_projection (4x12032x12032, b=2048): 0.0096
Throughput (in TFLOP/s) for attention_linear_projection (4x12032x12032, b=2048): 247.277
Elapsed time for mlp_h_to_4h (4x12032x48128, b=2048): 0.0373
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12032x48128, b=2048): 254.433
Elapsed time for mlp_4h_to_h (4x48128x12032, b=2048): 0.0376
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48128x12032, b=2048): 252.113

Attention duration (in seconds): 0.0480
Attention throughput (in TFLOP/s): 214.392
MLP duration (in seconds): 0.0749
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12160x36480, b=2048): 0.0286
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12160x36480, b=2048): 254.389
Elapsed time for attention_key_query_prob (512x2048x95x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x95x2048): 35.280
Elapsed time for attention_prob_times_values (512x2048x2048x95): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x95): 58.418
Elapsed time for attention_linear_projection (4x12160x12160, b=2048): 0.0103
Throughput (in TFLOP/s) for attention_linear_projection (4x12160x12160, b=2048): 235.394
Elapsed time for mlp_h_to_4h (4x12160x48640, b=2048): 0.0370
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12160x48640, b=2048): 261.611
Elapsed time for mlp_4h_to_h (4x48640x12160, b=2048): 0.0398
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48640x12160, b=2048): 243.563

Attention duration (in seconds): 0.0574
Attention throughput (in TFLOP/s): 183.005
MLP duration (in seconds): 0.0768
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1342
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12288x36864, b=2048): 0.0294
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12288x36864, b=2048): 252.235
Elapsed time for attention_key_query_prob (512x2048x96x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x96x2048): 95.314
Elapsed time for attention_prob_times_values (512x2048x2048x96): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x96): 110.766
Elapsed time for attention_linear_projection (4x12288x12288, b=2048): 0.0100
Throughput (in TFLOP/s) for attention_linear_projection (4x12288x12288, b=2048): 247.390
Elapsed time for mlp_h_to_4h (4x12288x49152, b=2048): 0.0391
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12288x49152, b=2048): 253.351
Elapsed time for mlp_4h_to_h (4x49152x12288, b=2048): 0.0391
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49152x12288, b=2048): 253.303

Attention duration (in seconds): 0.0475
Attention throughput (in TFLOP/s): 225.822
MLP duration (in seconds): 0.0781
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12416x37248, b=2048): 0.0298
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12416x37248, b=2048): 254.376
Elapsed time for attention_key_query_prob (512x2048x97x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x97x2048): 32.098
Elapsed time for attention_prob_times_values (512x2048x2048x97): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x97): 59.620
Elapsed time for attention_linear_projection (4x12416x12416, b=2048): 0.0102
Throughput (in TFLOP/s) for attention_linear_projection (4x12416x12416, b=2048): 246.799
Elapsed time for mlp_h_to_4h (4x12416x49664, b=2048): 0.0395
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12416x49664, b=2048): 255.601
Elapsed time for mlp_4h_to_h (4x49664x12416, b=2048): 0.0415
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49664x12416, b=2048): 243.432

Attention duration (in seconds): 0.0600
Attention throughput (in TFLOP/s): 182.304
MLP duration (in seconds): 0.0810
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1410
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12544x37632, b=2048): 0.0303
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12544x37632, b=2048): 255.483
Elapsed time for attention_key_query_prob (512x2048x98x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x98x2048): 59.979
Elapsed time for attention_prob_times_values (512x2048x2048x98): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x98): 96.566
Elapsed time for attention_linear_projection (4x12544x12544, b=2048): 0.0113
Throughput (in TFLOP/s) for attention_linear_projection (4x12544x12544, b=2048): 229.077
Elapsed time for mlp_h_to_4h (4x12544x50176, b=2048): 0.0395
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12544x50176, b=2048): 260.924
Elapsed time for mlp_4h_to_h (4x50176x12544, b=2048): 0.0413
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50176x12544, b=2048): 249.455

Attention duration (in seconds): 0.0529
Attention throughput (in TFLOP/s): 210.839
MLP duration (in seconds): 0.0809
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1338
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12672x38016, b=2048): 0.0311
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12672x38016, b=2048): 254.035
Elapsed time for attention_key_query_prob (512x2048x99x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x99x2048): 31.470
Elapsed time for attention_prob_times_values (512x2048x2048x99): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x99): 60.982
Elapsed time for attention_linear_projection (4x12672x12672, b=2048): 0.0105
Throughput (in TFLOP/s) for attention_linear_projection (4x12672x12672, b=2048): 250.362
Elapsed time for mlp_h_to_4h (4x12672x50688, b=2048): 0.0408
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12672x50688, b=2048): 258.218
Elapsed time for mlp_4h_to_h (4x50688x12672, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50688x12672, b=2048): 241.939

Attention duration (in seconds): 0.0621
Attention throughput (in TFLOP/s): 183.271
MLP duration (in seconds): 0.0843
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1463
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12800x38400, b=2048): 0.0311
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12800x38400, b=2048): 258.746
Elapsed time for attention_key_query_prob (512x2048x100x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x100x2048): 60.911
Elapsed time for attention_prob_times_values (512x2048x2048x100): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x100): 101.159
Elapsed time for attention_linear_projection (4x12800x12800, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x12800x12800, b=2048): 249.385
Elapsed time for mlp_h_to_4h (4x12800x51200, b=2048): 0.0415
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12800x51200, b=2048): 259.036
Elapsed time for mlp_4h_to_h (4x51200x12800, b=2048): 0.0421
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51200x12800, b=2048): 255.036

Attention duration (in seconds): 0.0532
Attention throughput (in TFLOP/s): 218.042
MLP duration (in seconds): 0.0836
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1367
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12928x38784, b=2048): 0.0322
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12928x38784, b=2048): 255.182
Elapsed time for attention_key_query_prob (512x2048x101x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x101x2048): 31.406
Elapsed time for attention_prob_times_values (512x2048x2048x101): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x101): 62.370
Elapsed time for attention_linear_projection (4x12928x12928, b=2048): 0.0112
Throughput (in TFLOP/s) for attention_linear_projection (4x12928x12928, b=2048): 245.119
Elapsed time for mlp_h_to_4h (4x12928x51712, b=2048): 0.0428
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12928x51712, b=2048): 255.625
Elapsed time for mlp_4h_to_h (4x51712x12928, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51712x12928, b=2048): 254.673

Attention duration (in seconds): 0.0641
Attention throughput (in TFLOP/s): 184.322
MLP duration (in seconds): 0.0859
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1500
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0328
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 255.284
Elapsed time for attention_key_query_prob (512x2048x102x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x102x2048): 62.083
Elapsed time for attention_prob_times_values (512x2048x2048x102): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x102): 103.876
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0111
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 251.765
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.992
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0438
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 254.822

Attention duration (in seconds): 0.0552
Attention throughput (in TFLOP/s): 218.302
MLP duration (in seconds): 0.0870
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1422
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13184x39552, b=2048): 0.0335
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13184x39552, b=2048): 254.988
Elapsed time for attention_key_query_prob (512x2048x103x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x103x2048): 30.760
Elapsed time for attention_prob_times_values (512x2048x2048x103): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x103): 60.979
Elapsed time for attention_linear_projection (4x13184x13184, b=2048): 0.0114
Throughput (in TFLOP/s) for attention_linear_projection (4x13184x13184, b=2048): 248.759
Elapsed time for mlp_h_to_4h (4x13184x52736, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13184x52736, b=2048): 261.113
Elapsed time for mlp_4h_to_h (4x52736x13184, b=2048): 0.0452
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52736x13184, b=2048): 252.042

Attention duration (in seconds): 0.0666
Attention throughput (in TFLOP/s): 184.353
MLP duration (in seconds): 0.0888
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1554
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13312x39936, b=2048): 0.0342
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13312x39936, b=2048): 254.981
Elapsed time for attention_key_query_prob (512x2048x104x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x104x2048): 85.536
Elapsed time for attention_prob_times_values (512x2048x2048x104): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x104): 118.001
Elapsed time for attention_linear_projection (4x13312x13312, b=2048): 0.0119
Throughput (in TFLOP/s) for attention_linear_projection (4x13312x13312, b=2048): 244.946
Elapsed time for mlp_h_to_4h (4x13312x53248, b=2048): 0.0449
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13312x53248, b=2048): 258.504
Elapsed time for mlp_4h_to_h (4x53248x13312, b=2048): 0.0451
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53248x13312, b=2048): 257.392

Attention duration (in seconds): 0.0550
Attention throughput (in TFLOP/s): 227.313
MLP duration (in seconds): 0.0900
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1451
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13440x40320, b=2048): 0.0349
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13440x40320, b=2048): 254.574
Elapsed time for attention_key_query_prob (512x2048x105x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x105x2048): 31.458
Elapsed time for attention_prob_times_values (512x2048x2048x105): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x105): 63.889
Elapsed time for attention_linear_projection (4x13440x13440, b=2048): 0.0117
Throughput (in TFLOP/s) for attention_linear_projection (4x13440x13440, b=2048): 251.975
Elapsed time for mlp_h_to_4h (4x13440x53760, b=2048): 0.0465
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13440x53760, b=2048): 254.636
Elapsed time for mlp_4h_to_h (4x53760x13440, b=2048): 0.0465
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53760x13440, b=2048): 254.436

Attention duration (in seconds): 0.0680
Attention throughput (in TFLOP/s): 187.310
MLP duration (in seconds): 0.0930
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1610
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13568x40704, b=2048): 0.0351
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13568x40704, b=2048): 257.561
Elapsed time for attention_key_query_prob (512x2048x106x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x106x2048): 64.063
Elapsed time for attention_prob_times_values (512x2048x2048x106): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x106): 107.804
Elapsed time for attention_linear_projection (4x13568x13568, b=2048): 0.0123
Throughput (in TFLOP/s) for attention_linear_projection (4x13568x13568, b=2048): 245.286
Elapsed time for mlp_h_to_4h (4x13568x54272, b=2048): 0.0470
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13568x54272, b=2048): 256.460
Elapsed time for mlp_4h_to_h (4x54272x13568, b=2048): 0.0481
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54272x13568, b=2048): 250.620

Attention duration (in seconds): 0.0588
Attention throughput (in TFLOP/s): 220.825
MLP duration (in seconds): 0.0952
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1539
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13696x41088, b=2048): 0.0362
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13696x41088, b=2048): 254.738
Elapsed time for attention_key_query_prob (512x2048x107x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x107x2048): 32.046
Elapsed time for attention_prob_times_values (512x2048x2048x107): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x107): 66.043
Elapsed time for attention_linear_projection (4x13696x13696, b=2048): 0.0126
Throughput (in TFLOP/s) for attention_linear_projection (4x13696x13696, b=2048): 244.424
Elapsed time for mlp_h_to_4h (4x13696x54784, b=2048): 0.0482
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13696x54784, b=2048): 254.906
Elapsed time for mlp_4h_to_h (4x54784x13696, b=2048): 0.0485
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54784x13696, b=2048): 253.447

Attention duration (in seconds): 0.0701
Attention throughput (in TFLOP/s): 188.569
MLP duration (in seconds): 0.0967
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1668
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13824x41472, b=2048): 0.0362
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13824x41472, b=2048): 259.602
Elapsed time for attention_key_query_prob (512x2048x108x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x108x2048): 65.132
Elapsed time for attention_prob_times_values (512x2048x2048x108): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x108): 109.566
Elapsed time for attention_linear_projection (4x13824x13824, b=2048): 0.0128
Throughput (in TFLOP/s) for attention_linear_projection (4x13824x13824, b=2048): 244.067
Elapsed time for mlp_h_to_4h (4x13824x55296, b=2048): 0.0482
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13824x55296, b=2048): 259.725
Elapsed time for mlp_4h_to_h (4x55296x13824, b=2048): 0.0490
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55296x13824, b=2048): 255.461

Attention duration (in seconds): 0.0604
Attention throughput (in TFLOP/s): 222.836
MLP duration (in seconds): 0.0972
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1576
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13952x41856, b=2048): 0.0369
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13952x41856, b=2048): 259.385
Elapsed time for attention_key_query_prob (512x2048x109x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x109x2048): 32.655
Elapsed time for attention_prob_times_values (512x2048x2048x109): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x109): 67.193
Elapsed time for attention_linear_projection (4x13952x13952, b=2048): 0.0130
Throughput (in TFLOP/s) for attention_linear_projection (4x13952x13952, b=2048): 244.925
Elapsed time for mlp_h_to_4h (4x13952x55808, b=2048): 0.0493
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13952x55808, b=2048): 258.705
Elapsed time for mlp_4h_to_h (4x55808x13952, b=2048): 0.0503
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55808x13952, b=2048): 253.798

Attention duration (in seconds): 0.0712
Attention throughput (in TFLOP/s): 192.292
MLP duration (in seconds): 0.0996
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1708
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14080x42240, b=2048): 0.0380
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14080x42240, b=2048): 256.658
Elapsed time for attention_key_query_prob (512x2048x110x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x110x2048): 66.243
Elapsed time for attention_prob_times_values (512x2048x2048x110): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x110): 111.294
Elapsed time for attention_linear_projection (4x14080x14080, b=2048): 0.0131
Throughput (in TFLOP/s) for attention_linear_projection (4x14080x14080, b=2048): 247.047
Elapsed time for mlp_h_to_4h (4x14080x56320, b=2048): 0.0501
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14080x56320, b=2048): 259.342
Elapsed time for mlp_4h_to_h (4x56320x14080, b=2048): 0.0511
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56320x14080, b=2048): 254.130

Attention duration (in seconds): 0.0625
Attention throughput (in TFLOP/s): 223.029
MLP duration (in seconds): 0.1012
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1637
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14208x42624, b=2048): 0.0383
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14208x42624, b=2048): 258.998
Elapsed time for attention_key_query_prob (512x2048x111x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x111x2048): 32.980
Elapsed time for attention_prob_times_values (512x2048x2048x111): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x111): 67.361
Elapsed time for attention_linear_projection (4x14208x14208, b=2048): 0.0135
Throughput (in TFLOP/s) for attention_linear_projection (4x14208x14208, b=2048): 245.756
Elapsed time for mlp_h_to_4h (4x14208x56832, b=2048): 0.0513
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14208x56832, b=2048): 257.933
Elapsed time for mlp_4h_to_h (4x56832x14208, b=2048): 0.0528
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56832x14208, b=2048): 250.546

Attention duration (in seconds): 0.0733
Attention throughput (in TFLOP/s): 193.491
MLP duration (in seconds): 0.1041
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1774
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14336x43008, b=2048): 0.0390
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14336x43008, b=2048): 258.869
Elapsed time for attention_key_query_prob (512x2048x112x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x112x2048): 96.476
Elapsed time for attention_prob_times_values (512x2048x2048x112): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x112): 127.006
Elapsed time for attention_linear_projection (4x14336x14336, b=2048): 0.0134
Throughput (in TFLOP/s) for attention_linear_projection (4x14336x14336, b=2048): 250.737
Elapsed time for mlp_h_to_4h (4x14336x57344, b=2048): 0.0522
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14336x57344, b=2048): 258.256
Elapsed time for mlp_4h_to_h (4x57344x14336, b=2048): 0.0529
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57344x14336, b=2048): 254.423

Attention duration (in seconds): 0.0612
Attention throughput (in TFLOP/s): 235.703
MLP duration (in seconds): 0.1051
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1663
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14464x43392, b=2048): 0.0396
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14464x43392, b=2048): 259.604
Elapsed time for attention_key_query_prob (512x2048x113x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x113x2048): 33.582
Elapsed time for attention_prob_times_values (512x2048x2048x113): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x113): 68.436
Elapsed time for attention_linear_projection (4x14464x14464, b=2048): 0.0139
Throughput (in TFLOP/s) for attention_linear_projection (4x14464x14464, b=2048): 246.906
Elapsed time for mlp_h_to_4h (4x14464x57856, b=2048): 0.0528
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14464x57856, b=2048): 259.784
Elapsed time for mlp_4h_to_h (4x57856x14464, b=2048): 0.0552
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57856x14464, b=2048): 248.372

Attention duration (in seconds): 0.0750
Attention throughput (in TFLOP/s): 195.656
MLP duration (in seconds): 0.1080
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1830
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14592x43776, b=2048): 0.0401
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14592x43776, b=2048): 260.988
Elapsed time for attention_key_query_prob (512x2048x114x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x114x2048): 68.227
Elapsed time for attention_prob_times_values (512x2048x2048x114): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x114): 114.633
Elapsed time for attention_linear_projection (4x14592x14592, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_linear_projection (4x14592x14592, b=2048): 242.263
Elapsed time for mlp_h_to_4h (4x14592x58368, b=2048): 0.0539
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14592x58368, b=2048): 258.812
Elapsed time for mlp_4h_to_h (4x58368x14592, b=2048): 0.0555
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58368x14592, b=2048): 251.207

Attention duration (in seconds): 0.0659
Attention throughput (in TFLOP/s): 226.444
MLP duration (in seconds): 0.1095
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1754
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14720x44160, b=2048): 0.0415
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14720x44160, b=2048): 256.857
Elapsed time for attention_key_query_prob (512x2048x115x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x115x2048): 34.444
Elapsed time for attention_prob_times_values (512x2048x2048x115): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x115): 70.642
Elapsed time for attention_linear_projection (4x14720x14720, b=2048): 0.0140
Throughput (in TFLOP/s) for attention_linear_projection (4x14720x14720, b=2048): 253.245
Elapsed time for mlp_h_to_4h (4x14720x58880, b=2048): 0.0548
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14720x58880, b=2048): 259.223
Elapsed time for mlp_4h_to_h (4x58880x14720, b=2048): 0.0581
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58880x14720, b=2048): 244.513

Attention duration (in seconds): 0.0768
Attention throughput (in TFLOP/s): 197.727
MLP duration (in seconds): 0.1129
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1897
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14848x44544, b=2048): 0.0414
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14848x44544, b=2048): 261.484
Elapsed time for attention_key_query_prob (512x2048x116x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x116x2048): 69.635
Elapsed time for attention_prob_times_values (512x2048x2048x116): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x116): 115.835
Elapsed time for attention_linear_projection (4x14848x14848, b=2048): 0.0147
Throughput (in TFLOP/s) for attention_linear_projection (4x14848x14848, b=2048): 245.955
Elapsed time for mlp_h_to_4h (4x14848x59392, b=2048): 0.0565
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14848x59392, b=2048): 255.846
Elapsed time for mlp_4h_to_h (4x59392x14848, b=2048): 0.0583
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59392x14848, b=2048): 247.646

Attention duration (in seconds): 0.0676
Attention throughput (in TFLOP/s): 228.530
MLP duration (in seconds): 0.1148
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1824
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14976x44928, b=2048): 0.0423
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14976x44928, b=2048): 260.329
Elapsed time for attention_key_query_prob (512x2048x117x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x117x2048): 35.259
Elapsed time for attention_prob_times_values (512x2048x2048x117): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x117): 71.622
Elapsed time for attention_linear_projection (4x14976x14976, b=2048): 0.0148
Throughput (in TFLOP/s) for attention_linear_projection (4x14976x14976, b=2048): 247.800
Elapsed time for mlp_h_to_4h (4x14976x59904, b=2048): 0.0566
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14976x59904, b=2048): 259.576
Elapsed time for mlp_4h_to_h (4x59904x14976, b=2048): 0.0700
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59904x14976, b=2048): 210.124

Attention duration (in seconds): 0.0784
Attention throughput (in TFLOP/s): 200.190
MLP duration (in seconds): 0.1266
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15104x45312, b=2048): 0.0427
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15104x45312, b=2048): 262.720
Elapsed time for attention_key_query_prob (512x2048x118x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x118x2048): 66.100
Elapsed time for attention_prob_times_values (512x2048x2048x118): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x118): 112.799
Elapsed time for attention_linear_projection (4x15104x15104, b=2048): 0.0151
Throughput (in TFLOP/s) for attention_linear_projection (4x15104x15104, b=2048): 247.182
Elapsed time for mlp_h_to_4h (4x15104x60416, b=2048): 0.0588
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15104x60416, b=2048): 254.256
Elapsed time for mlp_4h_to_h (4x60416x15104, b=2048): 0.0594
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60416x15104, b=2048): 251.699

Attention duration (in seconds): 0.0700
Attention throughput (in TFLOP/s): 228.186
MLP duration (in seconds): 0.1182
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1882
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15232x45696, b=2048): 0.0448
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15232x45696, b=2048): 254.590
Elapsed time for attention_key_query_prob (512x2048x119x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x119x2048): 34.134
Elapsed time for attention_prob_times_values (512x2048x2048x119): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x119): 71.493
Elapsed time for attention_linear_projection (4x15232x15232, b=2048): 0.0151
Throughput (in TFLOP/s) for attention_linear_projection (4x15232x15232, b=2048): 251.846
Elapsed time for mlp_h_to_4h (4x15232x60928, b=2048): 0.0588
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15232x60928, b=2048): 258.780
Elapsed time for mlp_4h_to_h (4x60928x15232, b=2048): 0.0606
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60928x15232, b=2048): 251.041

Attention duration (in seconds): 0.0820
Attention throughput (in TFLOP/s): 197.873
MLP duration (in seconds): 0.1193
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2013
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15360x46080, b=2048): 0.0825
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15360x46080, b=2048): 140.617
Elapsed time for attention_key_query_prob (512x2048x120x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x120x2048): 92.938
Elapsed time for attention_prob_times_values (512x2048x2048x120): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x120): 128.835
Elapsed time for attention_linear_projection (4x15360x15360, b=2048): 0.0156
Throughput (in TFLOP/s) for attention_linear_projection (4x15360x15360, b=2048): 247.043
Elapsed time for mlp_h_to_4h (4x15360x61440, b=2048): 0.0608
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15360x61440, b=2048): 254.263
Elapsed time for mlp_4h_to_h (4x61440x15360, b=2048): 0.0618
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61440x15360, b=2048): 250.040

Attention duration (in seconds): 0.1077
Attention throughput (in TFLOP/s): 153.190
MLP duration (in seconds): 0.1226
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15488x46464, b=2048): 0.0458
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15488x46464, b=2048): 257.578
Elapsed time for attention_key_query_prob (512x2048x121x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x121x2048): 34.541
Elapsed time for attention_prob_times_values (512x2048x2048x121): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x121): 72.794
Elapsed time for attention_linear_projection (4x15488x15488, b=2048): 0.0161
Throughput (in TFLOP/s) for attention_linear_projection (4x15488x15488, b=2048): 243.476
Elapsed time for mlp_h_to_4h (4x15488x61952, b=2048): 0.0613
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15488x61952, b=2048): 256.434
Elapsed time for mlp_4h_to_h (4x61952x15488, b=2048): 0.0625
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61952x15488, b=2048): 251.563

Attention duration (in seconds): 0.0841
Attention throughput (in TFLOP/s): 199.284
MLP duration (in seconds): 0.1238
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15616x46848, b=2048): 0.0463
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15616x46848, b=2048): 258.980
Elapsed time for attention_key_query_prob (512x2048x122x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x122x2048): 72.945
Elapsed time for attention_prob_times_values (512x2048x2048x122): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x122): 122.199
Elapsed time for attention_linear_projection (4x15616x15616, b=2048): 0.0159
Throughput (in TFLOP/s) for attention_linear_projection (4x15616x15616, b=2048): 250.694
Elapsed time for mlp_h_to_4h (4x15616x62464, b=2048): 0.0617
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15616x62464, b=2048): 258.865
Elapsed time for mlp_4h_to_h (4x62464x15616, b=2048): 0.0649
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62464x15616, b=2048): 246.083

Attention duration (in seconds): 0.0737
Attention throughput (in TFLOP/s): 231.094
MLP duration (in seconds): 0.1267
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2004
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15744x47232, b=2048): 0.0477
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15744x47232, b=2048): 255.611
Elapsed time for attention_key_query_prob (512x2048x123x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x123x2048): 36.656
Elapsed time for attention_prob_times_values (512x2048x2048x123): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x123): 74.900
Elapsed time for attention_linear_projection (4x15744x15744, b=2048): 0.0164
Throughput (in TFLOP/s) for attention_linear_projection (4x15744x15744, b=2048): 246.880
Elapsed time for mlp_h_to_4h (4x15744x62976, b=2048): 0.0625
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15744x62976, b=2048): 259.721
Elapsed time for mlp_4h_to_h (4x62976x15744, b=2048): 0.0655
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62976x15744, b=2048): 248.082

Attention duration (in seconds): 0.0856
Attention throughput (in TFLOP/s): 202.167
MLP duration (in seconds): 0.1280
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15872x47616, b=2048): 0.0544
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15872x47616, b=2048): 227.487
Elapsed time for attention_key_query_prob (512x2048x124x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x124x2048): 74.686
Elapsed time for attention_prob_times_values (512x2048x2048x124): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x124): 124.438
Elapsed time for attention_linear_projection (4x15872x15872, b=2048): 0.0168
Throughput (in TFLOP/s) for attention_linear_projection (4x15872x15872, b=2048): 245.959
Elapsed time for mlp_h_to_4h (4x15872x63488, b=2048): 0.0644
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15872x63488, b=2048): 256.179
Elapsed time for mlp_4h_to_h (4x63488x15872, b=2048): 0.0663
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63488x15872, b=2048): 249.086

Attention duration (in seconds): 0.0826
Attention throughput (in TFLOP/s): 212.713
MLP duration (in seconds): 0.1307
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16000x48000, b=2048): 0.0488
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16000x48000, b=2048): 258.086
Elapsed time for attention_key_query_prob (512x2048x125x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x125x2048): 37.410
Elapsed time for attention_prob_times_values (512x2048x2048x125): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x125): 76.151
Elapsed time for attention_linear_projection (4x16000x16000, b=2048): 0.0165
Throughput (in TFLOP/s) for attention_linear_projection (4x16000x16000, b=2048): 253.764
Elapsed time for mlp_h_to_4h (4x16000x64000, b=2048): 0.0648
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16000x64000, b=2048): 258.977
Elapsed time for mlp_4h_to_h (4x64000x16000, b=2048): 0.0680
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64000x16000, b=2048): 246.801

Attention duration (in seconds): 0.0867
Attention throughput (in TFLOP/s): 205.931
MLP duration (in seconds): 0.1328
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16128x48384, b=2048): 0.0491
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16128x48384, b=2048): 260.227
Elapsed time for attention_key_query_prob (512x2048x126x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x126x2048): 75.575
Elapsed time for attention_prob_times_values (512x2048x2048x126): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x126): 126.311
Elapsed time for attention_linear_projection (4x16128x16128, b=2048): 0.0171
Throughput (in TFLOP/s) for attention_linear_projection (4x16128x16128, b=2048): 249.512
Elapsed time for mlp_h_to_4h (4x16128x64512, b=2048): 0.0660
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16128x64512, b=2048): 258.279
Elapsed time for mlp_4h_to_h (4x64512x16128, b=2048): 0.0702
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64512x16128, b=2048): 242.707

Attention duration (in seconds): 0.0777
Attention throughput (in TFLOP/s): 233.455
MLP duration (in seconds): 0.1362
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16256x48768, b=2048): 0.0506
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16256x48768, b=2048): 256.821
Elapsed time for attention_key_query_prob (512x2048x127x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x127x2048): 36.868
Elapsed time for attention_prob_times_values (512x2048x2048x127): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x127): 76.421
Elapsed time for attention_linear_projection (4x16256x16256, b=2048): 0.0177
Throughput (in TFLOP/s) for attention_linear_projection (4x16256x16256, b=2048): 245.212
Elapsed time for mlp_h_to_4h (4x16256x65024, b=2048): 0.0683
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16256x65024, b=2048): 253.385
Elapsed time for mlp_4h_to_h (4x65024x16256, b=2048): 0.0722
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65024x16256, b=2048): 240.025

Attention duration (in seconds): 0.0902
Attention throughput (in TFLOP/s): 204.175
MLP duration (in seconds): 0.1405
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16384x49152, b=2048): 0.0503
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16384x49152, b=2048): 262.519
Elapsed time for attention_key_query_prob (512x2048x128x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x128x2048): 112.976
Elapsed time for attention_prob_times_values (512x2048x2048x128): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x128): 144.043
Elapsed time for attention_linear_projection (4x16384x16384, b=2048): 0.0178
Throughput (in TFLOP/s) for attention_linear_projection (4x16384x16384, b=2048): 246.809
Elapsed time for mlp_h_to_4h (4x16384x65536, b=2048): 0.0677
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16384x65536, b=2048): 259.761
Elapsed time for mlp_4h_to_h (4x65536x16384, b=2048): 0.0707
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65536x16384, b=2048): 248.700

Attention duration (in seconds): 0.0768
Attention throughput (in TFLOP/s): 243.501
MLP duration (in seconds): 0.1385
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16512x49536, b=2048): 0.0516
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16512x49536, b=2048): 259.788
Elapsed time for attention_key_query_prob (512x2048x129x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x129x2048): 34.239
Elapsed time for attention_prob_times_values (512x2048x2048x129): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x129): 55.861
Elapsed time for attention_linear_projection (4x16512x16512, b=2048): 0.0179
Throughput (in TFLOP/s) for attention_linear_projection (4x16512x16512, b=2048): 249.101
Elapsed time for mlp_h_to_4h (4x16512x66048, b=2048): 0.0686
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16512x66048, b=2048): 260.372
Elapsed time for mlp_4h_to_h (4x66048x16512, b=2048): 0.0715
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66048x16512, b=2048): 249.980

Attention duration (in seconds): 0.0956
Attention throughput (in TFLOP/s): 198.459
MLP duration (in seconds): 0.1401
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2357
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16640x49920, b=2048): 0.0524
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16640x49920, b=2048): 259.488
Elapsed time for attention_key_query_prob (512x2048x130x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x130x2048): 72.060
Elapsed time for attention_prob_times_values (512x2048x2048x130): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x130): 96.548
Elapsed time for attention_linear_projection (4x16640x16640, b=2048): 0.0185
Throughput (in TFLOP/s) for attention_linear_projection (4x16640x16640, b=2048): 245.509
Elapsed time for mlp_h_to_4h (4x16640x66560, b=2048): 0.0700
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16640x66560, b=2048): 259.381
Elapsed time for mlp_4h_to_h (4x66560x16640, b=2048): 0.0730
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66560x16640, b=2048): 248.464

Attention duration (in seconds): 0.0845
Attention throughput (in TFLOP/s): 228.077
MLP duration (in seconds): 0.1430
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16768x50304, b=2048): 0.0533
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16768x50304, b=2048): 259.319
Elapsed time for attention_key_query_prob (512x2048x131x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x131x2048): 34.700
Elapsed time for attention_prob_times_values (512x2048x2048x131): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x131): 58.947
Elapsed time for attention_linear_projection (4x16768x16768, b=2048): 0.0187
Throughput (in TFLOP/s) for attention_linear_projection (4x16768x16768, b=2048): 246.940
Elapsed time for mlp_h_to_4h (4x16768x67072, b=2048): 0.0721
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16768x67072, b=2048): 255.741
Elapsed time for mlp_4h_to_h (4x67072x16768, b=2048): 0.0737
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67072x16768, b=2048): 250.100

Attention duration (in seconds): 0.0977
Attention throughput (in TFLOP/s): 200.106
MLP duration (in seconds): 0.1457
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2434
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16896x50688, b=2048): 0.0557
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16896x50688, b=2048): 251.741
Elapsed time for attention_key_query_prob (512x2048x132x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x132x2048): 72.632
Elapsed time for attention_prob_times_values (512x2048x2048x132): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x132): 95.794
Elapsed time for attention_linear_projection (4x16896x16896, b=2048): 0.0186
Throughput (in TFLOP/s) for attention_linear_projection (4x16896x16896, b=2048): 251.796
Elapsed time for mlp_h_to_4h (4x16896x67584, b=2048): 0.0791
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16896x67584, b=2048): 236.558
Elapsed time for mlp_4h_to_h (4x67584x16896, b=2048): 0.0749
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67584x16896, b=2048): 249.737

Attention duration (in seconds): 0.0880
Attention throughput (in TFLOP/s): 225.389
MLP duration (in seconds): 0.1540
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2420
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17024x51072, b=2048): 0.0549
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17024x51072, b=2048): 259.325
Elapsed time for attention_key_query_prob (512x2048x133x2048): 0.0164
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x133x2048): 34.739
Elapsed time for attention_prob_times_values (512x2048x2048x133): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x133): 59.501
Elapsed time for attention_linear_projection (4x17024x17024, b=2048): 0.0190
Throughput (in TFLOP/s) for attention_linear_projection (4x17024x17024, b=2048): 250.283
Elapsed time for mlp_h_to_4h (4x17024x68096, b=2048): 0.0731
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17024x68096, b=2048): 259.979
Elapsed time for mlp_4h_to_h (4x68096x17024, b=2048): 0.0765
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68096x17024, b=2048): 248.441

Attention duration (in seconds): 0.0999
Attention throughput (in TFLOP/s): 201.465
MLP duration (in seconds): 0.1495
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2495
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17152x51456, b=2048): 0.0558
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17152x51456, b=2048): 259.079
Elapsed time for attention_key_query_prob (512x2048x134x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x134x2048): 73.352
Elapsed time for attention_prob_times_values (512x2048x2048x134): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x134): 94.493
Elapsed time for attention_linear_projection (4x17152x17152, b=2048): 0.0196
Throughput (in TFLOP/s) for attention_linear_projection (4x17152x17152, b=2048): 245.996
Elapsed time for mlp_h_to_4h (4x17152x68608, b=2048): 0.0744
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17152x68608, b=2048): 259.188
Elapsed time for mlp_4h_to_h (4x68608x17152, b=2048): 0.0788
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68608x17152, b=2048): 244.651

Attention duration (in seconds): 0.0893
Attention throughput (in TFLOP/s): 228.680
MLP duration (in seconds): 0.1532
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2425
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17280x51840, b=2048): 0.0564
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17280x51840, b=2048): 260.412
Elapsed time for attention_key_query_prob (512x2048x135x2048): 0.0173
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x135x2048): 33.544
Elapsed time for attention_prob_times_values (512x2048x2048x135): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x135): 58.446
Elapsed time for attention_linear_projection (4x17280x17280, b=2048): 0.0198
Throughput (in TFLOP/s) for attention_linear_projection (4x17280x17280, b=2048): 247.438
Elapsed time for mlp_h_to_4h (4x17280x69120, b=2048): 0.0759
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17280x69120, b=2048): 257.942
Elapsed time for mlp_4h_to_h (4x69120x17280, b=2048): 0.0790
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69120x17280, b=2048): 247.835

Attention duration (in seconds): 0.1033
Attention throughput (in TFLOP/s): 200.592
MLP duration (in seconds): 0.1548
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2582
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17408x52224, b=2048): 0.0577
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17408x52224, b=2048): 258.281
Elapsed time for attention_key_query_prob (512x2048x136x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x136x2048): 100.503
Elapsed time for attention_prob_times_values (512x2048x2048x136): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x136): 126.737
Elapsed time for attention_linear_projection (4x17408x17408, b=2048): 0.0318
Throughput (in TFLOP/s) for attention_linear_projection (4x17408x17408, b=2048): 156.225
Elapsed time for mlp_h_to_4h (4x17408x69632, b=2048): 0.0778
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17408x69632, b=2048): 255.198
Elapsed time for mlp_4h_to_h (4x69632x17408, b=2048): 0.0797
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69632x17408, b=2048): 249.182

Attention duration (in seconds): 0.0999
Attention throughput (in TFLOP/s): 210.552
MLP duration (in seconds): 0.1575
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2574
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17536x52608, b=2048): 0.0581
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17536x52608, b=2048): 260.270
Elapsed time for attention_key_query_prob (512x2048x137x2048): 0.0175
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x137x2048): 33.676
Elapsed time for attention_prob_times_values (512x2048x2048x137): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x137): 58.949
Elapsed time for attention_linear_projection (4x17536x17536, b=2048): 0.0198
Throughput (in TFLOP/s) for attention_linear_projection (4x17536x17536, b=2048): 254.974
Elapsed time for mlp_h_to_4h (4x17536x70144, b=2048): 0.0775
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17536x70144, b=2048): 260.193
Elapsed time for mlp_4h_to_h (4x70144x17536, b=2048): 0.0822
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70144x17536, b=2048): 245.163

Attention duration (in seconds): 0.1053
Attention throughput (in TFLOP/s): 202.587
MLP duration (in seconds): 0.1597
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2649
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17664x52992, b=2048): 0.0590
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17664x52992, b=2048): 260.121
Elapsed time for attention_key_query_prob (512x2048x138x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x138x2048): 74.936
Elapsed time for attention_prob_times_values (512x2048x2048x138): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x138): 92.519
Elapsed time for attention_linear_projection (4x17664x17664, b=2048): 0.0209
Throughput (in TFLOP/s) for attention_linear_projection (4x17664x17664, b=2048): 244.480
Elapsed time for mlp_h_to_4h (4x17664x70656, b=2048): 0.0786
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17664x70656, b=2048): 260.149
Elapsed time for mlp_4h_to_h (4x70656x17664, b=2048): 0.0823
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70656x17664, b=2048): 248.524

Attention duration (in seconds): 0.0942
Attention throughput (in TFLOP/s): 229.697
MLP duration (in seconds): 0.1609
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2551
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17792x53376, b=2048): 0.0598
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17792x53376, b=2048): 259.972
Elapsed time for attention_key_query_prob (512x2048x139x2048): 0.0168
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x139x2048): 35.550
Elapsed time for attention_prob_times_values (512x2048x2048x139): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x139): 60.902
Elapsed time for attention_linear_projection (4x17792x17792, b=2048): 0.0203
Throughput (in TFLOP/s) for attention_linear_projection (4x17792x17792, b=2048): 255.857
Elapsed time for mlp_h_to_4h (4x17792x71168, b=2048): 0.0823
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17792x71168, b=2048): 252.160
Elapsed time for mlp_4h_to_h (4x71168x17792, b=2048): 0.0828
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71168x17792, b=2048): 250.575

Attention duration (in seconds): 0.1067
Attention throughput (in TFLOP/s): 205.589
MLP duration (in seconds): 0.1651
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2718
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17920x53760, b=2048): 0.0621
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17920x53760, b=2048): 254.100
Elapsed time for attention_key_query_prob (512x2048x140x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x140x2048): 74.563
Elapsed time for attention_prob_times_values (512x2048x2048x140): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x140): 94.316
Elapsed time for attention_linear_projection (4x17920x17920, b=2048): 0.0205
Throughput (in TFLOP/s) for attention_linear_projection (4x17920x17920, b=2048): 256.896
Elapsed time for mlp_h_to_4h (4x17920x71680, b=2048): 0.0810
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17920x71680, b=2048): 259.868
Elapsed time for mlp_4h_to_h (4x71680x17920, b=2048): 0.0849
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71680x17920, b=2048): 247.873

Attention duration (in seconds): 0.0970
Attention throughput (in TFLOP/s): 229.272
MLP duration (in seconds): 0.1659
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2629
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18048x54144, b=2048): 0.0616
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18048x54144, b=2048): 259.757
Elapsed time for attention_key_query_prob (512x2048x141x2048): 0.0173
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x141x2048): 34.998
Elapsed time for attention_prob_times_values (512x2048x2048x141): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x141): 59.822
Elapsed time for attention_linear_projection (4x18048x18048, b=2048): 0.0209
Throughput (in TFLOP/s) for attention_linear_projection (4x18048x18048, b=2048): 255.782
Elapsed time for mlp_h_to_4h (4x18048x72192, b=2048): 0.0831
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18048x72192, b=2048): 256.739
Elapsed time for mlp_4h_to_h (4x72192x18048, b=2048): 0.0858
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72192x18048, b=2048): 248.882

Attention duration (in seconds): 0.1099
Attention throughput (in TFLOP/s): 205.211
MLP duration (in seconds): 0.1689
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2788
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18176x54528, b=2048): 0.0625
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18176x54528, b=2048): 259.852
Elapsed time for attention_key_query_prob (512x2048x142x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x142x2048): 76.680
Elapsed time for attention_prob_times_values (512x2048x2048x142): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x142): 88.069
Elapsed time for attention_linear_projection (4x18176x18176, b=2048): 0.0210
Throughput (in TFLOP/s) for attention_linear_projection (4x18176x18176, b=2048): 258.275
Elapsed time for mlp_h_to_4h (4x18176x72704, b=2048): 0.0851
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18176x72704, b=2048): 254.493
Elapsed time for mlp_4h_to_h (4x72704x18176, b=2048): 0.0939
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72704x18176, b=2048): 230.577

Attention duration (in seconds): 0.0983
Attention throughput (in TFLOP/s): 232.600
MLP duration (in seconds): 0.1790
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2773
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18304x54912, b=2048): 0.0635
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18304x54912, b=2048): 259.368
Elapsed time for attention_key_query_prob (512x2048x143x2048): 0.0175
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x143x2048): 35.079
Elapsed time for attention_prob_times_values (512x2048x2048x143): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x143): 59.696
Elapsed time for attention_linear_projection (4x18304x18304, b=2048): 0.0211
Throughput (in TFLOP/s) for attention_linear_projection (4x18304x18304, b=2048): 260.138
Elapsed time for mlp_h_to_4h (4x18304x73216, b=2048): 0.0855
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18304x73216, b=2048): 256.869
Elapsed time for mlp_4h_to_h (4x73216x18304, b=2048): 0.0882
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73216x18304, b=2048): 248.910

Attention duration (in seconds): 0.1124
Attention throughput (in TFLOP/s): 206.294
MLP duration (in seconds): 0.1737
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2861
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18432x55296, b=2048): 0.0640
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18432x55296, b=2048): 260.872
Elapsed time for attention_key_query_prob (512x2048x144x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x144x2048): 112.322
Elapsed time for attention_prob_times_values (512x2048x2048x144): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x144): 138.144
Elapsed time for attention_linear_projection (4x18432x18432, b=2048): 0.0215
Throughput (in TFLOP/s) for attention_linear_projection (4x18432x18432, b=2048): 258.505
Elapsed time for mlp_h_to_4h (4x18432x73728, b=2048): 0.0880
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18432x73728, b=2048): 252.985
Elapsed time for mlp_4h_to_h (4x73728x18432, b=2048): 0.0973
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73728x18432, b=2048): 228.916

Attention duration (in seconds): 0.0955
Attention throughput (in TFLOP/s): 246.024
MLP duration (in seconds): 0.1853
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2808
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18560x55680, b=2048): 0.0650
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18560x55680, b=2048): 260.664
Elapsed time for attention_key_query_prob (512x2048x145x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x145x2048): 57.101
Elapsed time for attention_prob_times_values (512x2048x2048x145): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x145): 59.455
Elapsed time for attention_linear_projection (4x18560x18560, b=2048): 0.0219
Throughput (in TFLOP/s) for attention_linear_projection (4x18560x18560, b=2048): 257.719
Elapsed time for mlp_h_to_4h (4x18560x74240, b=2048): 0.0870
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18560x74240, b=2048): 259.590
Elapsed time for mlp_4h_to_h (4x74240x18560, b=2048): 0.0901
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74240x18560, b=2048): 250.505

Attention duration (in seconds): 0.1082
Attention throughput (in TFLOP/s): 220.084
MLP duration (in seconds): 0.1771
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2853
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18688x56064, b=2048): 0.0659
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18688x56064, b=2048): 260.473
Elapsed time for attention_key_query_prob (512x2048x146x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x146x2048): 78.386
Elapsed time for attention_prob_times_values (512x2048x2048x146): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x146): 97.314
Elapsed time for attention_linear_projection (4x18688x18688, b=2048): 0.0223
Throughput (in TFLOP/s) for attention_linear_projection (4x18688x18688, b=2048): 256.100
Elapsed time for mlp_h_to_4h (4x18688x74752, b=2048): 0.0891
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18688x74752, b=2048): 256.912
Elapsed time for mlp_4h_to_h (4x74752x18688, b=2048): 0.0922
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74752x18688, b=2048): 248.138

Attention duration (in seconds): 0.1027
Attention throughput (in TFLOP/s): 235.098
MLP duration (in seconds): 0.1813
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2840
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18816x56448, b=2048): 0.0695
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18816x56448, b=2048): 250.415
Elapsed time for attention_key_query_prob (512x2048x147x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x147x2048): 57.846
Elapsed time for attention_prob_times_values (512x2048x2048x147): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x147): 60.267
Elapsed time for attention_linear_projection (4x18816x18816, b=2048): 0.0227
Throughput (in TFLOP/s) for attention_linear_projection (4x18816x18816, b=2048): 255.032
Elapsed time for mlp_h_to_4h (4x18816x75264, b=2048): 0.0931
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18816x75264, b=2048): 249.306
Elapsed time for mlp_4h_to_h (4x75264x18816, b=2048): 0.0930
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75264x18816, b=2048): 249.395

Attention duration (in seconds): 0.1136
Attention throughput (in TFLOP/s): 215.311
MLP duration (in seconds): 0.1861
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2997
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18944x56832, b=2048): 0.0686
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18944x56832, b=2048): 257.142
Elapsed time for attention_key_query_prob (512x2048x148x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x148x2048): 79.515
Elapsed time for attention_prob_times_values (512x2048x2048x148): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x148): 97.696
Elapsed time for attention_linear_projection (4x18944x18944, b=2048): 0.0232
Throughput (in TFLOP/s) for attention_linear_projection (4x18944x18944, b=2048): 253.406
Elapsed time for mlp_h_to_4h (4x18944x75776, b=2048): 0.0918
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18944x75776, b=2048): 256.140
Elapsed time for mlp_4h_to_h (4x75776x18944, b=2048): 0.1019
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75776x18944, b=2048): 230.855

Attention duration (in seconds): 0.1063
Attention throughput (in TFLOP/s): 233.209
MLP duration (in seconds): 0.1937
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19072x57216, b=2048): 0.0689
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19072x57216, b=2048): 259.328
Elapsed time for attention_key_query_prob (512x2048x149x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x149x2048): 58.159
Elapsed time for attention_prob_times_values (512x2048x2048x149): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x149): 60.519
Elapsed time for attention_linear_projection (4x19072x19072, b=2048): 0.0233
Throughput (in TFLOP/s) for attention_linear_projection (4x19072x19072, b=2048): 256.176
Elapsed time for mlp_h_to_4h (4x19072x76288, b=2048): 0.0924
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19072x76288, b=2048): 258.063
Elapsed time for mlp_4h_to_h (4x76288x19072, b=2048): 0.0955
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76288x19072, b=2048): 249.496

Attention duration (in seconds): 0.1138
Attention throughput (in TFLOP/s): 220.753
MLP duration (in seconds): 0.1879
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19200x57600, b=2048): 0.0697
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19200x57600, b=2048): 260.139
Elapsed time for attention_key_query_prob (512x2048x150x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x150x2048): 80.867
Elapsed time for attention_prob_times_values (512x2048x2048x150): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x150): 98.605
Elapsed time for attention_linear_projection (4x19200x19200, b=2048): 0.0234
Throughput (in TFLOP/s) for attention_linear_projection (4x19200x19200, b=2048): 257.614
Elapsed time for mlp_h_to_4h (4x19200x76800, b=2048): 0.0928
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19200x76800, b=2048): 260.375
Elapsed time for mlp_4h_to_h (4x76800x19200, b=2048): 0.0982
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76800x19200, b=2048): 245.972

Attention duration (in seconds): 0.1076
Attention throughput (in TFLOP/s): 236.506
MLP duration (in seconds): 0.1910
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2986
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19328x57984, b=2048): 0.0706
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19328x57984, b=2048): 259.962
Elapsed time for attention_key_query_prob (512x2048x151x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x151x2048): 58.447
Elapsed time for attention_prob_times_values (512x2048x2048x151): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x151): 59.808
Elapsed time for attention_linear_projection (4x19328x19328, b=2048): 0.0239
Throughput (in TFLOP/s) for attention_linear_projection (4x19328x19328, b=2048): 256.606
Elapsed time for mlp_h_to_4h (4x19328x77312, b=2048): 0.0955
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19328x77312, b=2048): 256.403
Elapsed time for mlp_4h_to_h (4x77312x19328, b=2048): 0.0997
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77312x19328, b=2048): 245.635

Attention duration (in seconds): 0.1164
Attention throughput (in TFLOP/s): 221.426
MLP duration (in seconds): 0.1952
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19456x58368, b=2048): 0.0716
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19456x58368, b=2048): 259.846
Elapsed time for attention_key_query_prob (512x2048x152x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x152x2048): 107.313
Elapsed time for attention_prob_times_values (512x2048x2048x152): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x152): 142.592
Elapsed time for attention_linear_projection (4x19456x19456, b=2048): 0.0240
Throughput (in TFLOP/s) for attention_linear_projection (4x19456x19456, b=2048): 258.865
Elapsed time for mlp_h_to_4h (4x19456x77824, b=2048): 0.0970
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19456x77824, b=2048): 255.725
Elapsed time for mlp_4h_to_h (4x77824x19456, b=2048): 0.0991
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77824x19456, b=2048): 250.362

Attention duration (in seconds): 0.1062
Attention throughput (in TFLOP/s): 245.835
MLP duration (in seconds): 0.1961
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19584x58752, b=2048): 0.0725
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19584x58752, b=2048): 260.115
Elapsed time for attention_key_query_prob (512x2048x153x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x153x2048): 58.753
Elapsed time for attention_prob_times_values (512x2048x2048x153): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x153): 60.170
Elapsed time for attention_linear_projection (4x19584x19584, b=2048): 0.0243
Throughput (in TFLOP/s) for attention_linear_projection (4x19584x19584, b=2048): 258.635
Elapsed time for mlp_h_to_4h (4x19584x78336, b=2048): 0.0989
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19584x78336, b=2048): 254.175
Elapsed time for mlp_4h_to_h (4x78336x19584, b=2048): 0.0990
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78336x19584, b=2048): 253.944

Attention duration (in seconds): 0.1189
Attention throughput (in TFLOP/s): 222.498
MLP duration (in seconds): 0.1979
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19712x59136, b=2048): 0.1255
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19712x59136, b=2048): 152.195
Elapsed time for attention_key_query_prob (512x2048x154x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x154x2048): 82.578
Elapsed time for attention_prob_times_values (512x2048x2048x154): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x154): 101.286
Elapsed time for attention_linear_projection (4x19712x19712, b=2048): 0.0246
Throughput (in TFLOP/s) for attention_linear_projection (4x19712x19712, b=2048): 258.714
Elapsed time for mlp_h_to_4h (4x19712x78848, b=2048): 0.0987
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19712x78848, b=2048): 257.920
Elapsed time for mlp_4h_to_h (4x78848x19712, b=2048): 0.1006
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78848x19712, b=2048): 253.223

Attention duration (in seconds): 0.1646
Attention throughput (in TFLOP/s): 162.710
MLP duration (in seconds): 0.1993
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3639
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19840x59520, b=2048): 0.0745
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19840x59520, b=2048): 259.602
Elapsed time for attention_key_query_prob (512x2048x155x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x155x2048): 59.037
Elapsed time for attention_prob_times_values (512x2048x2048x155): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x155): 62.877
Elapsed time for attention_linear_projection (4x19840x19840, b=2048): 0.0250
Throughput (in TFLOP/s) for attention_linear_projection (4x19840x19840, b=2048): 257.467
Elapsed time for mlp_h_to_4h (4x19840x79360, b=2048): 0.1012
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19840x79360, b=2048): 254.990
Elapsed time for mlp_4h_to_h (4x79360x19840, b=2048): 0.1044
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79360x19840, b=2048): 247.003

Attention duration (in seconds): 0.1214
Attention throughput (in TFLOP/s): 223.387
MLP duration (in seconds): 0.2056
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19968x59904, b=2048): 0.0765
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19968x59904, b=2048): 256.087
Elapsed time for attention_key_query_prob (512x2048x156x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x156x2048): 82.852
Elapsed time for attention_prob_times_values (512x2048x2048x156): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x156): 102.399
Elapsed time for attention_linear_projection (4x19968x19968, b=2048): 0.0255
Throughput (in TFLOP/s) for attention_linear_projection (4x19968x19968, b=2048): 256.609
Elapsed time for mlp_h_to_4h (4x19968x79872, b=2048): 0.1016
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19968x79872, b=2048): 257.202
Elapsed time for mlp_4h_to_h (4x79872x19968, b=2048): 0.1035
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79872x19968, b=2048): 252.396

Attention duration (in seconds): 0.1166
Attention throughput (in TFLOP/s): 235.564
MLP duration (in seconds): 0.2051
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20096x60288, b=2048): 0.0771
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20096x60288, b=2048): 257.568
Elapsed time for attention_key_query_prob (512x2048x157x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x157x2048): 59.279
Elapsed time for attention_prob_times_values (512x2048x2048x157): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x157): 61.878
Elapsed time for attention_linear_projection (4x20096x20096, b=2048): 0.0258
Throughput (in TFLOP/s) for attention_linear_projection (4x20096x20096, b=2048): 256.439
Elapsed time for mlp_h_to_4h (4x20096x80384, b=2048): 0.1322
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20096x80384, b=2048): 200.226
Elapsed time for mlp_4h_to_h (4x80384x20096, b=2048): 0.1053
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80384x20096, b=2048): 251.308

Attention duration (in seconds): 0.1251
Attention throughput (in TFLOP/s): 222.270
MLP duration (in seconds): 0.2375
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3626
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20224x60672, b=2048): 0.0777
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20224x60672, b=2048): 258.701
Elapsed time for attention_key_query_prob (512x2048x158x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x158x2048): 84.389
Elapsed time for attention_prob_times_values (512x2048x2048x158): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x158): 101.940
Elapsed time for attention_linear_projection (4x20224x20224, b=2048): 0.0264
Throughput (in TFLOP/s) for attention_linear_projection (4x20224x20224, b=2048): 253.547
Elapsed time for mlp_h_to_4h (4x20224x80896, b=2048): 0.1056
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20224x80896, b=2048): 253.827
Elapsed time for mlp_4h_to_h (4x80896x20224, b=2048): 0.1058
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80896x20224, b=2048): 253.431

Attention duration (in seconds): 0.1188
Attention throughput (in TFLOP/s): 236.979
MLP duration (in seconds): 0.2114
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3302
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20352x61056, b=2048): 0.0788
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20352x61056, b=2048): 258.307
Elapsed time for attention_key_query_prob (512x2048x159x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x159x2048): 60.861
Elapsed time for attention_prob_times_values (512x2048x2048x159): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x159): 63.401
Elapsed time for attention_linear_projection (4x20352x20352, b=2048): 0.0264
Throughput (in TFLOP/s) for attention_linear_projection (4x20352x20352, b=2048): 256.593
Elapsed time for mlp_h_to_4h (4x20352x81408, b=2048): 0.1074
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20352x81408, b=2048): 252.829
Elapsed time for mlp_4h_to_h (4x81408x20352, b=2048): 0.1087
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81408x20352, b=2048): 249.642

Attention duration (in seconds): 0.1273
Attention throughput (in TFLOP/s): 224.044
MLP duration (in seconds): 0.2161
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3434
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20480x61440, b=2048): 0.0810
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20480x61440, b=2048): 254.376
Elapsed time for attention_key_query_prob (512x2048x160x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x160x2048): 126.085
Elapsed time for attention_prob_times_values (512x2048x2048x160): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x160): 151.932
Elapsed time for attention_linear_projection (4x20480x20480, b=2048): 0.0266
Throughput (in TFLOP/s) for attention_linear_projection (4x20480x20480, b=2048): 257.887
Elapsed time for mlp_h_to_4h (4x20480x81920, b=2048): 0.1084
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20480x81920, b=2048): 253.598
Elapsed time for mlp_4h_to_h (4x81920x20480, b=2048): 0.1085
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81920x20480, b=2048): 253.241

Attention duration (in seconds): 0.1177
Attention throughput (in TFLOP/s): 245.291
MLP duration (in seconds): 0.2169
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3346
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20608x61824, b=2048): 0.0821
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20608x61824, b=2048): 254.335
Elapsed time for attention_key_query_prob (512x2048x161x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x161x2048): 57.530
Elapsed time for attention_prob_times_values (512x2048x2048x161): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x161): 63.317
Elapsed time for attention_linear_projection (4x20608x20608, b=2048): 0.0268
Throughput (in TFLOP/s) for attention_linear_projection (4x20608x20608, b=2048): 259.424
Elapsed time for mlp_h_to_4h (4x20608x82432, b=2048): 0.1099
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20608x82432, b=2048): 253.179
Elapsed time for mlp_4h_to_h (4x82432x20608, b=2048): 0.1795
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82432x20608, b=2048): 155.095

Attention duration (in seconds): 0.1318
Attention throughput (in TFLOP/s): 221.604
MLP duration (in seconds): 0.2894
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20736x62208, b=2048): 0.0822
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20736x62208, b=2048): 257.014
Elapsed time for attention_key_query_prob (512x2048x162x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x162x2048): 79.283
Elapsed time for attention_prob_times_values (512x2048x2048x162): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x162): 103.322
Elapsed time for attention_linear_projection (4x20736x20736, b=2048): 0.0272
Throughput (in TFLOP/s) for attention_linear_projection (4x20736x20736, b=2048): 258.613
Elapsed time for mlp_h_to_4h (4x20736x82944, b=2048): 0.1155
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20736x82944, b=2048): 244.014
Elapsed time for mlp_4h_to_h (4x82944x20736, b=2048): 0.1809
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82944x20736, b=2048): 155.799

Attention duration (in seconds): 0.1250
Attention throughput (in TFLOP/s): 236.602
MLP duration (in seconds): 0.2964
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20864x62592, b=2048): 0.0830
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20864x62592, b=2048): 257.754
Elapsed time for attention_key_query_prob (512x2048x163x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x163x2048): 57.610
Elapsed time for attention_prob_times_values (512x2048x2048x163): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x163): 65.642
Elapsed time for attention_linear_projection (4x20864x20864, b=2048): 0.0277
Throughput (in TFLOP/s) for attention_linear_projection (4x20864x20864, b=2048): 257.615
Elapsed time for mlp_h_to_4h (4x20864x83456, b=2048): 0.1120
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20864x83456, b=2048): 254.656
Elapsed time for mlp_4h_to_h (4x83456x20864, b=2048): 0.1926
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83456x20864, b=2048): 148.083

Attention duration (in seconds): 0.1335
Attention throughput (in TFLOP/s): 224.163
MLP duration (in seconds): 0.3047
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4382
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20992x62976, b=2048): 0.0865
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20992x62976, b=2048): 250.535
Elapsed time for attention_key_query_prob (512x2048x164x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x164x2048): 80.014
Elapsed time for attention_prob_times_values (512x2048x2048x164): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x164): 106.883
Elapsed time for attention_linear_projection (4x20992x20992, b=2048): 0.0279
Throughput (in TFLOP/s) for attention_linear_projection (4x20992x20992, b=2048): 258.889
Elapsed time for mlp_h_to_4h (4x20992x83968, b=2048): 0.1145
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20992x83968, b=2048): 252.288
Elapsed time for mlp_4h_to_h (4x83968x20992, b=2048): 0.1904
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83968x20992, b=2048): 151.655

Attention duration (in seconds): 0.1297
Attention throughput (in TFLOP/s): 233.463
MLP duration (in seconds): 0.3049
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4346
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21120x63360, b=2048): 0.0851
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21120x63360, b=2048): 257.779
Elapsed time for attention_key_query_prob (512x2048x165x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x165x2048): 58.659
Elapsed time for attention_prob_times_values (512x2048x2048x165): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x165): 66.193
Elapsed time for attention_linear_projection (4x21120x21120, b=2048): 0.0281
Throughput (in TFLOP/s) for attention_linear_projection (4x21120x21120, b=2048): 260.293
Elapsed time for mlp_h_to_4h (4x21120x84480, b=2048): 0.1149
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21120x84480, b=2048): 254.516
Elapsed time for mlp_4h_to_h (4x84480x21120, b=2048): 0.1961
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84480x21120, b=2048): 149.106

Attention duration (in seconds): 0.1359
Attention throughput (in TFLOP/s): 225.508
MLP duration (in seconds): 0.3109
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4468
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21248x63744, b=2048): 0.0866
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21248x63744, b=2048): 256.210
Elapsed time for attention_key_query_prob (512x2048x166x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x166x2048): 80.756
Elapsed time for attention_prob_times_values (512x2048x2048x166): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x166): 105.229
Elapsed time for attention_linear_projection (4x21248x21248, b=2048): 0.0291
Throughput (in TFLOP/s) for attention_linear_projection (4x21248x21248, b=2048): 254.231
Elapsed time for mlp_h_to_4h (4x21248x84992, b=2048): 0.1464
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21248x84992, b=2048): 202.043
Elapsed time for mlp_4h_to_h (4x84992x21248, b=2048): 0.1938
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84992x21248, b=2048): 152.642

Attention duration (in seconds): 0.1313
Attention throughput (in TFLOP/s): 236.185
MLP duration (in seconds): 0.3403
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4716
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21376x64128, b=2048): 0.0886
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21376x64128, b=2048): 253.440
Elapsed time for attention_key_query_prob (512x2048x167x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x167x2048): 59.165
Elapsed time for attention_prob_times_values (512x2048x2048x167): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x167): 64.958
Elapsed time for attention_linear_projection (4x21376x21376, b=2048): 0.0289
Throughput (in TFLOP/s) for attention_linear_projection (4x21376x21376, b=2048): 258.653
Elapsed time for mlp_h_to_4h (4x21376x85504, b=2048): 0.1174
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21376x85504, b=2048): 255.081
Elapsed time for mlp_4h_to_h (4x85504x21376, b=2048): 0.1942
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85504x21376, b=2048): 154.179

Attention duration (in seconds): 0.1407
Attention throughput (in TFLOP/s): 222.987
MLP duration (in seconds): 0.3116
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4523
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21504x64512, b=2048): 0.0896
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21504x64512, b=2048): 253.756
Elapsed time for attention_key_query_prob (512x2048x168x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x168x2048): 109.423
Elapsed time for attention_prob_times_values (512x2048x2048x168): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x168): 152.595
Elapsed time for attention_linear_projection (4x21504x21504, b=2048): 0.0294
Throughput (in TFLOP/s) for attention_linear_projection (4x21504x21504, b=2048): 257.319
Elapsed time for mlp_h_to_4h (4x21504x86016, b=2048): 0.1202
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21504x86016, b=2048): 252.181
Elapsed time for mlp_4h_to_h (4x86016x21504, b=2048): 0.1979
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86016x21504, b=2048): 153.163

Attention duration (in seconds): 0.1303
Attention throughput (in TFLOP/s): 243.589
MLP duration (in seconds): 0.3180
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4484
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21632x64896, b=2048): 0.0904
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21632x64896, b=2048): 254.457
Elapsed time for attention_key_query_prob (512x2048x169x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x169x2048): 59.042
Elapsed time for attention_prob_times_values (512x2048x2048x169): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x169): 65.475
Elapsed time for attention_linear_projection (4x21632x21632, b=2048): 0.0299
Throughput (in TFLOP/s) for attention_linear_projection (4x21632x21632, b=2048): 256.788
Elapsed time for mlp_h_to_4h (4x21632x86528, b=2048): 0.1192
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21632x86528, b=2048): 257.195
Elapsed time for mlp_4h_to_h (4x86528x21632, b=2048): 0.1973
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86528x21632, b=2048): 155.468

Attention duration (in seconds): 0.1436
Attention throughput (in TFLOP/s): 223.628
MLP duration (in seconds): 0.3165
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4601
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21760x65280, b=2048): 0.0904
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21760x65280, b=2048): 257.488
Elapsed time for attention_key_query_prob (512x2048x170x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x170x2048): 83.960
Elapsed time for attention_prob_times_values (512x2048x2048x170): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x170): 107.855
Elapsed time for attention_linear_projection (4x21760x21760, b=2048): 0.0311
Throughput (in TFLOP/s) for attention_linear_projection (4x21760x21760, b=2048): 249.693
Elapsed time for mlp_h_to_4h (4x21760x87040, b=2048): 0.1214
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21760x87040, b=2048): 255.609
Elapsed time for mlp_4h_to_h (4x87040x21760, b=2048): 0.1956
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87040x21760, b=2048): 158.646

Attention duration (in seconds): 0.1369
Attention throughput (in TFLOP/s): 237.300
MLP duration (in seconds): 0.3170
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4539
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21888x65664, b=2048): 0.0922
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21888x65664, b=2048): 255.393
Elapsed time for attention_key_query_prob (512x2048x171x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x171x2048): 60.386
Elapsed time for attention_prob_times_values (512x2048x2048x171): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x171): 68.885
Elapsed time for attention_linear_projection (4x21888x21888, b=2048): 0.0305
Throughput (in TFLOP/s) for attention_linear_projection (4x21888x21888, b=2048): 256.962
Elapsed time for mlp_h_to_4h (4x21888x87552, b=2048): 0.1237
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21888x87552, b=2048): 253.718
Elapsed time for mlp_4h_to_h (4x87552x21888, b=2048): 0.2063
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87552x21888, b=2048): 152.183

Attention duration (in seconds): 0.1456
Attention throughput (in TFLOP/s): 225.770
MLP duration (in seconds): 0.3301
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4756
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22016x66048, b=2048): 0.0925
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22016x66048, b=2048): 257.432
Elapsed time for attention_key_query_prob (512x2048x172x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x172x2048): 84.644
Elapsed time for attention_prob_times_values (512x2048x2048x172): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x172): 109.117
Elapsed time for attention_linear_projection (4x22016x22016, b=2048): 0.0308
Throughput (in TFLOP/s) for attention_linear_projection (4x22016x22016, b=2048): 257.567
Elapsed time for mlp_h_to_4h (4x22016x88064, b=2048): 0.1408
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22016x88064, b=2048): 225.662
Elapsed time for mlp_4h_to_h (4x88064x22016, b=2048): 0.2084
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88064x22016, b=2048): 152.421

Attention duration (in seconds): 0.1389
Attention throughput (in TFLOP/s): 239.373
MLP duration (in seconds): 0.3492
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4880
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22144x66432, b=2048): 0.0956
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22144x66432, b=2048): 252.065
Elapsed time for attention_key_query_prob (512x2048x173x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x173x2048): 60.793
Elapsed time for attention_prob_times_values (512x2048x2048x173): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x173): 68.906
Elapsed time for attention_linear_projection (4x22144x22144, b=2048): 0.0311
Throughput (in TFLOP/s) for attention_linear_projection (4x22144x22144, b=2048): 257.972
Elapsed time for mlp_h_to_4h (4x22144x88576, b=2048): 0.1259
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22144x88576, b=2048): 255.300
Elapsed time for mlp_4h_to_h (4x88576x22144, b=2048): 0.2111
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88576x22144, b=2048): 152.204

Attention duration (in seconds): 0.1498
Attention throughput (in TFLOP/s): 224.496
MLP duration (in seconds): 0.3370
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4868
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22272x66816, b=2048): 0.0947
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22272x66816, b=2048): 257.526
Elapsed time for attention_key_query_prob (512x2048x174x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x174x2048): 83.936
Elapsed time for attention_prob_times_values (512x2048x2048x174): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x174): 107.508
Elapsed time for attention_linear_projection (4x22272x22272, b=2048): 0.0314
Throughput (in TFLOP/s) for attention_linear_projection (4x22272x22272, b=2048): 259.102
Elapsed time for mlp_h_to_4h (4x22272x89088, b=2048): 0.1262
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22272x89088, b=2048): 257.669
Elapsed time for mlp_4h_to_h (4x89088x22272, b=2048): 0.2089
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89088x22272, b=2048): 155.625

Attention duration (in seconds): 0.1419
Attention throughput (in TFLOP/s): 239.633
MLP duration (in seconds): 0.3351
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4770
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22400x67200, b=2048): 0.0960
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22400x67200, b=2048): 256.913
Elapsed time for attention_key_query_prob (512x2048x175x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x175x2048): 61.048
Elapsed time for attention_prob_times_values (512x2048x2048x175): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x175): 67.668
Elapsed time for attention_linear_projection (4x22400x22400, b=2048): 0.0319
Throughput (in TFLOP/s) for attention_linear_projection (4x22400x22400, b=2048): 257.409
Elapsed time for mlp_h_to_4h (4x22400x89600, b=2048): 0.1293
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22400x89600, b=2048): 254.226
Elapsed time for mlp_4h_to_h (4x89600x22400, b=2048): 0.1394
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89600x22400, b=2048): 235.851

Attention duration (in seconds): 0.1514
Attention throughput (in TFLOP/s): 227.197
MLP duration (in seconds): 0.2688
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22528x67584, b=2048): 0.0975
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22528x67584, b=2048): 255.918
Elapsed time for attention_key_query_prob (512x2048x176x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x176x2048): 123.767
Elapsed time for attention_prob_times_values (512x2048x2048x176): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x176): 164.789
Elapsed time for attention_linear_projection (4x22528x22528, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_linear_projection (4x22528x22528, b=2048): 257.520
Elapsed time for mlp_h_to_4h (4x22528x90112, b=2048): 0.1361
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22528x90112, b=2048): 244.372
Elapsed time for mlp_4h_to_h (4x90112x22528, b=2048): 0.1373
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90112x22528, b=2048): 242.298

Attention duration (in seconds): 0.1405
Attention throughput (in TFLOP/s): 247.564
MLP duration (in seconds): 0.2734
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22656x67968, b=2048): 0.0989
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22656x67968, b=2048): 255.208
Elapsed time for attention_key_query_prob (512x2048x177x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x177x2048): 61.494
Elapsed time for attention_prob_times_values (512x2048x2048x177): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x177): 68.152
Elapsed time for attention_linear_projection (4x22656x22656, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_linear_projection (4x22656x22656, b=2048): 258.722
Elapsed time for mlp_h_to_4h (4x22656x90624, b=2048): 0.1305
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22656x90624, b=2048): 257.777
Elapsed time for mlp_4h_to_h (4x90624x22656, b=2048): 0.1384
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90624x22656, b=2048): 243.077

Attention duration (in seconds): 0.1549
Attention throughput (in TFLOP/s): 227.012
MLP duration (in seconds): 0.2689
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22784x68352, b=2048): 0.0988
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22784x68352, b=2048): 258.314
Elapsed time for attention_key_query_prob (512x2048x178x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x178x2048): 86.617
Elapsed time for attention_prob_times_values (512x2048x2048x178): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x178): 111.382
Elapsed time for attention_linear_projection (4x22784x22784, b=2048): 0.0330
Throughput (in TFLOP/s) for attention_linear_projection (4x22784x22784, b=2048): 257.731
Elapsed time for mlp_h_to_4h (4x22784x91136, b=2048): 0.1341
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22784x91136, b=2048): 253.625
Elapsed time for mlp_4h_to_h (4x91136x22784, b=2048): 0.1401
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91136x22784, b=2048): 242.851

Attention duration (in seconds): 0.1475
Attention throughput (in TFLOP/s): 241.068
MLP duration (in seconds): 0.2742
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22912x68736, b=2048): 0.1003
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22912x68736, b=2048): 257.317
Elapsed time for attention_key_query_prob (512x2048x179x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x179x2048): 61.535
Elapsed time for attention_prob_times_values (512x2048x2048x179): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x179): 70.795
Elapsed time for attention_linear_projection (4x22912x22912, b=2048): 0.0336
Throughput (in TFLOP/s) for attention_linear_projection (4x22912x22912, b=2048): 255.756
Elapsed time for mlp_h_to_4h (4x22912x91648, b=2048): 0.1359
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22912x91648, b=2048): 253.092
Elapsed time for mlp_4h_to_h (4x91648x22912, b=2048): 0.1419
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91648x22912, b=2048): 242.494

Attention duration (in seconds): 0.1573
Attention throughput (in TFLOP/s): 228.549
MLP duration (in seconds): 0.2778
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4351
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23040x69120, b=2048): 0.1018
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23040x69120, b=2048): 256.348
Elapsed time for attention_key_query_prob (512x2048x180x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x180x2048): 87.759
Elapsed time for attention_prob_times_values (512x2048x2048x180): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x180): 112.313
Elapsed time for attention_linear_projection (4x23040x23040, b=2048): 0.0352
Throughput (in TFLOP/s) for attention_linear_projection (4x23040x23040, b=2048): 246.841
Elapsed time for mlp_h_to_4h (4x23040x92160, b=2048): 0.1358
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23040x92160, b=2048): 256.097
Elapsed time for mlp_4h_to_h (4x92160x23040, b=2048): 0.1477
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92160x23040, b=2048): 235.466

Attention duration (in seconds): 0.1527
Attention throughput (in TFLOP/s): 237.937
MLP duration (in seconds): 0.2836
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4363
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23168x69504, b=2048): 0.1027
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23168x69504, b=2048): 256.889
Elapsed time for attention_key_query_prob (512x2048x181x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x181x2048): 61.575
Elapsed time for attention_prob_times_values (512x2048x2048x181): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x181): 71.633
Elapsed time for attention_linear_projection (4x23168x23168, b=2048): 0.0355
Throughput (in TFLOP/s) for attention_linear_projection (4x23168x23168, b=2048): 247.551
Elapsed time for mlp_h_to_4h (4x23168x92672, b=2048): 0.1382
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23168x92672, b=2048): 254.471
Elapsed time for mlp_4h_to_h (4x92672x23168, b=2048): 0.1444
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92672x23168, b=2048): 243.601

Attention duration (in seconds): 0.1617
Attention throughput (in TFLOP/s): 227.155
MLP duration (in seconds): 0.2826
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4443
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23296x69888, b=2048): 0.1044
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23296x69888, b=2048): 255.490
Elapsed time for attention_key_query_prob (512x2048x182x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x182x2048): 88.225
Elapsed time for attention_prob_times_values (512x2048x2048x182): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x182): 112.067
Elapsed time for attention_linear_projection (4x23296x23296, b=2048): 0.0354
Throughput (in TFLOP/s) for attention_linear_projection (4x23296x23296, b=2048): 251.323
Elapsed time for mlp_h_to_4h (4x23296x93184, b=2048): 0.1525
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23296x93184, b=2048): 233.185
Elapsed time for mlp_4h_to_h (4x93184x23296, b=2048): 0.1468
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93184x23296, b=2048): 242.268

Attention duration (in seconds): 0.1556
Attention throughput (in TFLOP/s): 238.591
MLP duration (in seconds): 0.2993
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4550
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23424x70272, b=2048): 0.1053
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23424x70272, b=2048): 256.224
Elapsed time for attention_key_query_prob (512x2048x183x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x183x2048): 61.995
Elapsed time for attention_prob_times_values (512x2048x2048x183): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x183): 69.838
Elapsed time for attention_linear_projection (4x23424x23424, b=2048): 0.0364
Throughput (in TFLOP/s) for attention_linear_projection (4x23424x23424, b=2048): 247.052
Elapsed time for mlp_h_to_4h (4x23424x93696, b=2048): 0.1409
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23424x93696, b=2048): 255.149
Elapsed time for mlp_4h_to_h (4x93696x23424, b=2048): 0.1481
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93696x23424, b=2048): 242.744

Attention duration (in seconds): 0.1656
Attention throughput (in TFLOP/s): 226.667
MLP duration (in seconds): 0.2891
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4546
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23552x70656, b=2048): 0.1074
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23552x70656, b=2048): 253.843
Elapsed time for attention_key_query_prob (512x2048x184x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x184x2048): 107.962
Elapsed time for attention_prob_times_values (512x2048x2048x184): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x184): 168.624
Elapsed time for attention_linear_projection (4x23552x23552, b=2048): 0.0430
Throughput (in TFLOP/s) for attention_linear_projection (4x23552x23552, b=2048): 211.134
Elapsed time for mlp_h_to_4h (4x23552x94208, b=2048): 0.1432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23552x94208, b=2048): 253.907
Elapsed time for mlp_4h_to_h (4x94208x23552, b=2048): 0.1494
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94208x23552, b=2048): 243.330

Attention duration (in seconds): 0.1625
Attention throughput (in TFLOP/s): 233.495
MLP duration (in seconds): 0.2926
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4550
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23680x71040, b=2048): 0.1076
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23680x71040, b=2048): 256.204
Elapsed time for attention_key_query_prob (512x2048x185x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x185x2048): 62.652
Elapsed time for attention_prob_times_values (512x2048x2048x185): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x185): 70.253
Elapsed time for attention_linear_projection (4x23680x23680, b=2048): 0.0357
Throughput (in TFLOP/s) for attention_linear_projection (4x23680x23680, b=2048): 257.085
Elapsed time for mlp_h_to_4h (4x23680x94720, b=2048): 0.1467
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23680x94720, b=2048): 250.588
Elapsed time for mlp_4h_to_h (4x94720x23680, b=2048): 0.1512
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94720x23680, b=2048): 243.089

Attention duration (in seconds): 0.1673
Attention throughput (in TFLOP/s): 229.150
MLP duration (in seconds): 0.2978
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4651
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23808x71424, b=2048): 0.1095
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23808x71424, b=2048): 254.463
Elapsed time for attention_key_query_prob (512x2048x186x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x186x2048): 90.520
Elapsed time for attention_prob_times_values (512x2048x2048x186): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x186): 113.256
Elapsed time for attention_linear_projection (4x23808x23808, b=2048): 0.0373
Throughput (in TFLOP/s) for attention_linear_projection (4x23808x23808, b=2048): 249.012
Elapsed time for mlp_h_to_4h (4x23808x95232, b=2048): 0.1450
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23808x95232, b=2048): 256.204
Elapsed time for mlp_4h_to_h (4x95232x23808, b=2048): 0.1541
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95232x23808, b=2048): 240.981

Attention duration (in seconds): 0.1627
Attention throughput (in TFLOP/s): 238.195
MLP duration (in seconds): 0.2991
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4618
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23936x71808, b=2048): 0.1096
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23936x71808, b=2048): 257.023
Elapsed time for attention_key_query_prob (512x2048x187x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x187x2048): 63.084
Elapsed time for attention_prob_times_values (512x2048x2048x187): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x187): 73.993
Elapsed time for attention_linear_projection (4x23936x23936, b=2048): 0.0545
Throughput (in TFLOP/s) for attention_linear_projection (4x23936x23936, b=2048): 172.258
Elapsed time for mlp_h_to_4h (4x23936x95744, b=2048): 0.1484
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23936x95744, b=2048): 253.047
Elapsed time for mlp_4h_to_h (4x95744x23936, b=2048): 0.1548
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95744x23936, b=2048): 242.608

Attention duration (in seconds): 0.1876
Attention throughput (in TFLOP/s): 208.661
MLP duration (in seconds): 0.3031
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4908
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24064x72192, b=2048): 0.1102
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24064x72192, b=2048): 258.342
Elapsed time for attention_key_query_prob (512x2048x188x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x188x2048): 91.163
Elapsed time for attention_prob_times_values (512x2048x2048x188): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x188): 116.755
Elapsed time for attention_linear_projection (4x24064x24064, b=2048): 0.0381
Throughput (in TFLOP/s) for attention_linear_projection (4x24064x24064, b=2048): 248.976
Elapsed time for mlp_h_to_4h (4x24064x96256, b=2048): 0.1568
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24064x96256, b=2048): 242.075
Elapsed time for mlp_4h_to_h (4x96256x24064, b=2048): 0.1620
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96256x24064, b=2048): 234.269

Attention duration (in seconds): 0.1641
Attention throughput (in TFLOP/s): 241.171
MLP duration (in seconds): 0.3188
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4828
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24192x72576, b=2048): 0.1114
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24192x72576, b=2048): 258.220
Elapsed time for attention_key_query_prob (512x2048x189x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x189x2048): 63.871
Elapsed time for attention_prob_times_values (512x2048x2048x189): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x189): 74.901
Elapsed time for attention_linear_projection (4x24192x24192, b=2048): 0.0372
Throughput (in TFLOP/s) for attention_linear_projection (4x24192x24192, b=2048): 257.877
Elapsed time for mlp_h_to_4h (4x24192x96768, b=2048): 0.1506
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24192x96768, b=2048): 254.630
Elapsed time for mlp_4h_to_h (4x96768x24192, b=2048): 0.1577
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96768x24192, b=2048): 243.148

Attention duration (in seconds): 0.1721
Attention throughput (in TFLOP/s): 232.255
MLP duration (in seconds): 0.3084
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4805
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24320x72960, b=2048): 0.1481
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24320x72960, b=2048): 196.255
Elapsed time for attention_key_query_prob (512x2048x190x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x190x2048): 88.856
Elapsed time for attention_prob_times_values (512x2048x2048x190): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x190): 121.039
Elapsed time for attention_linear_projection (4x24320x24320, b=2048): 0.0396
Throughput (in TFLOP/s) for attention_linear_projection (4x24320x24320, b=2048): 244.509
Elapsed time for mlp_h_to_4h (4x24320x97280, b=2048): 0.1509
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24320x97280, b=2048): 256.915
Elapsed time for mlp_4h_to_h (4x97280x24320, b=2048): 0.1589
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97280x24320, b=2048): 243.994

Attention duration (in seconds): 0.2037
Attention throughput (in TFLOP/s): 198.312
MLP duration (in seconds): 0.3097
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24448x73344, b=2048): 0.1148
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24448x73344, b=2048): 255.856
Elapsed time for attention_key_query_prob (512x2048x191x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x191x2048): 64.537
Elapsed time for attention_prob_times_values (512x2048x2048x191): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x191): 73.890
Elapsed time for attention_linear_projection (4x24448x24448, b=2048): 0.0377
Throughput (in TFLOP/s) for attention_linear_projection (4x24448x24448, b=2048): 259.743
Elapsed time for mlp_h_to_4h (4x24448x97792, b=2048): 0.1538
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24448x97792, b=2048): 254.747
Elapsed time for mlp_4h_to_h (4x97792x24448, b=2048): 0.1615
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97792x24448, b=2048): 242.481

Attention duration (in seconds): 0.1763
Attention throughput (in TFLOP/s): 231.440
MLP duration (in seconds): 0.3153
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4916
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24576x73728, b=2048): 0.1185
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24576x73728, b=2048): 250.540
Elapsed time for attention_key_query_prob (512x2048x192x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x192x2048): 136.543
Elapsed time for attention_prob_times_values (512x2048x2048x192): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x192): 178.434
Elapsed time for attention_linear_projection (4x24576x24576, b=2048): 0.0385
Throughput (in TFLOP/s) for attention_linear_projection (4x24576x24576, b=2048): 257.076
Elapsed time for mlp_h_to_4h (4x24576x98304, b=2048): 0.1563
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24576x98304, b=2048): 253.236
Elapsed time for mlp_4h_to_h (4x98304x24576, b=2048): 0.1632
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98304x24576, b=2048): 242.563

Attention duration (in seconds): 0.1676
Attention throughput (in TFLOP/s): 245.946
MLP duration (in seconds): 0.3195
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4871
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24704x74112, b=2048): 0.1221
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24704x74112, b=2048): 245.671
Elapsed time for attention_key_query_prob (512x2048x193x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x193x2048): 61.447
Elapsed time for attention_prob_times_values (512x2048x2048x193): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x193): 73.949
Elapsed time for attention_linear_projection (4x24704x24704, b=2048): 0.0406
Throughput (in TFLOP/s) for attention_linear_projection (4x24704x24704, b=2048): 246.305
Elapsed time for mlp_h_to_4h (4x24704x98816, b=2048): 0.1583
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24704x98816, b=2048): 252.669
Elapsed time for mlp_4h_to_h (4x98816x24704, b=2048): 0.1642
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98816x24704, b=2048): 243.510

Attention duration (in seconds): 0.1874
Attention throughput (in TFLOP/s): 222.275
MLP duration (in seconds): 0.3225
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24832x74496, b=2048): 0.1180
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24832x74496, b=2048): 256.841
Elapsed time for attention_key_query_prob (512x2048x194x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x194x2048): 88.261
Elapsed time for attention_prob_times_values (512x2048x2048x194): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x194): 118.023
Elapsed time for attention_linear_projection (4x24832x24832, b=2048): 0.0396
Throughput (in TFLOP/s) for attention_linear_projection (4x24832x24832, b=2048): 254.949
Elapsed time for mlp_h_to_4h (4x24832x99328, b=2048): 0.1583
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24832x99328, b=2048): 255.364
Elapsed time for mlp_4h_to_h (4x99328x24832, b=2048): 0.1663
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99328x24832, b=2048): 242.984

Attention duration (in seconds): 0.1741
Attention throughput (in TFLOP/s): 241.643
MLP duration (in seconds): 0.3246
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4987
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24960x74880, b=2048): 0.1204
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24960x74880, b=2048): 254.436
Elapsed time for attention_key_query_prob (512x2048x195x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x195x2048): 61.956
Elapsed time for attention_prob_times_values (512x2048x2048x195): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x195): 76.385
Elapsed time for attention_linear_projection (4x24960x24960, b=2048): 0.0396
Throughput (in TFLOP/s) for attention_linear_projection (4x24960x24960, b=2048): 258.016
Elapsed time for mlp_h_to_4h (4x24960x99840, b=2048): 0.1598
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24960x99840, b=2048): 255.551
Elapsed time for mlp_4h_to_h (4x99840x24960, b=2048): 0.2143
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99840x24960, b=2048): 190.482

Attention duration (in seconds): 0.1844
Attention throughput (in TFLOP/s): 230.506
MLP duration (in seconds): 0.3741
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5585
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25088x75264, b=2048): 0.1209
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25088x75264, b=2048): 255.805
Elapsed time for attention_key_query_prob (512x2048x196x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x196x2048): 88.035
Elapsed time for attention_prob_times_values (512x2048x2048x196): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x196): 119.293
Elapsed time for attention_linear_projection (4x25088x25088, b=2048): 0.0492
Throughput (in TFLOP/s) for attention_linear_projection (4x25088x25088, b=2048): 209.451
Elapsed time for mlp_h_to_4h (4x25088x100352, b=2048): 0.1620
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25088x100352, b=2048): 254.622
Elapsed time for mlp_4h_to_h (4x100352x25088, b=2048): 0.1686
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100352x25088, b=2048): 244.609

Attention duration (in seconds): 0.1868
Attention throughput (in TFLOP/s): 229.842
MLP duration (in seconds): 0.3306
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25216x75648, b=2048): 0.1242
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25216x75648, b=2048): 251.640
Elapsed time for attention_key_query_prob (512x2048x197x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x197x2048): 62.453
Elapsed time for attention_prob_times_values (512x2048x2048x197): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x197): 76.316
Elapsed time for attention_linear_projection (4x25216x25216, b=2048): 0.0402
Throughput (in TFLOP/s) for attention_linear_projection (4x25216x25216, b=2048): 259.193
Elapsed time for mlp_h_to_4h (4x25216x100864, b=2048): 0.1642
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25216x100864, b=2048): 253.753
Elapsed time for mlp_4h_to_h (4x100864x25216, b=2048): 0.2179
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100864x25216, b=2048): 191.264

Attention duration (in seconds): 0.1890
Attention throughput (in TFLOP/s): 229.403
MLP duration (in seconds): 0.3821
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5711
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25344x76032, b=2048): 0.1231
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25344x76032, b=2048): 256.518
Elapsed time for attention_key_query_prob (512x2048x198x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x198x2048): 89.532
Elapsed time for attention_prob_times_values (512x2048x2048x198): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x198): 115.623
Elapsed time for attention_linear_projection (4x25344x25344, b=2048): 0.0404
Throughput (in TFLOP/s) for attention_linear_projection (4x25344x25344, b=2048): 260.234
Elapsed time for mlp_h_to_4h (4x25344x101376, b=2048): 0.1656
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25344x101376, b=2048): 254.123
Elapsed time for mlp_4h_to_h (4x101376x25344, b=2048): 0.1722
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101376x25344, b=2048): 244.403

Attention duration (in seconds): 0.1804
Attention throughput (in TFLOP/s): 242.812
MLP duration (in seconds): 0.3379
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25472x76416, b=2048): 0.1244
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25472x76416, b=2048): 256.340
Elapsed time for attention_key_query_prob (512x2048x199x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x199x2048): 62.043
Elapsed time for attention_prob_times_values (512x2048x2048x199): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x199): 74.104
Elapsed time for attention_linear_projection (4x25472x25472, b=2048): 0.0407
Throughput (in TFLOP/s) for attention_linear_projection (4x25472x25472, b=2048): 261.400
Elapsed time for mlp_h_to_4h (4x25472x101888, b=2048): 0.1676
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25472x101888, b=2048): 253.696
Elapsed time for mlp_4h_to_h (4x101888x25472, b=2048): 0.1743
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101888x25472, b=2048): 243.894

Attention duration (in seconds): 0.1904
Attention throughput (in TFLOP/s): 232.322
MLP duration (in seconds): 0.3420
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5323
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25600x76800, b=2048): 0.1248
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25600x76800, b=2048): 258.196
Elapsed time for attention_key_query_prob (512x2048x200x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x200x2048): 117.152
Elapsed time for attention_prob_times_values (512x2048x2048x200): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x200): 180.686
Elapsed time for attention_linear_projection (4x25600x25600, b=2048): 0.0415
Throughput (in TFLOP/s) for attention_linear_projection (4x25600x25600, b=2048): 258.823
Elapsed time for mlp_h_to_4h (4x25600x102400, b=2048): 0.1694
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25600x102400, b=2048): 253.592
Elapsed time for mlp_4h_to_h (4x102400x25600, b=2048): 0.1760
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102400x25600, b=2048): 243.973

Attention duration (in seconds): 0.1783
Attention throughput (in TFLOP/s): 250.477
MLP duration (in seconds): 0.3454
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25728x77184, b=2048): 0.1270
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25728x77184, b=2048): 256.111
Elapsed time for attention_key_query_prob (512x2048x201x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x201x2048): 62.946
Elapsed time for attention_prob_times_values (512x2048x2048x201): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x201): 73.887
Elapsed time for attention_linear_projection (4x25728x25728, b=2048): 0.0417
Throughput (in TFLOP/s) for attention_linear_projection (4x25728x25728, b=2048): 259.762
Elapsed time for mlp_h_to_4h (4x25728x102912, b=2048): 0.1768
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25728x102912, b=2048): 245.382
Elapsed time for mlp_4h_to_h (4x102912x25728, b=2048): 0.1790
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102912x25728, b=2048): 242.389

Attention duration (in seconds): 0.1942
Attention throughput (in TFLOP/s): 232.289
MLP duration (in seconds): 0.3558
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5499
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25856x77568, b=2048): 0.1298
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25856x77568, b=2048): 253.093
Elapsed time for attention_key_query_prob (512x2048x202x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x202x2048): 90.662
Elapsed time for attention_prob_times_values (512x2048x2048x202): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x202): 117.707
Elapsed time for attention_linear_projection (4x25856x25856, b=2048): 0.0420
Throughput (in TFLOP/s) for attention_linear_projection (4x25856x25856, b=2048): 260.744
Elapsed time for mlp_h_to_4h (4x25856x103424, b=2048): 0.1731
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25856x103424, b=2048): 253.134
Elapsed time for mlp_4h_to_h (4x103424x25856, b=2048): 0.1810
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103424x25856, b=2048): 242.070

Attention duration (in seconds): 0.1888
Attention throughput (in TFLOP/s): 241.276
MLP duration (in seconds): 0.3541
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5429
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25984x77952, b=2048): 0.1301
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25984x77952, b=2048): 255.170
Elapsed time for attention_key_query_prob (512x2048x203x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x203x2048): 63.933
Elapsed time for attention_prob_times_values (512x2048x2048x203): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x203): 76.601
Elapsed time for attention_linear_projection (4x25984x25984, b=2048): 0.0423
Throughput (in TFLOP/s) for attention_linear_projection (4x25984x25984, b=2048): 261.353
Elapsed time for mlp_h_to_4h (4x25984x103936, b=2048): 0.1779
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25984x103936, b=2048): 248.692
Elapsed time for mlp_4h_to_h (4x103936x25984, b=2048): 0.1937
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103936x25984, b=2048): 228.478

Attention duration (in seconds): 0.1974
Attention throughput (in TFLOP/s): 232.988
MLP duration (in seconds): 0.3716
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5690
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26112x78336, b=2048): 0.1314
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26112x78336, b=2048): 255.139
Elapsed time for attention_key_query_prob (512x2048x204x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x204x2048): 91.510
Elapsed time for attention_prob_times_values (512x2048x2048x204): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x204): 119.088
Elapsed time for attention_linear_projection (4x26112x26112, b=2048): 0.0431
Throughput (in TFLOP/s) for attention_linear_projection (4x26112x26112, b=2048): 259.476
Elapsed time for mlp_h_to_4h (4x26112x104448, b=2048): 0.1763
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26112x104448, b=2048): 253.424
Elapsed time for mlp_4h_to_h (4x104448x26112, b=2048): 0.1837
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104448x26112, b=2048): 243.271

Attention duration (in seconds): 0.1913
Attention throughput (in TFLOP/s): 242.696
MLP duration (in seconds): 0.3600
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5513
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26240x78720, b=2048): 0.1325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26240x78720, b=2048): 255.353
Elapsed time for attention_key_query_prob (512x2048x205x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x205x2048): 64.323
Elapsed time for attention_prob_times_values (512x2048x2048x205): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x205): 76.837
Elapsed time for attention_linear_projection (4x26240x26240, b=2048): 0.0433
Throughput (in TFLOP/s) for attention_linear_projection (4x26240x26240, b=2048): 260.553
Elapsed time for mlp_h_to_4h (4x26240x104960, b=2048): 0.1770
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26240x104960, b=2048): 254.894
Elapsed time for mlp_4h_to_h (4x104960x26240, b=2048): 0.1848
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104960x26240, b=2048): 244.185

Attention duration (in seconds): 0.2010
Attention throughput (in TFLOP/s): 233.284
MLP duration (in seconds): 0.3618
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5628
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26368x79104, b=2048): 0.1342
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26368x79104, b=2048): 254.744
Elapsed time for attention_key_query_prob (512x2048x206x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x206x2048): 92.125
Elapsed time for attention_prob_times_values (512x2048x2048x206): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x206): 119.701
Elapsed time for attention_linear_projection (4x26368x26368, b=2048): 0.0444
Throughput (in TFLOP/s) for attention_linear_projection (4x26368x26368, b=2048): 256.750
Elapsed time for mlp_h_to_4h (4x26368x105472, b=2048): 0.1781
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26368x105472, b=2048): 255.906
Elapsed time for mlp_4h_to_h (4x105472x26368, b=2048): 0.1876
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105472x26368, b=2048): 242.847

Attention duration (in seconds): 0.1955
Attention throughput (in TFLOP/s): 242.106
MLP duration (in seconds): 0.3657
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5612
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26496x79488, b=2048): 0.1344
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26496x79488, b=2048): 256.690
Elapsed time for attention_key_query_prob (512x2048x207x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x207x2048): 64.657
Elapsed time for attention_prob_times_values (512x2048x2048x207): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x207): 76.248
Elapsed time for attention_linear_projection (4x26496x26496, b=2048): 0.0443
Throughput (in TFLOP/s) for attention_linear_projection (4x26496x26496, b=2048): 259.405
Elapsed time for mlp_h_to_4h (4x26496x105984, b=2048): 0.1818
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26496x105984, b=2048): 253.094
Elapsed time for mlp_4h_to_h (4x105984x26496, b=2048): 0.1976
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105984x26496, b=2048): 232.873

Attention duration (in seconds): 0.2042
Attention throughput (in TFLOP/s): 234.043
MLP duration (in seconds): 0.3794
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5835
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26624x79872, b=2048): 0.1355
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26624x79872, b=2048): 257.058
Elapsed time for attention_key_query_prob (512x2048x208x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x208x2048): 128.776
Elapsed time for attention_prob_times_values (512x2048x2048x208): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x208): 188.575
Elapsed time for attention_linear_projection (4x26624x26624, b=2048): 0.0446
Throughput (in TFLOP/s) for attention_linear_projection (4x26624x26624, b=2048): 260.614
Elapsed time for mlp_h_to_4h (4x26624x106496, b=2048): 0.1844
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26624x106496, b=2048): 251.924
Elapsed time for mlp_4h_to_h (4x106496x26624, b=2048): 0.1910
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106496x26624, b=2048): 243.265

Attention duration (in seconds): 0.1918
Attention throughput (in TFLOP/s): 251.552
MLP duration (in seconds): 0.3754
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5671
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26752x80256, b=2048): 0.1365
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26752x80256, b=2048): 257.627
Elapsed time for attention_key_query_prob (512x2048x209x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x209x2048): 65.047
Elapsed time for attention_prob_times_values (512x2048x2048x209): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x209): 76.172
Elapsed time for attention_linear_projection (4x26752x26752, b=2048): 0.0454
Throughput (in TFLOP/s) for attention_linear_projection (4x26752x26752, b=2048): 258.050
Elapsed time for mlp_h_to_4h (4x26752x107008, b=2048): 0.1839
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26752x107008, b=2048): 255.074
Elapsed time for mlp_4h_to_h (4x107008x26752, b=2048): 0.1929
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107008x26752, b=2048): 243.092

Attention duration (in seconds): 0.2076
Attention throughput (in TFLOP/s): 234.614
MLP duration (in seconds): 0.3768
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5844
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26880x80640, b=2048): 0.1374
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26880x80640, b=2048): 258.512
Elapsed time for attention_key_query_prob (512x2048x210x2048): 0.0314
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x210x2048): 28.770
Elapsed time for attention_prob_times_values (512x2048x2048x210): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x210): 120.165
Elapsed time for attention_linear_projection (4x26880x26880, b=2048): 0.0458
Throughput (in TFLOP/s) for attention_linear_projection (4x26880x26880, b=2048): 258.380
Elapsed time for mlp_h_to_4h (4x26880x107520, b=2048): 0.1862
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26880x107520, b=2048): 254.267
Elapsed time for mlp_4h_to_h (4x107520x26880, b=2048): 0.1946
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107520x26880, b=2048): 243.311

Attention duration (in seconds): 0.2221
Attention throughput (in TFLOP/s): 221.372
MLP duration (in seconds): 0.3808
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27008x81024, b=2048): 0.1403
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27008x81024, b=2048): 255.554
Elapsed time for attention_key_query_prob (512x2048x211x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x211x2048): 64.670
Elapsed time for attention_prob_times_values (512x2048x2048x211): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x211): 78.026
Elapsed time for attention_linear_projection (4x27008x27008, b=2048): 0.1374
Throughput (in TFLOP/s) for attention_linear_projection (4x27008x27008, b=2048): 86.987
Elapsed time for mlp_h_to_4h (4x27008x108032, b=2048): 0.1908
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27008x108032, b=2048): 250.541
Elapsed time for mlp_4h_to_h (4x108032x27008, b=2048): 0.1969
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108032x27008, b=2048): 242.779

Attention duration (in seconds): 0.3033
Attention throughput (in TFLOP/s): 163.582
MLP duration (in seconds): 0.3877
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6910
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27136x81408, b=2048): 0.1402
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27136x81408, b=2048): 258.141
Elapsed time for attention_key_query_prob (512x2048x212x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x212x2048): 93.839
Elapsed time for attention_prob_times_values (512x2048x2048x212): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x212): 121.025
Elapsed time for attention_linear_projection (4x27136x27136, b=2048): 0.0462
Throughput (in TFLOP/s) for attention_linear_projection (4x27136x27136, b=2048): 260.957
Elapsed time for mlp_h_to_4h (4x27136x108544, b=2048): 0.1900
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27136x108544, b=2048): 253.951
Elapsed time for mlp_4h_to_h (4x108544x27136, b=2048): 0.1993
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108544x27136, b=2048): 242.124

Attention duration (in seconds): 0.2037
Attention throughput (in TFLOP/s): 245.888
MLP duration (in seconds): 0.3893
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5930
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27264x81792, b=2048): 0.1430
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27264x81792, b=2048): 255.501
Elapsed time for attention_key_query_prob (512x2048x213x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x213x2048): 65.498
Elapsed time for attention_prob_times_values (512x2048x2048x213): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x213): 78.570
Elapsed time for attention_linear_projection (4x27264x27264, b=2048): 0.0543
Throughput (in TFLOP/s) for attention_linear_projection (4x27264x27264, b=2048): 224.176
Elapsed time for mlp_h_to_4h (4x27264x109056, b=2048): 0.1928
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27264x109056, b=2048): 252.640
Elapsed time for mlp_4h_to_h (4x109056x27264, b=2048): 0.2009
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109056x27264, b=2048): 242.512

Attention duration (in seconds): 0.2229
Attention throughput (in TFLOP/s): 226.723
MLP duration (in seconds): 0.3937
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27392x82176, b=2048): 0.1473
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27392x82176, b=2048): 250.332
Elapsed time for attention_key_query_prob (512x2048x214x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x214x2048): 95.555
Elapsed time for attention_prob_times_values (512x2048x2048x214): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x214): 121.969
Elapsed time for attention_linear_projection (4x27392x27392, b=2048): 0.0473
Throughput (in TFLOP/s) for attention_linear_projection (4x27392x27392, b=2048): 259.761
Elapsed time for mlp_h_to_4h (4x27392x109568, b=2048): 0.1940
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27392x109568, b=2048): 253.427
Elapsed time for mlp_4h_to_h (4x109568x27392, b=2048): 0.2025
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109568x27392, b=2048): 242.785

Attention duration (in seconds): 0.2118
Attention throughput (in TFLOP/s): 240.843
MLP duration (in seconds): 0.3966
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27520x82560, b=2048): 0.1477
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27520x82560, b=2048): 252.110
Elapsed time for attention_key_query_prob (512x2048x215x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x215x2048): 65.885
Elapsed time for attention_prob_times_values (512x2048x2048x215): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x215): 77.715
Elapsed time for attention_linear_projection (4x27520x27520, b=2048): 0.0482
Throughput (in TFLOP/s) for attention_linear_projection (4x27520x27520, b=2048): 257.335
Elapsed time for mlp_h_to_4h (4x27520x110080, b=2048): 0.2020
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27520x110080, b=2048): 245.749
Elapsed time for mlp_4h_to_h (4x110080x27520, b=2048): 0.2052
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110080x27520, b=2048): 241.904

Attention duration (in seconds): 0.2218
Attention throughput (in TFLOP/s): 232.133
MLP duration (in seconds): 0.4071
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6289
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27648x82944, b=2048): 0.1481
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27648x82944, b=2048): 253.684
Elapsed time for attention_key_query_prob (512x2048x216x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x216x2048): 116.535
Elapsed time for attention_prob_times_values (512x2048x2048x216): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x216): 192.524
Elapsed time for attention_linear_projection (4x27648x27648, b=2048): 0.0485
Throughput (in TFLOP/s) for attention_linear_projection (4x27648x27648, b=2048): 258.338
Elapsed time for mlp_h_to_4h (4x27648x110592, b=2048): 0.2006
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27648x110592, b=2048): 249.746
Elapsed time for mlp_4h_to_h (4x110592x27648, b=2048): 0.2049
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110592x27648, b=2048): 244.442

Attention duration (in seconds): 0.2094
Attention throughput (in TFLOP/s): 248.139
MLP duration (in seconds): 0.4055
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27776x83328, b=2048): 0.1469
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27776x83328, b=2048): 258.160
Elapsed time for attention_key_query_prob (512x2048x217x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x217x2048): 65.980
Elapsed time for attention_prob_times_values (512x2048x2048x217): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x217): 77.986
Elapsed time for attention_linear_projection (4x27776x27776, b=2048): 0.0487
Throughput (in TFLOP/s) for attention_linear_projection (4x27776x27776, b=2048): 259.429
Elapsed time for mlp_h_to_4h (4x27776x111104, b=2048): 0.1996
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27776x111104, b=2048): 253.260
Elapsed time for mlp_4h_to_h (4x111104x27776, b=2048): 0.2084
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111104x27776, b=2048): 242.648

Attention duration (in seconds): 0.2217
Attention throughput (in TFLOP/s): 236.481
MLP duration (in seconds): 0.4080
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27904x83712, b=2048): 0.1494
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27904x83712, b=2048): 256.105
Elapsed time for attention_key_query_prob (512x2048x218x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x218x2048): 97.014
Elapsed time for attention_prob_times_values (512x2048x2048x218): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x218): 125.036
Elapsed time for attention_linear_projection (4x27904x27904, b=2048): 0.0495
Throughput (in TFLOP/s) for attention_linear_projection (4x27904x27904, b=2048): 257.560
Elapsed time for mlp_h_to_4h (4x27904x111616, b=2048): 0.2017
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27904x111616, b=2048): 252.943
Elapsed time for mlp_4h_to_h (4x111616x27904, b=2048): 0.2100
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111616x27904, b=2048): 243.024

Attention duration (in seconds): 0.2161
Attention throughput (in TFLOP/s): 244.792
MLP duration (in seconds): 0.4117
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28032x84096, b=2048): 0.1537
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28032x84096, b=2048): 251.249
Elapsed time for attention_key_query_prob (512x2048x219x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x219x2048): 66.954
Elapsed time for attention_prob_times_values (512x2048x2048x219): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x219): 80.402
Elapsed time for attention_linear_projection (4x28032x28032, b=2048): 0.0504
Throughput (in TFLOP/s) for attention_linear_projection (4x28032x28032, b=2048): 255.482
Elapsed time for mlp_h_to_4h (4x28032x112128, b=2048): 0.2047
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28032x112128, b=2048): 251.610
Elapsed time for mlp_4h_to_h (4x112128x28032, b=2048): 0.2121
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112128x28032, b=2048): 242.843

Attention duration (in seconds): 0.2299
Attention throughput (in TFLOP/s): 232.219
MLP duration (in seconds): 0.4167
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6466
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28160x84480, b=2048): 0.1525
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28160x84480, b=2048): 255.508
Elapsed time for attention_key_query_prob (512x2048x220x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x220x2048): 96.060
Elapsed time for attention_prob_times_values (512x2048x2048x220): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x220): 122.638
Elapsed time for attention_linear_projection (4x28160x28160, b=2048): 0.0501
Throughput (in TFLOP/s) for attention_linear_projection (4x28160x28160, b=2048): 259.070
Elapsed time for mlp_h_to_4h (4x28160x112640, b=2048): 0.2045
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28160x112640, b=2048): 254.163
Elapsed time for mlp_4h_to_h (4x112640x28160, b=2048): 0.2147
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112640x28160, b=2048): 242.001

Attention duration (in seconds): 0.2202
Attention throughput (in TFLOP/s): 244.549
MLP duration (in seconds): 0.4192
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6395
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28288x84864, b=2048): 0.1542
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28288x84864, b=2048): 255.025
Elapsed time for attention_key_query_prob (512x2048x221x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x221x2048): 67.150
Elapsed time for attention_prob_times_values (512x2048x2048x221): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x221): 80.790
Elapsed time for attention_linear_projection (4x28288x28288, b=2048): 0.0508
Throughput (in TFLOP/s) for attention_linear_projection (4x28288x28288, b=2048): 257.943
Elapsed time for mlp_h_to_4h (4x28288x113152, b=2048): 0.2073
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28288x113152, b=2048): 253.024
Elapsed time for mlp_4h_to_h (4x113152x28288, b=2048): 0.2157
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113152x28288, b=2048): 243.145

Attention duration (in seconds): 0.2309
Attention throughput (in TFLOP/s): 235.304
MLP duration (in seconds): 0.4229
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6539
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28416x85248, b=2048): 0.1550
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28416x85248, b=2048): 256.066
Elapsed time for attention_key_query_prob (512x2048x222x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x222x2048): 97.938
Elapsed time for attention_prob_times_values (512x2048x2048x222): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x222): 127.546
Elapsed time for attention_linear_projection (4x28416x28416, b=2048): 0.0507
Throughput (in TFLOP/s) for attention_linear_projection (4x28416x28416, b=2048): 261.076
Elapsed time for mlp_h_to_4h (4x28416x113664, b=2048): 0.2074
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28416x113664, b=2048): 255.175
Elapsed time for mlp_4h_to_h (4x113664x28416, b=2048): 0.2173
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113664x28416, b=2048): 243.486

Attention duration (in seconds): 0.2229
Attention throughput (in TFLOP/s): 245.987
MLP duration (in seconds): 0.4247
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6476
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28544x85632, b=2048): 0.1602
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28544x85632, b=2048): 249.915
Elapsed time for attention_key_query_prob (512x2048x223x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x223x2048): 69.150
Elapsed time for attention_prob_times_values (512x2048x2048x223): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x223): 81.416
Elapsed time for attention_linear_projection (4x28544x28544, b=2048): 0.0969
Throughput (in TFLOP/s) for attention_linear_projection (4x28544x28544, b=2048): 137.693
Elapsed time for mlp_h_to_4h (4x28544x114176, b=2048): 0.2122
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28544x114176, b=2048): 251.657
Elapsed time for mlp_4h_to_h (4x114176x28544, b=2048): 0.2225
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114176x28544, b=2048): 239.948

Attention duration (in seconds): 0.2828
Attention throughput (in TFLOP/s): 195.582
MLP duration (in seconds): 0.4347
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28672x86016, b=2048): 0.1586
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28672x86016, b=2048): 254.854
Elapsed time for attention_key_query_prob (512x2048x224x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x224x2048): 141.965
Elapsed time for attention_prob_times_values (512x2048x2048x224): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x224): 201.701
Elapsed time for attention_linear_projection (4x28672x28672, b=2048): 0.0519
Throughput (in TFLOP/s) for attention_linear_projection (4x28672x28672, b=2048): 259.592
Elapsed time for mlp_h_to_4h (4x28672x114688, b=2048): 0.2150
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28672x114688, b=2048): 250.553
Elapsed time for mlp_4h_to_h (4x114688x28672, b=2048): 0.2216
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114688x28672, b=2048): 243.141

Attention duration (in seconds): 0.2220
Attention throughput (in TFLOP/s): 251.373
MLP duration (in seconds): 0.4366
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6586
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28800x86400, b=2048): 0.1608
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28800x86400, b=2048): 253.584
Elapsed time for attention_key_query_prob (512x2048x225x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x225x2048): 66.363
Elapsed time for attention_prob_times_values (512x2048x2048x225): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x225): 81.750
Elapsed time for attention_linear_projection (4x28800x28800, b=2048): 0.0521
Throughput (in TFLOP/s) for attention_linear_projection (4x28800x28800, b=2048): 260.655
Elapsed time for mlp_h_to_4h (4x28800x115200, b=2048): 0.2165
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28800x115200, b=2048): 251.114
Elapsed time for mlp_4h_to_h (4x115200x28800, b=2048): 0.2243
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115200x28800, b=2048): 242.359

Attention duration (in seconds): 0.2393
Attention throughput (in TFLOP/s): 235.243
MLP duration (in seconds): 0.4408
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6800
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28928x86784, b=2048): 0.1935
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28928x86784, b=2048): 212.572
Elapsed time for attention_key_query_prob (512x2048x226x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x226x2048): 94.799
Elapsed time for attention_prob_times_values (512x2048x2048x226): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x226): 130.770
Elapsed time for attention_linear_projection (4x28928x28928, b=2048): 0.0526
Throughput (in TFLOP/s) for attention_linear_projection (4x28928x28928, b=2048): 260.735
Elapsed time for mlp_h_to_4h (4x28928x115712, b=2048): 0.2187
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28928x115712, b=2048): 250.766
Elapsed time for mlp_4h_to_h (4x115712x28928, b=2048): 0.2286
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115712x28928, b=2048): 239.934

Attention duration (in seconds): 0.2637
Attention throughput (in TFLOP/s): 215.300
MLP duration (in seconds): 0.4473
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29056x87168, b=2048): 0.1641
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29056x87168, b=2048): 252.893
Elapsed time for attention_key_query_prob (512x2048x227x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x227x2048): 66.728
Elapsed time for attention_prob_times_values (512x2048x2048x227): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x227): 83.879
Elapsed time for attention_linear_projection (4x29056x29056, b=2048): 0.0535
Throughput (in TFLOP/s) for attention_linear_projection (4x29056x29056, b=2048): 258.308
Elapsed time for mlp_h_to_4h (4x29056x116224, b=2048): 0.2311
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29056x116224, b=2048): 239.423
Elapsed time for mlp_4h_to_h (4x116224x29056, b=2048): 0.2275
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116224x29056, b=2048): 243.226

Attention duration (in seconds): 0.2439
Attention throughput (in TFLOP/s): 234.873
MLP duration (in seconds): 0.4586
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29184x87552, b=2048): 0.1659
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29184x87552, b=2048): 252.391
Elapsed time for attention_key_query_prob (512x2048x228x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x228x2048): 95.230
Elapsed time for attention_prob_times_values (512x2048x2048x228): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x228): 131.614
Elapsed time for attention_linear_projection (4x29184x29184, b=2048): 0.0542
Throughput (in TFLOP/s) for attention_linear_projection (4x29184x29184, b=2048): 257.307
Elapsed time for mlp_h_to_4h (4x29184x116736, b=2048): 0.2259
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29184x116736, b=2048): 247.090
Elapsed time for mlp_4h_to_h (4x116736x29184, b=2048): 0.2294
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116736x29184, b=2048): 243.326

Attention duration (in seconds): 0.2378
Attention throughput (in TFLOP/s): 242.938
MLP duration (in seconds): 0.4553
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6931
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29312x87936, b=2048): 0.1664
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29312x87936, b=2048): 253.752
Elapsed time for attention_key_query_prob (512x2048x229x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x229x2048): 67.197
Elapsed time for attention_prob_times_values (512x2048x2048x229): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x229): 84.322
Elapsed time for attention_linear_projection (4x29312x29312, b=2048): 0.0546
Throughput (in TFLOP/s) for attention_linear_projection (4x29312x29312, b=2048): 257.969
Elapsed time for mlp_h_to_4h (4x29312x117248, b=2048): 0.2250
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29312x117248, b=2048): 250.290
Elapsed time for mlp_4h_to_h (4x117248x29312, b=2048): 0.2418
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117248x29312, b=2048): 232.876

Attention duration (in seconds): 0.2473
Attention throughput (in TFLOP/s): 235.649
MLP duration (in seconds): 0.4668
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29440x88320, b=2048): 0.1673
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29440x88320, b=2048): 254.580
Elapsed time for attention_key_query_prob (512x2048x230x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x230x2048): 81.560
Elapsed time for attention_prob_times_values (512x2048x2048x230): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x230): 131.425
Elapsed time for attention_linear_projection (4x29440x29440, b=2048): 0.0550
Throughput (in TFLOP/s) for attention_linear_projection (4x29440x29440, b=2048): 258.159
Elapsed time for mlp_h_to_4h (4x29440x117760, b=2048): 0.2244
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29440x117760, b=2048): 253.099
Elapsed time for mlp_4h_to_h (4x117760x29440, b=2048): 0.2407
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117760x29440, b=2048): 235.997

Attention duration (in seconds): 0.2420
Attention throughput (in TFLOP/s): 242.907
MLP duration (in seconds): 0.4651
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29568x88704, b=2048): 0.1676
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29568x88704, b=2048): 256.445
Elapsed time for attention_key_query_prob (512x2048x231x2048): 0.0171
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x231x2048): 57.940
Elapsed time for attention_prob_times_values (512x2048x2048x231): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x231): 83.067
Elapsed time for attention_linear_projection (4x29568x29568, b=2048): 0.0551
Throughput (in TFLOP/s) for attention_linear_projection (4x29568x29568, b=2048): 259.830
Elapsed time for mlp_h_to_4h (4x29568x118272, b=2048): 0.2367
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29568x118272, b=2048): 242.081
Elapsed time for mlp_4h_to_h (4x118272x29568, b=2048): 0.2379
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118272x29568, b=2048): 240.824

Attention duration (in seconds): 0.2518
Attention throughput (in TFLOP/s): 235.460
MLP duration (in seconds): 0.4746
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29696x89088, b=2048): 0.1781
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29696x89088, b=2048): 243.332
Elapsed time for attention_key_query_prob (512x2048x232x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x232x2048): 119.536
Elapsed time for attention_prob_times_values (512x2048x2048x232): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x232): 204.549
Elapsed time for attention_linear_projection (4x29696x29696, b=2048): 0.0560
Throughput (in TFLOP/s) for attention_linear_projection (4x29696x29696, b=2048): 257.941
Elapsed time for mlp_h_to_4h (4x29696x118784, b=2048): 0.2321
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29696x118784, b=2048): 249.006
Elapsed time for mlp_4h_to_h (4x118784x29696, b=2048): 0.2374
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118784x29696, b=2048): 243.473

Attention duration (in seconds): 0.2474
Attention throughput (in TFLOP/s): 241.704
MLP duration (in seconds): 0.4695
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29824x89472, b=2048): 0.1715
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29824x89472, b=2048): 254.892
Elapsed time for attention_key_query_prob (512x2048x233x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x233x2048): 66.973
Elapsed time for attention_prob_times_values (512x2048x2048x233): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x233): 83.678
Elapsed time for attention_linear_projection (4x29824x29824, b=2048): 0.0563
Throughput (in TFLOP/s) for attention_linear_projection (4x29824x29824, b=2048): 258.965
Elapsed time for mlp_h_to_4h (4x29824x119296, b=2048): 0.2303
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29824x119296, b=2048): 253.083
Elapsed time for mlp_4h_to_h (4x119296x29824, b=2048): 0.2405
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119296x29824, b=2048): 242.422

Attention duration (in seconds): 0.2547
Attention throughput (in TFLOP/s): 236.728
MLP duration (in seconds): 0.4708
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29952x89856, b=2048): 0.1751
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29952x89856, b=2048): 251.835
Elapsed time for attention_key_query_prob (512x2048x234x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x234x2048): 97.341
Elapsed time for attention_prob_times_values (512x2048x2048x234): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x234): 132.935
Elapsed time for attention_linear_projection (4x29952x29952, b=2048): 0.0566
Throughput (in TFLOP/s) for attention_linear_projection (4x29952x29952, b=2048): 259.611
Elapsed time for mlp_h_to_4h (4x29952x119808, b=2048): 0.2322
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29952x119808, b=2048): 253.222
Elapsed time for mlp_4h_to_h (4x119808x29952, b=2048): 0.2767
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119808x29952, b=2048): 212.471

Attention duration (in seconds): 0.2496
Attention throughput (in TFLOP/s): 243.607
MLP duration (in seconds): 0.5089
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7585
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30080x90240, b=2048): 0.1756
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30080x90240, b=2048): 253.313
Elapsed time for attention_key_query_prob (512x2048x235x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x235x2048): 68.544
Elapsed time for attention_prob_times_values (512x2048x2048x235): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x235): 86.987
Elapsed time for attention_linear_projection (4x30080x30080, b=2048): 0.0569
Throughput (in TFLOP/s) for attention_linear_projection (4x30080x30080, b=2048): 260.704
Elapsed time for mlp_h_to_4h (4x30080x120320, b=2048): 0.2356
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30080x120320, b=2048): 251.731
Elapsed time for mlp_4h_to_h (4x120320x30080, b=2048): 0.2452
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120320x30080, b=2048): 241.838

Attention duration (in seconds): 0.2588
Attention throughput (in TFLOP/s): 236.964
MLP duration (in seconds): 0.4808
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7395
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30208x90624, b=2048): 0.1806
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30208x90624, b=2048): 248.291
Elapsed time for attention_key_query_prob (512x2048x236x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x236x2048): 97.319
Elapsed time for attention_prob_times_values (512x2048x2048x236): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x236): 133.344
Elapsed time for attention_linear_projection (4x30208x30208, b=2048): 0.0574
Throughput (in TFLOP/s) for attention_linear_projection (4x30208x30208, b=2048): 260.572
Elapsed time for mlp_h_to_4h (4x30208x120832, b=2048): 0.2362
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30208x120832, b=2048): 253.149
Elapsed time for mlp_4h_to_h (4x120832x30208, b=2048): 0.2572
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120832x30208, b=2048): 232.505

Attention duration (in seconds): 0.2560
Attention throughput (in TFLOP/s): 241.489
MLP duration (in seconds): 0.4934
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7495
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30336x91008, b=2048): 0.1784
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30336x91008, b=2048): 253.522
Elapsed time for attention_key_query_prob (512x2048x237x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x237x2048): 69.109
Elapsed time for attention_prob_times_values (512x2048x2048x237): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x237): 87.617
Elapsed time for attention_linear_projection (4x30336x30336, b=2048): 0.0583
Throughput (in TFLOP/s) for attention_linear_projection (4x30336x30336, b=2048): 258.556
Elapsed time for mlp_h_to_4h (4x30336x121344, b=2048): 0.2390
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30336x121344, b=2048): 252.371
Elapsed time for mlp_4h_to_h (4x121344x30336, b=2048): 0.2492
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121344x30336, b=2048): 241.997

Attention duration (in seconds): 0.2631
Attention throughput (in TFLOP/s): 236.987
MLP duration (in seconds): 0.4882
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7513
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30464x91392, b=2048): 0.1781
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30464x91392, b=2048): 256.067
Elapsed time for attention_key_query_prob (512x2048x238x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x238x2048): 99.363
Elapsed time for attention_prob_times_values (512x2048x2048x238): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x238): 136.703
Elapsed time for attention_linear_projection (4x30464x30464, b=2048): 0.0584
Throughput (in TFLOP/s) for attention_linear_projection (4x30464x30464, b=2048): 260.361
Elapsed time for mlp_h_to_4h (4x30464x121856, b=2048): 0.2413
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30464x121856, b=2048): 252.067
Elapsed time for mlp_4h_to_h (4x121856x30464, b=2048): 0.2496
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121856x30464, b=2048): 243.704

Attention duration (in seconds): 0.2543
Attention throughput (in TFLOP/s): 247.205
MLP duration (in seconds): 0.4909
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7452
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30592x91776, b=2048): 0.1824
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30592x91776, b=2048): 252.214
Elapsed time for attention_key_query_prob (512x2048x239x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x239x2048): 68.696
Elapsed time for attention_prob_times_values (512x2048x2048x239): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x239): 86.595
Elapsed time for attention_linear_projection (4x30592x30592, b=2048): 0.0594
Throughput (in TFLOP/s) for attention_linear_projection (4x30592x30592, b=2048): 258.299
Elapsed time for mlp_h_to_4h (4x30592x122368, b=2048): 0.2429
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30592x122368, b=2048): 252.456
Elapsed time for mlp_4h_to_h (4x122368x30592, b=2048): 0.2525
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122368x30592, b=2048): 242.888

Attention duration (in seconds): 0.2685
Attention throughput (in TFLOP/s): 236.037
MLP duration (in seconds): 0.4955
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7640
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30720x92160, b=2048): 0.1825
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30720x92160, b=2048): 254.139
Elapsed time for attention_key_query_prob (512x2048x240x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x240x2048): 141.276
Elapsed time for attention_prob_times_values (512x2048x2048x240): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x240): 212.831
Elapsed time for attention_linear_projection (4x30720x30720, b=2048): 0.0598
Throughput (in TFLOP/s) for attention_linear_projection (4x30720x30720, b=2048): 258.493
Elapsed time for mlp_h_to_4h (4x30720x122880, b=2048): 0.2477
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30720x122880, b=2048): 249.661
Elapsed time for mlp_4h_to_h (4x122880x30720, b=2048): 0.2535
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122880x30720, b=2048): 243.935

Attention duration (in seconds): 0.2545
Attention throughput (in TFLOP/s): 251.140
MLP duration (in seconds): 0.5013
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7557
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30848x92544, b=2048): 0.1858
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30848x92544, b=2048): 251.741
Elapsed time for attention_key_query_prob (512x2048x241x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x241x2048): 69.644
Elapsed time for attention_prob_times_values (512x2048x2048x241): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x241): 87.498
Elapsed time for attention_linear_projection (4x30848x30848, b=2048): 0.0604
Throughput (in TFLOP/s) for attention_linear_projection (4x30848x30848, b=2048): 257.980
Elapsed time for mlp_h_to_4h (4x30848x123392, b=2048): 0.2470
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30848x123392, b=2048): 252.454
Elapsed time for mlp_4h_to_h (4x123392x30848, b=2048): 0.2576
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123392x30848, b=2048): 242.079

Attention duration (in seconds): 0.2729
Attention throughput (in TFLOP/s): 236.087
MLP duration (in seconds): 0.5046
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7776
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30976x92928, b=2048): 0.1853
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30976x92928, b=2048): 254.571
Elapsed time for attention_key_query_prob (512x2048x242x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x242x2048): 100.977
Elapsed time for attention_prob_times_values (512x2048x2048x242): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x242): 137.302
Elapsed time for attention_linear_projection (4x30976x30976, b=2048): 0.0615
Throughput (in TFLOP/s) for attention_linear_projection (4x30976x30976, b=2048): 255.519
Elapsed time for mlp_h_to_4h (4x30976x123904, b=2048): 0.2506
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30976x123904, b=2048): 250.956
Elapsed time for mlp_4h_to_h (4x123904x30976, b=2048): 0.2585
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123904x30976, b=2048): 243.247

Attention duration (in seconds): 0.2646
Attention throughput (in TFLOP/s): 245.463
MLP duration (in seconds): 0.5091
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7737
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31104x93312, b=2048): 0.1875
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31104x93312, b=2048): 253.583
Elapsed time for attention_key_query_prob (512x2048x243x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x243x2048): 70.300
Elapsed time for attention_prob_times_values (512x2048x2048x243): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x243): 90.198
Elapsed time for attention_linear_projection (4x31104x31104, b=2048): 0.0612
Throughput (in TFLOP/s) for attention_linear_projection (4x31104x31104, b=2048): 259.137
Elapsed time for mlp_h_to_4h (4x31104x124416, b=2048): 0.2495
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31104x124416, b=2048): 254.094
Elapsed time for mlp_4h_to_h (4x124416x31104, b=2048): 0.2605
Throughput (in TFLOP/s) for mlp_4h_to_h (4x124416x31104, b=2048): 243.403

Attention duration (in seconds): 0.2751
Attention throughput (in TFLOP/s): 238.055
MLP duration (in seconds): 0.5100
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7851
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31232x93696, b=2048): 0.1895
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31232x93696, b=2048): 252.964
Elapsed time for attention_key_query_prob (512x2048x244x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x244x2048): 102.350
Elapsed time for attention_prob_times_values (512x2048x2048x244): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x244): 138.822
Elapsed time for attention_linear_projection (4x31232x31232, b=2048): 0.0617
Throughput (in TFLOP/s) for attention_linear_projection (4x31232x31232, b=2048): 258.924
Elapsed time for mlp_h_to_4h (4x31232x124928, b=2048): 0.2514
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31232x124928, b=2048): 254.280
Elapsed time for mlp_4h_to_h (4x124928x31232, b=2048): 0.2745
Throughput (in TFLOP/s) for mlp_4h_to_h (4x124928x31232, b=2048): 232.910

Attention duration (in seconds): 0.2690
Attention throughput (in TFLOP/s): 245.397
MLP duration (in seconds): 0.5259
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7949
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31360x94080, b=2048): 0.1891
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31360x94080, b=2048): 255.631
Elapsed time for attention_key_query_prob (512x2048x245x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x245x2048): 71.368
Elapsed time for attention_prob_times_values (512x2048x2048x245): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x245): 90.832
Elapsed time for attention_linear_projection (4x31360x31360, b=2048): 0.0620
Throughput (in TFLOP/s) for attention_linear_projection (4x31360x31360, b=2048): 259.872
Elapsed time for mlp_h_to_4h (4x31360x125440, b=2048): 0.2553
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31360x125440, b=2048): 252.492
Elapsed time for mlp_4h_to_h (4x125440x31360, b=2048): 0.2670
Throughput (in TFLOP/s) for mlp_4h_to_h (4x125440x31360, b=2048): 241.352

Attention duration (in seconds): 0.2774
Attention throughput (in TFLOP/s): 239.904
MLP duration (in seconds): 0.5223
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7997
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31488x94464, b=2048): 0.1905
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31488x94464, b=2048): 255.872
Elapsed time for attention_key_query_prob (512x2048x246x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x246x2048): 102.863
Elapsed time for attention_prob_times_values (512x2048x2048x246): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x246): 142.985
Elapsed time for attention_linear_projection (4x31488x31488, b=2048): 0.0625
Throughput (in TFLOP/s) for attention_linear_projection (4x31488x31488, b=2048): 259.972
Elapsed time for mlp_h_to_4h (4x31488x125952, b=2048): 0.2595
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31488x125952, b=2048): 250.419
Elapsed time for mlp_4h_to_h (4x125952x31488, b=2048): 0.2671
Throughput (in TFLOP/s) for mlp_4h_to_h (4x125952x31488, b=2048): 243.262

Attention duration (in seconds): 0.2706
Attention throughput (in TFLOP/s): 247.928
MLP duration (in seconds): 0.5266
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7972
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31616x94848, b=2048): 0.1969
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31616x94848, b=2048): 249.546
Elapsed time for attention_key_query_prob (512x2048x247x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x247x2048): 70.469
Elapsed time for attention_prob_times_values (512x2048x2048x247): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x247): 90.438
Elapsed time for attention_linear_projection (4x31616x31616, b=2048): 0.0638
Throughput (in TFLOP/s) for attention_linear_projection (4x31616x31616, b=2048): 256.575
Elapsed time for mlp_h_to_4h (4x31616x126464, b=2048): 0.3276
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31616x126464, b=2048): 199.944
Elapsed time for mlp_4h_to_h (4x126464x31616, b=2048): 0.2766
Throughput (in TFLOP/s) for mlp_4h_to_h (4x126464x31616, b=2048): 236.825

Attention duration (in seconds): 0.2875
Attention throughput (in TFLOP/s): 235.238
MLP duration (in seconds): 0.6042
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8917
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31744x95232, b=2048): 0.1991
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31744x95232, b=2048): 248.783
Elapsed time for attention_key_query_prob (512x2048x248x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x248x2048): 128.931
Elapsed time for attention_prob_times_values (512x2048x2048x248): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x248): 216.368
Elapsed time for attention_linear_projection (4x31744x31744, b=2048): 0.0642
Throughput (in TFLOP/s) for attention_linear_projection (4x31744x31744, b=2048): 257.217
Elapsed time for mlp_h_to_4h (4x31744x126976, b=2048): 0.2682
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31744x126976, b=2048): 246.203
Elapsed time for mlp_4h_to_h (4x126976x31744, b=2048): 0.2733
Throughput (in TFLOP/s) for mlp_4h_to_h (4x126976x31744, b=2048): 241.615

Attention duration (in seconds): 0.2765
Attention throughput (in TFLOP/s): 246.582
MLP duration (in seconds): 0.5416
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31872x95616, b=2048): 0.1967
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31872x95616, b=2048): 253.878
Elapsed time for attention_key_query_prob (512x2048x249x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x249x2048): 71.395
Elapsed time for attention_prob_times_values (512x2048x2048x249): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x249): 91.267
Elapsed time for attention_linear_projection (4x31872x31872, b=2048): 0.0644
Throughput (in TFLOP/s) for attention_linear_projection (4x31872x31872, b=2048): 258.380
Elapsed time for mlp_h_to_4h (4x31872x127488, b=2048): 0.2650
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31872x127488, b=2048): 251.200
Elapsed time for mlp_4h_to_h (4x127488x31872, b=2048): 0.2739
Throughput (in TFLOP/s) for mlp_4h_to_h (4x127488x31872, b=2048): 243.085

Attention duration (in seconds): 0.2878
Attention throughput (in TFLOP/s): 238.766
MLP duration (in seconds): 0.5389
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32000x96000, b=2048): 0.1975
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32000x96000, b=2048): 254.781
Elapsed time for attention_key_query_prob (512x2048x250x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x250x2048): 105.062
Elapsed time for attention_prob_times_values (512x2048x2048x250): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x250): 146.325
Elapsed time for attention_linear_projection (4x32000x32000, b=2048): 0.0647
Throughput (in TFLOP/s) for attention_linear_projection (4x32000x32000, b=2048): 259.426
Elapsed time for mlp_h_to_4h (4x32000x128000, b=2048): 0.3174
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32000x128000, b=2048): 211.441
Elapsed time for mlp_4h_to_h (4x128000x32000, b=2048): 0.2750
Throughput (in TFLOP/s) for mlp_4h_to_h (4x128000x32000, b=2048): 244.049

Attention duration (in seconds): 0.2798
Attention throughput (in TFLOP/s): 247.541
MLP duration (in seconds): 0.5924
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8721
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32128x96384, b=2048): 0.2010
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32128x96384, b=2048): 252.362
Elapsed time for attention_key_query_prob (512x2048x251x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x251x2048): 72.992
Elapsed time for attention_prob_times_values (512x2048x2048x251): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x251): 94.494
Elapsed time for attention_linear_projection (4x32128x32128, b=2048): 0.0652
Throughput (in TFLOP/s) for attention_linear_projection (4x32128x32128, b=2048): 259.476
Elapsed time for mlp_h_to_4h (4x32128x128512, b=2048): 0.2685
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32128x128512, b=2048): 251.967
Elapsed time for mlp_4h_to_h (4x128512x32128, b=2048): 0.2778
Throughput (in TFLOP/s) for mlp_4h_to_h (4x128512x32128, b=2048): 243.534

Attention duration (in seconds): 0.2924
Attention throughput (in TFLOP/s): 238.728
MLP duration (in seconds): 0.5462
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8386
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32256x96768, b=2048): 0.2041
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32256x96768, b=2048): 250.600
Elapsed time for attention_key_query_prob (512x2048x252x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x252x2048): 106.165
Elapsed time for attention_prob_times_values (512x2048x2048x252): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x252): 149.418
Elapsed time for attention_linear_projection (4x32256x32256, b=2048): 0.0661
Throughput (in TFLOP/s) for attention_linear_projection (4x32256x32256, b=2048): 257.761
Elapsed time for mlp_h_to_4h (4x32256x129024, b=2048): 0.2715
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32256x129024, b=2048): 251.194
Elapsed time for mlp_4h_to_h (4x129024x32256, b=2048): 0.2814
Throughput (in TFLOP/s) for mlp_4h_to_h (4x129024x32256, b=2048): 242.326

Attention duration (in seconds): 0.2876
Attention throughput (in TFLOP/s): 244.579
MLP duration (in seconds): 0.5528
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8405
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32384x97152, b=2048): 0.2021
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32384x97152, b=2048): 255.028
Elapsed time for attention_key_query_prob (512x2048x253x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x253x2048): 73.609
Elapsed time for attention_prob_times_values (512x2048x2048x253): 0.0259
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x253): 41.985
Elapsed time for attention_linear_projection (4x32384x32384, b=2048): 0.0675
Throughput (in TFLOP/s) for attention_linear_projection (4x32384x32384, b=2048): 254.411
Elapsed time for mlp_h_to_4h (4x32384x129536, b=2048): 0.2743
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32384x129536, b=2048): 250.602
Elapsed time for mlp_4h_to_h (4x129536x32384, b=2048): 0.2824
Throughput (in TFLOP/s) for mlp_4h_to_h (4x129536x32384, b=2048): 243.342

Attention duration (in seconds): 0.3103
Attention throughput (in TFLOP/s): 228.494
MLP duration (in seconds): 0.5567
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8670
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32512x97536, b=2048): 0.2052
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32512x97536, b=2048): 253.175
Elapsed time for attention_key_query_prob (512x2048x254x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x254x2048): 107.756
Elapsed time for attention_prob_times_values (512x2048x2048x254): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x254): 145.968
Elapsed time for attention_linear_projection (4x32512x32512, b=2048): 0.0674
Throughput (in TFLOP/s) for attention_linear_projection (4x32512x32512, b=2048): 257.094
Elapsed time for mlp_h_to_4h (4x32512x130048, b=2048): 0.3083
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32512x130048, b=2048): 224.692
Elapsed time for mlp_4h_to_h (4x130048x32512, b=2048): 0.2844
Throughput (in TFLOP/s) for mlp_4h_to_h (4x130048x32512, b=2048): 243.548

Attention duration (in seconds): 0.2902
Attention throughput (in TFLOP/s): 246.250
MLP duration (in seconds): 0.5927
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8829
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32640x97920, b=2048): 0.2054
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32640x97920, b=2048): 254.978
Elapsed time for attention_key_query_prob (512x2048x255x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x255x2048): 73.571
Elapsed time for attention_prob_times_values (512x2048x2048x255): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x255): 94.867
Elapsed time for attention_linear_projection (4x32640x32640, b=2048): 0.0678
Throughput (in TFLOP/s) for attention_linear_projection (4x32640x32640, b=2048): 257.441
Elapsed time for mlp_h_to_4h (4x32640x130560, b=2048): 0.2781
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32640x130560, b=2048): 251.048
Elapsed time for mlp_4h_to_h (4x130560x32640, b=2048): 0.2888
Throughput (in TFLOP/s) for mlp_4h_to_h (4x130560x32640, b=2048): 241.728

Attention duration (in seconds): 0.2996
Attention throughput (in TFLOP/s): 240.352
MLP duration (in seconds): 0.5670
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8666
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8192x12288, b=2048): 0.0072
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8192x12288, b=2048): 229.361
Elapsed time for attention_key_query_prob (256x2048x64x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x64x2048): 61.131
Elapsed time for attention_prob_times_values (256x2048x2048x64): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x64): 80.253
Elapsed time for attention_linear_projection (4x4096x8192, b=2048): 0.0024
Throughput (in TFLOP/s) for attention_linear_projection (4x4096x8192, b=2048): 231.023
Elapsed time for mlp_h_to_4h (4x8192x16384, b=2048): 0.0092
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8192x16384, b=2048): 239.923
Elapsed time for mlp_4h_to_h (4x16384x8192, b=2048): 0.0089
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16384x8192, b=2048): 246.239

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 182.829
MLP duration (in seconds): 0.0181
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0316
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8320x12480, b=2048): 0.0069
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8320x12480, b=2048): 246.550
Elapsed time for attention_key_query_prob (256x2048x65x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x65x2048): 26.858
Elapsed time for attention_prob_times_values (256x2048x2048x65): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x65): 41.239
Elapsed time for attention_linear_projection (4x4160x8320, b=2048): 0.0023
Throughput (in TFLOP/s) for attention_linear_projection (4x4160x8320, b=2048): 245.278
Elapsed time for mlp_h_to_4h (4x8320x16640, b=2048): 0.0093
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8320x16640, b=2048): 243.502
Elapsed time for mlp_4h_to_h (4x16640x8320, b=2048): 0.0093
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16640x8320, b=2048): 243.914

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 143.163
MLP duration (in seconds): 0.0186
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0364
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8448x12672, b=2048): 0.0072
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8448x12672, b=2048): 243.791
Elapsed time for attention_key_query_prob (256x2048x66x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x66x2048): 19.645
Elapsed time for attention_prob_times_values (256x2048x2048x66): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x66): 69.831
Elapsed time for attention_linear_projection (4x4224x8448, b=2048): 0.0024
Throughput (in TFLOP/s) for attention_linear_projection (4x4224x8448, b=2048): 244.707
Elapsed time for mlp_h_to_4h (4x8448x16896, b=2048): 0.0096
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8448x16896, b=2048): 244.878
Elapsed time for mlp_4h_to_h (4x16896x8448, b=2048): 0.0097
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16896x8448, b=2048): 242.002

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 139.264
MLP duration (in seconds): 0.0192
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0380
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8576x12864, b=2048): 0.0076
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8576x12864, b=2048): 238.622
Elapsed time for attention_key_query_prob (256x2048x67x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x67x2048): 27.735
Elapsed time for attention_prob_times_values (256x2048x2048x67): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x67): 42.658
Elapsed time for attention_linear_projection (4x4288x8576, b=2048): 0.0024
Throughput (in TFLOP/s) for attention_linear_projection (4x4288x8576, b=2048): 252.683
Elapsed time for mlp_h_to_4h (4x8576x17152, b=2048): 0.0102
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8576x17152, b=2048): 235.581
Elapsed time for mlp_4h_to_h (4x17152x8576, b=2048): 0.0105
Throughput (in TFLOP/s) for mlp_4h_to_h (4x17152x8576, b=2048): 229.042

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 145.669
MLP duration (in seconds): 0.0208
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0393
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8704x13056, b=2048): 0.0077
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8704x13056, b=2048): 241.048
Elapsed time for attention_key_query_prob (256x2048x68x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x68x2048): 19.124
Elapsed time for attention_prob_times_values (256x2048x2048x68): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x68): 72.007
Elapsed time for attention_linear_projection (4x4352x8704, b=2048): 0.0026
Throughput (in TFLOP/s) for attention_linear_projection (4x4352x8704, b=2048): 242.170
Elapsed time for mlp_h_to_4h (4x8704x17408, b=2048): 0.0101
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8704x17408, b=2048): 246.358
Elapsed time for mlp_4h_to_h (4x17408x8704, b=2048): 0.0109
Throughput (in TFLOP/s) for mlp_4h_to_h (4x17408x8704, b=2048): 228.345

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 139.071
MLP duration (in seconds): 0.0209
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0409
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8832x13248, b=2048): 0.0079
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8832x13248, b=2048): 243.699
Elapsed time for attention_key_query_prob (256x2048x69x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x69x2048): 28.242
Elapsed time for attention_prob_times_values (256x2048x2048x69): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x69): 43.605
Elapsed time for attention_linear_projection (4x4416x8832, b=2048): 0.0026
Throughput (in TFLOP/s) for attention_linear_projection (4x4416x8832, b=2048): 243.522
Elapsed time for mlp_h_to_4h (4x8832x17664, b=2048): 0.0106
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8832x17664, b=2048): 240.971
Elapsed time for mlp_4h_to_h (4x17664x8832, b=2048): 0.0106
Throughput (in TFLOP/s) for mlp_4h_to_h (4x17664x8832, b=2048): 240.836

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 149.065
MLP duration (in seconds): 0.0212
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0404
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8960x13440, b=2048): 0.0082
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8960x13440, b=2048): 241.991
Elapsed time for attention_key_query_prob (256x2048x70x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x70x2048): 20.135
Elapsed time for attention_prob_times_values (256x2048x2048x70): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x70): 73.289
Elapsed time for attention_linear_projection (4x4480x8960, b=2048): 0.0027
Throughput (in TFLOP/s) for attention_linear_projection (4x4480x8960, b=2048): 247.196
Elapsed time for mlp_h_to_4h (4x8960x17920, b=2048): 0.0107
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8960x17920, b=2048): 244.755
Elapsed time for mlp_4h_to_h (4x17920x8960, b=2048): 0.0109
Throughput (in TFLOP/s) for mlp_4h_to_h (4x17920x8960, b=2048): 241.287

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 144.182
MLP duration (in seconds): 0.0217
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0420
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9088x13632, b=2048): 0.0085
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9088x13632, b=2048): 237.482
Elapsed time for attention_key_query_prob (256x2048x71x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x71x2048): 27.984
Elapsed time for attention_prob_times_values (256x2048x2048x71): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x71): 44.297
Elapsed time for attention_linear_projection (4x4544x9088, b=2048): 0.0028
Throughput (in TFLOP/s) for attention_linear_projection (4x4544x9088, b=2048): 242.425
Elapsed time for mlp_h_to_4h (4x9088x18176, b=2048): 0.0110
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9088x18176, b=2048): 246.030
Elapsed time for mlp_4h_to_h (4x18176x9088, b=2048): 0.0112
Throughput (in TFLOP/s) for mlp_4h_to_h (4x18176x9088, b=2048): 241.005

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 148.864
MLP duration (in seconds): 0.0222
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0425
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9216x13824, b=2048): 0.0088
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9216x13824, b=2048): 237.514
Elapsed time for attention_key_query_prob (256x2048x72x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x72x2048): 68.172
Elapsed time for attention_prob_times_values (256x2048x2048x72): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x72): 84.322
Elapsed time for attention_linear_projection (4x4608x9216, b=2048): 0.0032
Throughput (in TFLOP/s) for attention_linear_projection (4x4608x9216, b=2048): 217.235
Elapsed time for mlp_h_to_4h (4x9216x18432, b=2048): 0.0118
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9216x18432, b=2048): 236.638
Elapsed time for mlp_4h_to_h (4x18432x9216, b=2048): 0.0113
Throughput (in TFLOP/s) for mlp_4h_to_h (4x18432x9216, b=2048): 246.221

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 192.156
MLP duration (in seconds): 0.0231
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0392
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9344x14016, b=2048): 0.0086
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9344x14016, b=2048): 249.408
Elapsed time for attention_key_query_prob (256x2048x73x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x73x2048): 28.391
Elapsed time for attention_prob_times_values (256x2048x2048x73): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x73): 43.272
Elapsed time for attention_linear_projection (4x4672x9344, b=2048): 0.0030
Throughput (in TFLOP/s) for attention_linear_projection (4x4672x9344, b=2048): 237.658
Elapsed time for mlp_h_to_4h (4x9344x18688, b=2048): 0.0117
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9344x18688, b=2048): 244.088
Elapsed time for mlp_4h_to_h (4x18688x9344, b=2048): 0.0116
Throughput (in TFLOP/s) for mlp_4h_to_h (4x18688x9344, b=2048): 246.454

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 152.934
MLP duration (in seconds): 0.0233
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0441
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9472x14208, b=2048): 0.0094
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9472x14208, b=2048): 234.582
Elapsed time for attention_key_query_prob (256x2048x74x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x74x2048): 50.671
Elapsed time for attention_prob_times_values (256x2048x2048x74): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x74): 75.588
Elapsed time for attention_linear_projection (4x4736x9472, b=2048): 0.0030
Throughput (in TFLOP/s) for attention_linear_projection (4x4736x9472, b=2048): 248.666
Elapsed time for mlp_h_to_4h (4x9472x18944, b=2048): 0.0123
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9472x18944, b=2048): 239.383
Elapsed time for mlp_4h_to_h (4x18944x9472, b=2048): 0.0121
Throughput (in TFLOP/s) for mlp_4h_to_h (4x18944x9472, b=2048): 243.342

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 185.166
MLP duration (in seconds): 0.0244
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0420
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9600x14400, b=2048): 0.0096
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9600x14400, b=2048): 235.808
Elapsed time for attention_key_query_prob (256x2048x75x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x75x2048): 29.189
Elapsed time for attention_prob_times_values (256x2048x2048x75): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x75): 46.825
Elapsed time for attention_linear_projection (4x4800x9600, b=2048): 0.0032
Throughput (in TFLOP/s) for attention_linear_projection (4x4800x9600, b=2048): 239.549
Elapsed time for mlp_h_to_4h (4x9600x19200, b=2048): 0.0125
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9600x19200, b=2048): 240.751
Elapsed time for mlp_4h_to_h (4x19200x9600, b=2048): 0.0120
Throughput (in TFLOP/s) for mlp_4h_to_h (4x19200x9600, b=2048): 251.897

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 153.909
MLP duration (in seconds): 0.0245
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0462
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9728x14592, b=2048): 0.0097
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9728x14592, b=2048): 239.888
Elapsed time for attention_key_query_prob (256x2048x76x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x76x2048): 51.769
Elapsed time for attention_prob_times_values (256x2048x2048x76): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x76): 72.118
Elapsed time for attention_linear_projection (4x4864x9728, b=2048): 0.0031
Throughput (in TFLOP/s) for attention_linear_projection (4x4864x9728, b=2048): 246.632
Elapsed time for mlp_h_to_4h (4x9728x19456, b=2048): 0.0128
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9728x19456, b=2048): 242.575
Elapsed time for mlp_4h_to_h (4x19456x9728, b=2048): 0.0126
Throughput (in TFLOP/s) for mlp_4h_to_h (4x19456x9728, b=2048): 245.686

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 187.760
MLP duration (in seconds): 0.0254
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0437
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9856x14784, b=2048): 0.0101
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9856x14784, b=2048): 236.400
Elapsed time for attention_key_query_prob (256x2048x77x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x77x2048): 29.856
Elapsed time for attention_prob_times_values (256x2048x2048x77): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x77): 48.060
Elapsed time for attention_linear_projection (4x4928x9856, b=2048): 0.0032
Throughput (in TFLOP/s) for attention_linear_projection (4x4928x9856, b=2048): 247.331
Elapsed time for mlp_h_to_4h (4x9856x19712, b=2048): 0.0134
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9856x19712, b=2048): 236.950
Elapsed time for mlp_4h_to_h (4x19712x9856, b=2048): 0.0131
Throughput (in TFLOP/s) for mlp_4h_to_h (4x19712x9856, b=2048): 243.701

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 157.604
MLP duration (in seconds): 0.0265
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0488
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9984x14976, b=2048): 0.0103
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9984x14976, b=2048): 237.731
Elapsed time for attention_key_query_prob (256x2048x78x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x78x2048): 52.364
Elapsed time for attention_prob_times_values (256x2048x2048x78): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x78): 80.256
Elapsed time for attention_linear_projection (4x4992x9984, b=2048): 0.0034
Throughput (in TFLOP/s) for attention_linear_projection (4x4992x9984, b=2048): 242.907
Elapsed time for mlp_h_to_4h (4x9984x19968, b=2048): 0.0136
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9984x19968, b=2048): 240.156
Elapsed time for mlp_4h_to_h (4x19968x9984, b=2048): 0.0132
Throughput (in TFLOP/s) for mlp_4h_to_h (4x19968x9984, b=2048): 248.080

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 190.020
MLP duration (in seconds): 0.0268
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0457
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10112x15168, b=2048): 0.0106
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10112x15168, b=2048): 236.750
Elapsed time for attention_key_query_prob (256x2048x79x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x79x2048): 29.730
Elapsed time for attention_prob_times_values (256x2048x2048x79): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x79): 48.989
Elapsed time for attention_linear_projection (4x5056x10112, b=2048): 0.0036
Throughput (in TFLOP/s) for attention_linear_projection (4x5056x10112, b=2048): 231.615
Elapsed time for mlp_h_to_4h (4x10112x20224, b=2048): 0.0141
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10112x20224, b=2048): 238.292
Elapsed time for mlp_4h_to_h (4x20224x10112, b=2048): 0.0135
Throughput (in TFLOP/s) for mlp_4h_to_h (4x20224x10112, b=2048): 248.937

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 157.687
MLP duration (in seconds): 0.0275
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0509
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10240x15360, b=2048): 0.0107
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10240x15360, b=2048): 240.368
Elapsed time for attention_key_query_prob (256x2048x80x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x80x2048): 78.247
Elapsed time for attention_prob_times_values (256x2048x2048x80): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x80): 92.679
Elapsed time for attention_linear_projection (4x5120x10240, b=2048): 0.0035
Throughput (in TFLOP/s) for attention_linear_projection (4x5120x10240, b=2048): 244.578
Elapsed time for mlp_h_to_4h (4x10240x20480, b=2048): 0.0143
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10240x20480, b=2048): 240.927
Elapsed time for mlp_4h_to_h (4x20480x10240, b=2048): 0.0137
Throughput (in TFLOP/s) for mlp_4h_to_h (4x20480x10240, b=2048): 251.225

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 206.733
MLP duration (in seconds): 0.0279
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0462
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10368x15552, b=2048): 0.0113
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10368x15552, b=2048): 234.241
Elapsed time for attention_key_query_prob (256x2048x81x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x81x2048): 30.597
Elapsed time for attention_prob_times_values (256x2048x2048x81): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x81): 49.605
Elapsed time for attention_linear_projection (4x5184x10368, b=2048): 0.0035
Throughput (in TFLOP/s) for attention_linear_projection (4x5184x10368, b=2048): 248.471
Elapsed time for mlp_h_to_4h (4x10368x20736, b=2048): 0.0149
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10368x20736, b=2048): 235.669
Elapsed time for mlp_4h_to_h (4x20736x10368, b=2048): 0.0142
Throughput (in TFLOP/s) for mlp_4h_to_h (4x20736x10368, b=2048): 248.354

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 161.169
MLP duration (in seconds): 0.0291
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0531
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10496x15744, b=2048): 0.0115
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10496x15744, b=2048): 235.603
Elapsed time for attention_key_query_prob (256x2048x82x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x82x2048): 55.558
Elapsed time for attention_prob_times_values (256x2048x2048x82): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x82): 84.256
Elapsed time for attention_linear_projection (4x5248x10496, b=2048): 0.0040
Throughput (in TFLOP/s) for attention_linear_projection (4x5248x10496, b=2048): 226.718
Elapsed time for mlp_h_to_4h (4x10496x20992, b=2048): 0.0150
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10496x20992, b=2048): 241.016
Elapsed time for mlp_4h_to_h (4x20992x10496, b=2048): 0.0146
Throughput (in TFLOP/s) for mlp_4h_to_h (4x20992x10496, b=2048): 247.412

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 191.114
MLP duration (in seconds): 0.0296
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0503
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10624x15936, b=2048): 0.0118
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10624x15936, b=2048): 234.203
Elapsed time for attention_key_query_prob (256x2048x83x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x83x2048): 31.094
Elapsed time for attention_prob_times_values (256x2048x2048x83): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x83): 51.104
Elapsed time for attention_linear_projection (4x5312x10624, b=2048): 0.0037
Throughput (in TFLOP/s) for attention_linear_projection (4x5312x10624, b=2048): 247.300
Elapsed time for mlp_h_to_4h (4x10624x21248, b=2048): 0.0155
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10624x21248, b=2048): 238.216
Elapsed time for mlp_4h_to_h (4x21248x10624, b=2048): 0.0148
Throughput (in TFLOP/s) for mlp_4h_to_h (4x21248x10624, b=2048): 250.430

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 163.488
MLP duration (in seconds): 0.0303
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0551
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10752x16128, b=2048): 0.0121
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10752x16128, b=2048): 235.133
Elapsed time for attention_key_query_prob (256x2048x84x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x84x2048): 56.802
Elapsed time for attention_prob_times_values (256x2048x2048x84): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x84): 86.282
Elapsed time for attention_linear_projection (4x5376x10752, b=2048): 0.0038
Throughput (in TFLOP/s) for attention_linear_projection (4x5376x10752, b=2048): 249.775
Elapsed time for mlp_h_to_4h (4x10752x21504, b=2048): 0.0159
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10752x21504, b=2048): 238.190
Elapsed time for mlp_4h_to_h (4x21504x10752, b=2048): 0.0152
Throughput (in TFLOP/s) for mlp_4h_to_h (4x21504x10752, b=2048): 249.869

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 196.250
MLP duration (in seconds): 0.0311
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0522
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10880x16320, b=2048): 0.0140
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10880x16320, b=2048): 207.937
Elapsed time for attention_key_query_prob (256x2048x85x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x85x2048): 31.581
Elapsed time for attention_prob_times_values (256x2048x2048x85): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x85): 52.666
Elapsed time for attention_linear_projection (4x5440x10880, b=2048): 0.0040
Throughput (in TFLOP/s) for attention_linear_projection (4x5440x10880, b=2048): 244.533
Elapsed time for mlp_h_to_4h (4x10880x21760, b=2048): 0.0166
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10880x21760, b=2048): 233.027
Elapsed time for mlp_4h_to_h (4x21760x10880, b=2048): 0.0157
Throughput (in TFLOP/s) for mlp_4h_to_h (4x21760x10880, b=2048): 246.904

Attention duration (in seconds): 0.0272
Attention throughput (in TFLOP/s): 156.016
MLP duration (in seconds): 0.0324
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0596
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11008x16512, b=2048): 0.0129
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11008x16512, b=2048): 230.588
Elapsed time for attention_key_query_prob (256x2048x86x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x86x2048): 58.303
Elapsed time for attention_prob_times_values (256x2048x2048x86): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x86): 88.316
Elapsed time for attention_linear_projection (4x5504x11008, b=2048): 0.0040
Throughput (in TFLOP/s) for attention_linear_projection (4x5504x11008, b=2048): 247.862
Elapsed time for mlp_h_to_4h (4x11008x22016, b=2048): 0.0166
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11008x22016, b=2048): 239.862
Elapsed time for mlp_4h_to_h (4x22016x11008, b=2048): 0.0161
Throughput (in TFLOP/s) for mlp_4h_to_h (4x22016x11008, b=2048): 246.457

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 195.687
MLP duration (in seconds): 0.0327
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0548
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11136x16704, b=2048): 0.0131
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11136x16704, b=2048): 233.099
Elapsed time for attention_key_query_prob (256x2048x87x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x87x2048): 31.888
Elapsed time for attention_prob_times_values (256x2048x2048x87): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x87): 53.207
Elapsed time for attention_linear_projection (4x5568x11136, b=2048): 0.0040
Throughput (in TFLOP/s) for attention_linear_projection (4x5568x11136, b=2048): 252.696
Elapsed time for mlp_h_to_4h (4x11136x22272, b=2048): 0.0170
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11136x22272, b=2048): 238.830
Elapsed time for mlp_4h_to_h (4x22272x11136, b=2048): 0.0171
Throughput (in TFLOP/s) for mlp_4h_to_h (4x22272x11136, b=2048): 237.737

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 167.663
MLP duration (in seconds): 0.0341
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0606
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11264x16896, b=2048): 0.0132
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11264x16896, b=2048): 236.479
Elapsed time for attention_key_query_prob (256x2048x88x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x88x2048): 76.384
Elapsed time for attention_prob_times_values (256x2048x2048x88): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x88): 99.992
Elapsed time for attention_linear_projection (4x5632x11264, b=2048): 0.0042
Throughput (in TFLOP/s) for attention_linear_projection (4x5632x11264, b=2048): 247.417
Elapsed time for mlp_h_to_4h (4x11264x22528, b=2048): 0.0173
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11264x22528, b=2048): 239.637
Elapsed time for mlp_4h_to_h (4x22528x11264, b=2048): 0.0165
Throughput (in TFLOP/s) for mlp_4h_to_h (4x22528x11264, b=2048): 252.405

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 208.521
MLP duration (in seconds): 0.0338
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0556
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11392x17088, b=2048): 0.0137
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11392x17088, b=2048): 233.218
Elapsed time for attention_key_query_prob (256x2048x89x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x89x2048): 32.514
Elapsed time for attention_prob_times_values (256x2048x2048x89): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x89): 54.537
Elapsed time for attention_linear_projection (4x5696x11392, b=2048): 0.0043
Throughput (in TFLOP/s) for attention_linear_projection (4x5696x11392, b=2048): 249.755
Elapsed time for mlp_h_to_4h (4x11392x22784, b=2048): 0.0178
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11392x22784, b=2048): 239.407
Elapsed time for mlp_4h_to_h (4x22784x11392, b=2048): 0.0173
Throughput (in TFLOP/s) for mlp_4h_to_h (4x22784x11392, b=2048): 245.828

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 169.679
MLP duration (in seconds): 0.0351
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0624
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11520x17280, b=2048): 0.0139
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11520x17280, b=2048): 234.707
Elapsed time for attention_key_query_prob (256x2048x90x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x90x2048): 60.465
Elapsed time for attention_prob_times_values (256x2048x2048x90): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x90): 91.952
Elapsed time for attention_linear_projection (4x5760x11520, b=2048): 0.0044
Throughput (in TFLOP/s) for attention_linear_projection (4x5760x11520, b=2048): 249.379
Elapsed time for mlp_h_to_4h (4x11520x23040, b=2048): 0.0181
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11520x23040, b=2048): 240.567
Elapsed time for mlp_4h_to_h (4x23040x11520, b=2048): 0.0186
Throughput (in TFLOP/s) for mlp_4h_to_h (4x23040x11520, b=2048): 233.284

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 201.037
MLP duration (in seconds): 0.0367
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0603
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11648x17472, b=2048): 0.0141
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11648x17472, b=2048): 236.136
Elapsed time for attention_key_query_prob (256x2048x91x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x91x2048): 33.168
Elapsed time for attention_prob_times_values (256x2048x2048x91): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x91): 55.903
Elapsed time for attention_linear_projection (4x5824x11648, b=2048): 0.0045
Throughput (in TFLOP/s) for attention_linear_projection (4x5824x11648, b=2048): 249.214
Elapsed time for mlp_h_to_4h (4x11648x23296, b=2048): 0.0185
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11648x23296, b=2048): 240.147
Elapsed time for mlp_4h_to_h (4x23296x11648, b=2048): 0.0194
Throughput (in TFLOP/s) for mlp_4h_to_h (4x23296x11648, b=2048): 229.602

Attention duration (in seconds): 0.0280
Attention throughput (in TFLOP/s): 172.936
MLP duration (in seconds): 0.0379
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0658
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11776x17664, b=2048): 0.0145
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11776x17664, b=2048): 235.098
Elapsed time for attention_key_query_prob (256x2048x92x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x92x2048): 61.790
Elapsed time for attention_prob_times_values (256x2048x2048x92): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x92): 93.528
Elapsed time for attention_linear_projection (4x5888x11776, b=2048): 0.0046
Throughput (in TFLOP/s) for attention_linear_projection (4x5888x11776, b=2048): 247.986
Elapsed time for mlp_h_to_4h (4x11776x23552, b=2048): 0.0190
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11776x23552, b=2048): 239.633
Elapsed time for mlp_4h_to_h (4x23552x11776, b=2048): 0.0192
Throughput (in TFLOP/s) for mlp_4h_to_h (4x23552x11776, b=2048): 237.138

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 202.534
MLP duration (in seconds): 0.0381
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0625
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11904x17856, b=2048): 0.0148
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11904x17856, b=2048): 234.520
Elapsed time for attention_key_query_prob (256x2048x93x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x93x2048): 33.853
Elapsed time for attention_prob_times_values (256x2048x2048x93): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x93): 54.415
Elapsed time for attention_linear_projection (4x5952x11904, b=2048): 0.0047
Throughput (in TFLOP/s) for attention_linear_projection (4x5952x11904, b=2048): 247.331
Elapsed time for mlp_h_to_4h (4x11904x23808, b=2048): 0.0194
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11904x23808, b=2048): 239.405
Elapsed time for mlp_4h_to_h (4x23808x11904, b=2048): 0.0198
Throughput (in TFLOP/s) for mlp_4h_to_h (4x23808x11904, b=2048): 234.838

Attention duration (in seconds): 0.0291
Attention throughput (in TFLOP/s): 173.217
MLP duration (in seconds): 0.0392
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0683
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12032x18048, b=2048): 0.0145
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12032x18048, b=2048): 244.803
Elapsed time for attention_key_query_prob (256x2048x94x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x94x2048): 63.256
Elapsed time for attention_prob_times_values (256x2048x2048x94): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x94): 95.475
Elapsed time for attention_linear_projection (4x6016x12032, b=2048): 0.0048
Throughput (in TFLOP/s) for attention_linear_projection (4x6016x12032, b=2048): 246.066
Elapsed time for mlp_h_to_4h (4x12032x24064, b=2048): 0.0196
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12032x24064, b=2048): 242.022
Elapsed time for mlp_4h_to_h (4x24064x12032, b=2048): 0.0193
Throughput (in TFLOP/s) for mlp_4h_to_h (4x24064x12032, b=2048): 246.048

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 208.751
MLP duration (in seconds): 0.0389
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0635
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12160x18240, b=2048): 0.0147
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12160x18240, b=2048): 248.017
Elapsed time for attention_key_query_prob (256x2048x95x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x95x2048): 35.078
Elapsed time for attention_prob_times_values (256x2048x2048x95): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x95): 58.241
Elapsed time for attention_linear_projection (4x6080x12160, b=2048): 0.0049
Throughput (in TFLOP/s) for attention_linear_projection (4x6080x12160, b=2048): 247.702
Elapsed time for mlp_h_to_4h (4x12160x24320, b=2048): 0.0194
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12160x24320, b=2048): 250.034
Elapsed time for mlp_4h_to_h (4x24320x12160, b=2048): 0.0206
Throughput (in TFLOP/s) for mlp_4h_to_h (4x24320x12160, b=2048): 235.623

Attention duration (in seconds): 0.0289
Attention throughput (in TFLOP/s): 182.020
MLP duration (in seconds): 0.0399
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0688
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12288x18432, b=2048): 0.0150
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12288x18432, b=2048): 247.463
Elapsed time for attention_key_query_prob (256x2048x96x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x96x2048): 93.978
Elapsed time for attention_prob_times_values (256x2048x2048x96): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x96): 109.970
Elapsed time for attention_linear_projection (4x6144x12288, b=2048): 0.0049
Throughput (in TFLOP/s) for attention_linear_projection (4x6144x12288, b=2048): 251.157
Elapsed time for mlp_h_to_4h (4x12288x24576, b=2048): 0.0192
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12288x24576, b=2048): 257.250
Elapsed time for mlp_4h_to_h (4x24576x12288, b=2048): 0.0202
Throughput (in TFLOP/s) for mlp_4h_to_h (4x24576x12288, b=2048): 244.433

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 223.441
MLP duration (in seconds): 0.0395
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0635
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12416x18624, b=2048): 0.0155
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12416x18624, b=2048): 244.739
Elapsed time for attention_key_query_prob (256x2048x97x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x97x2048): 32.017
Elapsed time for attention_prob_times_values (256x2048x2048x97): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x97): 58.708
Elapsed time for attention_linear_projection (4x6208x12416, b=2048): 0.0051
Throughput (in TFLOP/s) for attention_linear_projection (4x6208x12416, b=2048): 249.566
Elapsed time for mlp_h_to_4h (4x12416x24832, b=2048): 0.0202
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12416x24832, b=2048): 250.416
Elapsed time for mlp_4h_to_h (4x24832x12416, b=2048): 0.0213
Throughput (in TFLOP/s) for mlp_4h_to_h (4x24832x12416, b=2048): 237.001

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 178.725
MLP duration (in seconds): 0.0415
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0721
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12544x18816, b=2048): 0.0156
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12544x18816, b=2048): 247.263
Elapsed time for attention_key_query_prob (256x2048x98x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x98x2048): 59.654
Elapsed time for attention_prob_times_values (256x2048x2048x98): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x98): 99.058
Elapsed time for attention_linear_projection (4x6272x12544, b=2048): 0.0052
Throughput (in TFLOP/s) for attention_linear_projection (4x6272x12544, b=2048): 249.438
Elapsed time for mlp_h_to_4h (4x12544x25088, b=2048): 0.0204
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12544x25088, b=2048): 253.306
Elapsed time for mlp_4h_to_h (4x25088x12544, b=2048): 0.0206
Throughput (in TFLOP/s) for mlp_4h_to_h (4x25088x12544, b=2048): 250.824

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 210.774
MLP duration (in seconds): 0.0409
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0674
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12672x19008, b=2048): 0.0163
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12672x19008, b=2048): 242.235
Elapsed time for attention_key_query_prob (256x2048x99x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x99x2048): 31.285
Elapsed time for attention_prob_times_values (256x2048x2048x99): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x99): 60.632
Elapsed time for attention_linear_projection (4x6336x12672, b=2048): 0.0056
Throughput (in TFLOP/s) for attention_linear_projection (4x6336x12672, b=2048): 236.274
Elapsed time for mlp_h_to_4h (4x12672x25344, b=2048): 0.0206
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12672x25344, b=2048): 255.111
Elapsed time for mlp_4h_to_h (4x25344x12672, b=2048): 0.0218
Throughput (in TFLOP/s) for mlp_4h_to_h (4x25344x12672, b=2048): 240.877

Attention duration (in seconds): 0.0322
Attention throughput (in TFLOP/s): 176.830
MLP duration (in seconds): 0.0425
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0746
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12800x19200, b=2048): 0.0163
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12800x19200, b=2048): 246.677
Elapsed time for attention_key_query_prob (256x2048x100x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x100x2048): 60.253
Elapsed time for attention_prob_times_values (256x2048x2048x100): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x100): 101.159
Elapsed time for attention_linear_projection (4x6400x12800, b=2048): 0.0053
Throughput (in TFLOP/s) for attention_linear_projection (4x6400x12800, b=2048): 254.383
Elapsed time for mlp_h_to_4h (4x12800x25600, b=2048): 0.0209
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12800x25600, b=2048): 256.569
Elapsed time for mlp_4h_to_h (4x25600x12800, b=2048): 0.0215
Throughput (in TFLOP/s) for mlp_4h_to_h (4x25600x12800, b=2048): 249.283

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 212.495
MLP duration (in seconds): 0.0425
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0697
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12928x19392, b=2048): 0.0168
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12928x19392, b=2048): 244.070
Elapsed time for attention_key_query_prob (256x2048x101x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x101x2048): 31.272
Elapsed time for attention_prob_times_values (256x2048x2048x101): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x101): 62.085
Elapsed time for attention_linear_projection (4x6464x12928, b=2048): 0.0054
Throughput (in TFLOP/s) for attention_linear_projection (4x6464x12928, b=2048): 251.981
Elapsed time for mlp_h_to_4h (4x12928x25856, b=2048): 0.0214
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12928x25856, b=2048): 256.026
Elapsed time for mlp_4h_to_h (4x25856x12928, b=2048): 0.0217
Throughput (in TFLOP/s) for mlp_4h_to_h (4x25856x12928, b=2048): 252.322

Attention duration (in seconds): 0.0327
Attention throughput (in TFLOP/s): 180.791
MLP duration (in seconds): 0.0431
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0758
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x19584, b=2048): 0.0170
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x19584, b=2048): 246.393
Elapsed time for attention_key_query_prob (256x2048x102x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x102x2048): 61.689
Elapsed time for attention_prob_times_values (256x2048x2048x102): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x102): 102.652
Elapsed time for attention_linear_projection (4x6528x13056, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_linear_projection (4x6528x13056, b=2048): 251.716
Elapsed time for mlp_h_to_4h (4x13056x26112, b=2048): 0.0220
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x26112, b=2048): 253.890
Elapsed time for mlp_4h_to_h (4x26112x13056, b=2048): 0.0221
Throughput (in TFLOP/s) for mlp_4h_to_h (4x26112x13056, b=2048): 252.696

Attention duration (in seconds): 0.0282
Attention throughput (in TFLOP/s): 213.347
MLP duration (in seconds): 0.0441
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0723
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13184x19776, b=2048): 0.0174
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13184x19776, b=2048): 245.812
Elapsed time for attention_key_query_prob (256x2048x103x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x103x2048): 30.910
Elapsed time for attention_prob_times_values (256x2048x2048x103): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x103): 62.382
Elapsed time for attention_linear_projection (4x6592x13184, b=2048): 0.0057
Throughput (in TFLOP/s) for attention_linear_projection (4x6592x13184, b=2048): 251.245
Elapsed time for mlp_h_to_4h (4x13184x26368, b=2048): 0.0225
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13184x26368, b=2048): 252.632
Elapsed time for mlp_4h_to_h (4x26368x13184, b=2048): 0.0231
Throughput (in TFLOP/s) for mlp_4h_to_h (4x26368x13184, b=2048): 247.010

Attention duration (in seconds): 0.0337
Attention throughput (in TFLOP/s): 181.883
MLP duration (in seconds): 0.0456
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0794
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13312x19968, b=2048): 0.0177
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13312x19968, b=2048): 246.141
Elapsed time for attention_key_query_prob (256x2048x104x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x104x2048): 85.353
Elapsed time for attention_prob_times_values (256x2048x2048x104): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x104): 117.431
Elapsed time for attention_linear_projection (4x6656x13312, b=2048): 0.0057
Throughput (in TFLOP/s) for attention_linear_projection (4x6656x13312, b=2048): 253.070
Elapsed time for mlp_h_to_4h (4x13312x26624, b=2048): 0.0228
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13312x26624, b=2048): 254.575
Elapsed time for mlp_4h_to_h (4x26624x13312, b=2048): 0.0230
Throughput (in TFLOP/s) for mlp_4h_to_h (4x26624x13312, b=2048): 252.237

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 223.751
MLP duration (in seconds): 0.0458
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0738
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13440x20160, b=2048): 0.0179
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13440x20160, b=2048): 248.576
Elapsed time for attention_key_query_prob (256x2048x105x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x105x2048): 31.357
Elapsed time for attention_prob_times_values (256x2048x2048x105): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x105): 62.662
Elapsed time for attention_linear_projection (4x6720x13440, b=2048): 0.0064
Throughput (in TFLOP/s) for attention_linear_projection (4x6720x13440, b=2048): 231.345
Elapsed time for mlp_h_to_4h (4x13440x26880, b=2048): 0.0233
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13440x26880, b=2048): 253.534
Elapsed time for mlp_4h_to_h (4x26880x13440, b=2048): 0.0235
Throughput (in TFLOP/s) for mlp_4h_to_h (4x26880x13440, b=2048): 252.246

Attention duration (in seconds): 0.0350
Attention throughput (in TFLOP/s): 181.768
MLP duration (in seconds): 0.0468
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0819
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13568x20352, b=2048): 0.0182
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13568x20352, b=2048): 249.002
Elapsed time for attention_key_query_prob (256x2048x106x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x106x2048): 63.774
Elapsed time for attention_prob_times_values (256x2048x2048x106): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x106): 107.156
Elapsed time for attention_linear_projection (4x6784x13568, b=2048): 0.0063
Throughput (in TFLOP/s) for attention_linear_projection (4x6784x13568, b=2048): 239.513
Elapsed time for mlp_h_to_4h (4x13568x27136, b=2048): 0.0233
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13568x27136, b=2048): 258.611
Elapsed time for mlp_4h_to_h (4x27136x13568, b=2048): 0.0239
Throughput (in TFLOP/s) for mlp_4h_to_h (4x27136x13568, b=2048): 252.349

Attention duration (in seconds): 0.0302
Attention throughput (in TFLOP/s): 215.108
MLP duration (in seconds): 0.0472
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0774
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13696x20544, b=2048): 0.0190
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13696x20544, b=2048): 243.246
Elapsed time for attention_key_query_prob (256x2048x107x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x107x2048): 29.430
Elapsed time for attention_prob_times_values (256x2048x2048x107): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x107): 65.407
Elapsed time for attention_linear_projection (4x6848x13696, b=2048): 0.0066
Throughput (in TFLOP/s) for attention_linear_projection (4x6848x13696, b=2048): 232.704
Elapsed time for mlp_h_to_4h (4x13696x27392, b=2048): 0.0242
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13696x27392, b=2048): 254.129
Elapsed time for mlp_4h_to_h (4x27392x13696, b=2048): 0.0248
Throughput (in TFLOP/s) for mlp_4h_to_h (4x27392x13696, b=2048): 247.740

Attention duration (in seconds): 0.0369
Attention throughput (in TFLOP/s): 179.145
MLP duration (in seconds): 0.0490
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0859
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13824x20736, b=2048): 0.0198
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13824x20736, b=2048): 237.655
Elapsed time for attention_key_query_prob (256x2048x108x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x108x2048): 64.007
Elapsed time for attention_prob_times_values (256x2048x2048x108): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x108): 109.485
Elapsed time for attention_linear_projection (4x6912x13824, b=2048): 0.0066
Throughput (in TFLOP/s) for attention_linear_projection (4x6912x13824, b=2048): 235.544
Elapsed time for mlp_h_to_4h (4x13824x27648, b=2048): 0.0245
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13824x27648, b=2048): 255.790
Elapsed time for mlp_4h_to_h (4x27648x13824, b=2048): 0.0308
Throughput (in TFLOP/s) for mlp_4h_to_h (4x27648x13824, b=2048): 203.452

Attention duration (in seconds): 0.0322
Attention throughput (in TFLOP/s): 209.203
MLP duration (in seconds): 0.0553
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0874
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13952x20928, b=2048): 0.0198
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13952x20928, b=2048): 242.106
Elapsed time for attention_key_query_prob (256x2048x109x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x109x2048): 32.444
Elapsed time for attention_prob_times_values (256x2048x2048x109): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x109): 66.652
Elapsed time for attention_linear_projection (4x6976x13952, b=2048): 0.0066
Throughput (in TFLOP/s) for attention_linear_projection (4x6976x13952, b=2048): 243.330
Elapsed time for mlp_h_to_4h (4x13952x27904, b=2048): 0.0247
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13952x27904, b=2048): 258.035
Elapsed time for mlp_4h_to_h (4x27904x13952, b=2048): 0.0258
Throughput (in TFLOP/s) for mlp_4h_to_h (4x27904x13952, b=2048): 246.805

Attention duration (in seconds): 0.0370
Attention throughput (in TFLOP/s): 184.848
MLP duration (in seconds): 0.0506
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0876
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14080x21120, b=2048): 0.0203
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14080x21120, b=2048): 239.696
Elapsed time for attention_key_query_prob (256x2048x110x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x110x2048): 65.017
Elapsed time for attention_prob_times_values (256x2048x2048x110): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x110): 110.814
Elapsed time for attention_linear_projection (4x7040x14080, b=2048): 0.0067
Throughput (in TFLOP/s) for attention_linear_projection (4x7040x14080, b=2048): 241.224
Elapsed time for mlp_h_to_4h (4x14080x28160, b=2048): 0.0253
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14080x28160, b=2048): 256.834
Elapsed time for mlp_4h_to_h (4x28160x14080, b=2048): 0.0256
Throughput (in TFLOP/s) for mlp_4h_to_h (4x28160x14080, b=2048): 253.348

Attention duration (in seconds): 0.0328
Attention throughput (in TFLOP/s): 212.304
MLP duration (in seconds): 0.0509
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0838
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14208x21312, b=2048): 0.0196
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14208x21312, b=2048): 252.826
Elapsed time for attention_key_query_prob (256x2048x111x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x111x2048): 32.725
Elapsed time for attention_prob_times_values (256x2048x2048x111): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x111): 66.675
Elapsed time for attention_linear_projection (4x7104x14208, b=2048): 0.0069
Throughput (in TFLOP/s) for attention_linear_projection (4x7104x14208, b=2048): 241.147
Elapsed time for mlp_h_to_4h (4x14208x28416, b=2048): 0.0255
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14208x28416, b=2048): 258.899
Elapsed time for mlp_4h_to_h (4x28416x14208, b=2048): 0.0265
Throughput (in TFLOP/s) for mlp_4h_to_h (4x28416x14208, b=2048): 250.004

Attention duration (in seconds): 0.0373
Attention throughput (in TFLOP/s): 189.921
MLP duration (in seconds): 0.0520
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0893
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14336x21504, b=2048): 0.0199
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14336x21504, b=2048): 253.791
Elapsed time for attention_key_query_prob (256x2048x112x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x112x2048): 94.652
Elapsed time for attention_prob_times_values (256x2048x2048x112): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x112): 125.645
Elapsed time for attention_linear_projection (4x7168x14336, b=2048): 0.0165
Throughput (in TFLOP/s) for attention_linear_projection (4x7168x14336, b=2048): 101.867
Elapsed time for mlp_h_to_4h (4x14336x28672, b=2048): 0.0261
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14336x28672, b=2048): 257.567
Elapsed time for mlp_4h_to_h (4x28672x14336, b=2048): 0.0269
Throughput (in TFLOP/s) for mlp_4h_to_h (4x28672x14336, b=2048): 250.089

Attention duration (in seconds): 0.0409
Attention throughput (in TFLOP/s): 176.485
MLP duration (in seconds): 0.0531
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0940
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14464x21696, b=2048): 0.0202
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14464x21696, b=2048): 253.944
Elapsed time for attention_key_query_prob (256x2048x113x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x113x2048): 33.548
Elapsed time for attention_prob_times_values (256x2048x2048x113): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x113): 58.630
Elapsed time for attention_linear_projection (4x7232x14464, b=2048): 0.0071
Throughput (in TFLOP/s) for attention_linear_projection (4x7232x14464, b=2048): 241.551
Elapsed time for mlp_h_to_4h (4x14464x28928, b=2048): 0.0269
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14464x28928, b=2048): 254.990
Elapsed time for mlp_4h_to_h (4x28928x14464, b=2048): 0.0274
Throughput (in TFLOP/s) for mlp_4h_to_h (4x28928x14464, b=2048): 249.993

Attention duration (in seconds): 0.0387
Attention throughput (in TFLOP/s): 189.612
MLP duration (in seconds): 0.0543
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0930
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14592x21888, b=2048): 0.0206
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14592x21888, b=2048): 253.996
Elapsed time for attention_key_query_prob (256x2048x114x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x114x2048): 67.174
Elapsed time for attention_prob_times_values (256x2048x2048x114): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x114): 113.977
Elapsed time for attention_linear_projection (4x7296x14592, b=2048): 0.0072
Throughput (in TFLOP/s) for attention_linear_projection (4x7296x14592, b=2048): 243.870
Elapsed time for mlp_h_to_4h (4x14592x29184, b=2048): 0.0276
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14592x29184, b=2048): 252.453
Elapsed time for mlp_4h_to_h (4x29184x14592, b=2048): 0.0279
Throughput (in TFLOP/s) for mlp_4h_to_h (4x29184x14592, b=2048): 250.044

Attention duration (in seconds): 0.0335
Attention throughput (in TFLOP/s): 222.576
MLP duration (in seconds): 0.0555
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0891
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14720x22080, b=2048): 0.0208
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14720x22080, b=2048): 255.713
Elapsed time for attention_key_query_prob (256x2048x115x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x115x2048): 34.235
Elapsed time for attention_prob_times_values (256x2048x2048x115): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x115): 70.050
Elapsed time for attention_linear_projection (4x7360x14720, b=2048): 0.0072
Throughput (in TFLOP/s) for attention_linear_projection (4x7360x14720, b=2048): 245.556
Elapsed time for mlp_h_to_4h (4x14720x29440, b=2048): 0.0280
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14720x29440, b=2048): 253.840
Elapsed time for mlp_4h_to_h (4x29440x14720, b=2048): 0.0284
Throughput (in TFLOP/s) for mlp_4h_to_h (4x29440x14720, b=2048): 249.736

Attention duration (in seconds): 0.0388
Attention throughput (in TFLOP/s): 195.762
MLP duration (in seconds): 0.0564
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0952
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14848x22272, b=2048): 0.0210
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14848x22272, b=2048): 257.579
Elapsed time for attention_key_query_prob (256x2048x116x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x116x2048): 69.012
Elapsed time for attention_prob_times_values (256x2048x2048x116): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x116): 115.159
Elapsed time for attention_linear_projection (4x7424x14848, b=2048): 0.0073
Throughput (in TFLOP/s) for attention_linear_projection (4x7424x14848, b=2048): 245.999
Elapsed time for mlp_h_to_4h (4x14848x29696, b=2048): 0.0282
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14848x29696, b=2048): 256.005
Elapsed time for mlp_4h_to_h (4x29696x14848, b=2048): 0.0285
Throughput (in TFLOP/s) for mlp_4h_to_h (4x29696x14848, b=2048): 253.315

Attention duration (in seconds): 0.0341
Attention throughput (in TFLOP/s): 226.136
MLP duration (in seconds): 0.0567
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0909
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14976x22464, b=2048): 0.0216
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14976x22464, b=2048): 255.353
Elapsed time for attention_key_query_prob (256x2048x117x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x117x2048): 35.088
Elapsed time for attention_prob_times_values (256x2048x2048x117): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x117): 71.081
Elapsed time for attention_linear_projection (4x7488x14976, b=2048): 0.0075
Throughput (in TFLOP/s) for attention_linear_projection (4x7488x14976, b=2048): 246.575
Elapsed time for mlp_h_to_4h (4x14976x29952, b=2048): 0.0289
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14976x29952, b=2048): 254.627
Elapsed time for mlp_4h_to_h (4x29952x14976, b=2048): 0.0297
Throughput (in TFLOP/s) for mlp_4h_to_h (4x29952x14976, b=2048): 247.095

Attention duration (in seconds): 0.0397
Attention throughput (in TFLOP/s): 197.616
MLP duration (in seconds): 0.0586
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0983
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15104x22656, b=2048): 0.0221
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15104x22656, b=2048): 254.071
Elapsed time for attention_key_query_prob (256x2048x118x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x118x2048): 70.448
Elapsed time for attention_prob_times_values (256x2048x2048x118): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x118): 117.118
Elapsed time for attention_linear_projection (4x7552x15104, b=2048): 0.0076
Throughput (in TFLOP/s) for attention_linear_projection (4x7552x15104, b=2048): 245.968
Elapsed time for mlp_h_to_4h (4x15104x30208, b=2048): 0.0294
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15104x30208, b=2048): 253.998
Elapsed time for mlp_4h_to_h (4x30208x15104, b=2048): 0.0301
Throughput (in TFLOP/s) for mlp_4h_to_h (4x30208x15104, b=2048): 248.554

Attention duration (in seconds): 0.0354
Attention throughput (in TFLOP/s): 225.324
MLP duration (in seconds): 0.0595
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0949
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15232x22848, b=2048): 0.0224
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15232x22848, b=2048): 255.007
Elapsed time for attention_key_query_prob (256x2048x119x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x119x2048): 33.913
Elapsed time for attention_prob_times_values (256x2048x2048x119): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x119): 71.187
Elapsed time for attention_linear_projection (4x7616x15232, b=2048): 0.0127
Throughput (in TFLOP/s) for attention_linear_projection (4x7616x15232, b=2048): 149.587
Elapsed time for mlp_h_to_4h (4x15232x30464, b=2048): 0.0298
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15232x30464, b=2048): 254.914
Elapsed time for mlp_4h_to_h (4x30464x15232, b=2048): 0.0305
Throughput (in TFLOP/s) for mlp_4h_to_h (4x30464x15232, b=2048): 249.195

Attention duration (in seconds): 0.0462
Attention throughput (in TFLOP/s): 175.654
MLP duration (in seconds): 0.0603
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15360x23040, b=2048): 0.0226
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15360x23040, b=2048): 257.003
Elapsed time for attention_key_query_prob (256x2048x120x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x120x2048): 91.281
Elapsed time for attention_prob_times_values (256x2048x2048x120): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x120): 131.829
Elapsed time for attention_linear_projection (4x7680x15360, b=2048): 0.0080
Throughput (in TFLOP/s) for attention_linear_projection (4x7680x15360, b=2048): 241.545
Elapsed time for mlp_h_to_4h (4x15360x30720, b=2048): 0.0304
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15360x30720, b=2048): 254.012
Elapsed time for mlp_4h_to_h (4x30720x15360, b=2048): 0.0304
Throughput (in TFLOP/s) for mlp_4h_to_h (4x30720x15360, b=2048): 254.249

Attention duration (in seconds): 0.0353
Attention throughput (in TFLOP/s): 233.341
MLP duration (in seconds): 0.0608
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0962
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15488x23232, b=2048): 0.0228
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15488x23232, b=2048): 258.694
Elapsed time for attention_key_query_prob (256x2048x121x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x121x2048): 34.627
Elapsed time for attention_prob_times_values (256x2048x2048x121): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x121): 72.316
Elapsed time for attention_linear_projection (4x7744x15488, b=2048): 0.0082
Throughput (in TFLOP/s) for attention_linear_projection (4x7744x15488, b=2048): 238.972
Elapsed time for mlp_h_to_4h (4x15488x30976, b=2048): 0.0310
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15488x30976, b=2048): 253.369
Elapsed time for mlp_4h_to_h (4x30976x15488, b=2048): 0.0313
Throughput (in TFLOP/s) for mlp_4h_to_h (4x30976x15488, b=2048): 251.222

Attention duration (in seconds): 0.0421
Attention throughput (in TFLOP/s): 199.008
MLP duration (in seconds): 0.0623
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15616x23424, b=2048): 0.0236
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15616x23424, b=2048): 253.633
Elapsed time for attention_key_query_prob (256x2048x122x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x122x2048): 72.677
Elapsed time for attention_prob_times_values (256x2048x2048x122): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x122): 121.517
Elapsed time for attention_linear_projection (4x7808x15616, b=2048): 0.0081
Throughput (in TFLOP/s) for attention_linear_projection (4x7808x15616, b=2048): 247.356
Elapsed time for mlp_h_to_4h (4x15616x31232, b=2048): 0.0318
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15616x31232, b=2048): 251.567
Elapsed time for mlp_4h_to_h (4x31232x15616, b=2048): 0.0321
Throughput (in TFLOP/s) for mlp_4h_to_h (4x31232x15616, b=2048): 248.777

Attention duration (in seconds): 0.0375
Attention throughput (in TFLOP/s): 227.266
MLP duration (in seconds): 0.0639
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15744x23616, b=2048): 0.0236
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15744x23616, b=2048): 258.301
Elapsed time for attention_key_query_prob (256x2048x123x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x123x2048): 36.423
Elapsed time for attention_prob_times_values (256x2048x2048x123): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x123): 74.560
Elapsed time for attention_linear_projection (4x7872x15744, b=2048): 0.0084
Throughput (in TFLOP/s) for attention_linear_projection (4x7872x15744, b=2048): 242.694
Elapsed time for mlp_h_to_4h (4x15744x31488, b=2048): 0.0316
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15744x31488, b=2048): 257.222
Elapsed time for mlp_4h_to_h (4x31488x15744, b=2048): 0.0334
Throughput (in TFLOP/s) for mlp_4h_to_h (4x31488x15744, b=2048): 242.840

Attention duration (in seconds): 0.0427
Attention throughput (in TFLOP/s): 202.375
MLP duration (in seconds): 0.0650
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15872x23808, b=2048): 0.0244
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15872x23808, b=2048): 253.255
Elapsed time for attention_key_query_prob (256x2048x124x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x124x2048): 73.272
Elapsed time for attention_prob_times_values (256x2048x2048x124): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x124): 123.906
Elapsed time for attention_linear_projection (4x7936x15872, b=2048): 0.0084
Throughput (in TFLOP/s) for attention_linear_projection (4x7936x15872, b=2048): 244.607
Elapsed time for mlp_h_to_4h (4x15872x31744, b=2048): 0.0323
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15872x31744, b=2048): 255.803
Elapsed time for mlp_4h_to_h (4x31744x15872, b=2048): 0.0332
Throughput (in TFLOP/s) for mlp_4h_to_h (4x31744x15872, b=2048): 248.757

Attention duration (in seconds): 0.0387
Attention throughput (in TFLOP/s): 227.263
MLP duration (in seconds): 0.0655
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16000x24000, b=2048): 0.0244
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16000x24000, b=2048): 258.114
Elapsed time for attention_key_query_prob (256x2048x125x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x125x2048): 37.074
Elapsed time for attention_prob_times_values (256x2048x2048x125): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x125): 68.282
Elapsed time for attention_linear_projection (4x8000x16000, b=2048): 0.0085
Throughput (in TFLOP/s) for attention_linear_projection (4x8000x16000, b=2048): 246.714
Elapsed time for mlp_h_to_4h (4x16000x32000, b=2048): 0.0326
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16000x32000, b=2048): 257.459
Elapsed time for mlp_4h_to_h (4x32000x16000, b=2048): 0.0335
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32000x16000, b=2048): 250.713

Attention duration (in seconds): 0.0440
Attention throughput (in TFLOP/s): 202.636
MLP duration (in seconds): 0.0660
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16128x24192, b=2048): 0.0245
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16128x24192, b=2048): 260.631
Elapsed time for attention_key_query_prob (256x2048x126x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x126x2048): 75.469
Elapsed time for attention_prob_times_values (256x2048x2048x126): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x126): 125.266
Elapsed time for attention_linear_projection (4x8064x16128, b=2048): 0.0087
Throughput (in TFLOP/s) for attention_linear_projection (4x8064x16128, b=2048): 245.533
Elapsed time for mlp_h_to_4h (4x16128x32256, b=2048): 0.0337
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16128x32256, b=2048): 252.758
Elapsed time for mlp_4h_to_h (4x32256x16128, b=2048): 0.0343
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32256x16128, b=2048): 248.596

Attention duration (in seconds): 0.0390
Attention throughput (in TFLOP/s): 232.717
MLP duration (in seconds): 0.0680
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16256x24384, b=2048): 0.0345
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16256x24384, b=2048): 188.098
Elapsed time for attention_key_query_prob (256x2048x127x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x127x2048): 36.937
Elapsed time for attention_prob_times_values (256x2048x2048x127): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x127): 75.726
Elapsed time for attention_linear_projection (4x8128x16256, b=2048): 0.0088
Throughput (in TFLOP/s) for attention_linear_projection (4x8128x16256, b=2048): 247.164
Elapsed time for mlp_h_to_4h (4x16256x32512, b=2048): 0.0340
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16256x32512, b=2048): 255.008
Elapsed time for mlp_4h_to_h (4x32512x16256, b=2048): 0.0345
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32512x16256, b=2048): 251.190

Attention duration (in seconds): 0.0543
Attention throughput (in TFLOP/s): 169.608
MLP duration (in seconds): 0.0684
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16384x24576, b=2048): 0.0254
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16384x24576, b=2048): 259.360
Elapsed time for attention_key_query_prob (256x2048x128x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x128x2048): 110.698
Elapsed time for attention_prob_times_values (256x2048x2048x128): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x128): 141.463
Elapsed time for attention_linear_projection (4x8192x16384, b=2048): 0.0091
Throughput (in TFLOP/s) for attention_linear_projection (4x8192x16384, b=2048): 242.759
Elapsed time for mlp_h_to_4h (4x16384x32768, b=2048): 0.0343
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16384x32768, b=2048): 256.418
Elapsed time for mlp_4h_to_h (4x32768x16384, b=2048): 0.0352
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32768x16384, b=2048): 249.624

Attention duration (in seconds): 0.0389
Attention throughput (in TFLOP/s): 240.126
MLP duration (in seconds): 0.0695
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16512x24768, b=2048): 0.0263
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16512x24768, b=2048): 254.578
Elapsed time for attention_key_query_prob (256x2048x129x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x129x2048): 34.232
Elapsed time for attention_prob_times_values (256x2048x2048x129): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x129): 55.507
Elapsed time for attention_linear_projection (4x8256x16512, b=2048): 0.0091
Throughput (in TFLOP/s) for attention_linear_projection (4x8256x16512, b=2048): 245.340
Elapsed time for mlp_h_to_4h (4x16512x33024, b=2048): 0.0350
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16512x33024, b=2048): 255.476
Elapsed time for mlp_4h_to_h (4x33024x16512, b=2048): 0.0358
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33024x16512, b=2048): 249.215

Attention duration (in seconds): 0.0485
Attention throughput (in TFLOP/s): 195.601
MLP duration (in seconds): 0.0708
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16640x24960, b=2048): 0.0263
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16640x24960, b=2048): 259.207
Elapsed time for attention_key_query_prob (256x2048x130x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x130x2048): 71.757
Elapsed time for attention_prob_times_values (256x2048x2048x130): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x130): 94.545
Elapsed time for attention_linear_projection (4x8320x16640, b=2048): 0.0093
Throughput (in TFLOP/s) for attention_linear_projection (4x8320x16640, b=2048): 243.870
Elapsed time for mlp_h_to_4h (4x16640x33280, b=2048): 0.0352
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16640x33280, b=2048): 257.563
Elapsed time for mlp_4h_to_h (4x33280x16640, b=2048): 0.0363
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33280x16640, b=2048): 249.707

Attention duration (in seconds): 0.0424
Attention throughput (in TFLOP/s): 227.173
MLP duration (in seconds): 0.0716
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16768x25152, b=2048): 0.0265
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16768x25152, b=2048): 260.377
Elapsed time for attention_key_query_prob (256x2048x131x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x131x2048): 34.762
Elapsed time for attention_prob_times_values (256x2048x2048x131): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x131): 58.303
Elapsed time for attention_linear_projection (4x8384x16768, b=2048): 0.0096
Throughput (in TFLOP/s) for attention_linear_projection (4x8384x16768, b=2048): 239.555
Elapsed time for mlp_h_to_4h (4x16768x33536, b=2048): 0.0356
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16768x33536, b=2048): 259.096
Elapsed time for mlp_4h_to_h (4x33536x16768, b=2048): 0.0368
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33536x16768, b=2048): 250.528

Attention duration (in seconds): 0.0491
Attention throughput (in TFLOP/s): 199.220
MLP duration (in seconds): 0.0723
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16896x25344, b=2048): 0.0271
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16896x25344, b=2048): 259.233
Elapsed time for attention_key_query_prob (256x2048x132x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x132x2048): 72.294
Elapsed time for attention_prob_times_values (256x2048x2048x132): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x132): 94.541
Elapsed time for attention_linear_projection (4x8448x16896, b=2048): 0.0590
Throughput (in TFLOP/s) for attention_linear_projection (4x8448x16896, b=2048): 39.610
Elapsed time for mlp_h_to_4h (4x16896x33792, b=2048): 0.0442
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16896x33792, b=2048): 211.828
Elapsed time for mlp_4h_to_h (4x33792x16896, b=2048): 0.0375
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33792x16896, b=2048): 249.741

Attention duration (in seconds): 0.0930
Attention throughput (in TFLOP/s): 106.653
MLP duration (in seconds): 0.0816
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1746
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17024x25536, b=2048): 0.0284
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17024x25536, b=2048): 250.745
Elapsed time for attention_key_query_prob (256x2048x133x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x133x2048): 34.657
Elapsed time for attention_prob_times_values (256x2048x2048x133): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x133): 59.311
Elapsed time for attention_linear_projection (4x8512x17024, b=2048): 0.0099
Throughput (in TFLOP/s) for attention_linear_projection (4x8512x17024, b=2048): 240.817
Elapsed time for mlp_h_to_4h (4x17024x34048, b=2048): 0.0374
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17024x34048, b=2048): 253.661
Elapsed time for mlp_4h_to_h (4x34048x17024, b=2048): 0.0383
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34048x17024, b=2048): 248.121

Attention duration (in seconds): 0.0513
Attention throughput (in TFLOP/s): 196.176
MLP duration (in seconds): 0.0757
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17152x25728, b=2048): 0.0276
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17152x25728, b=2048): 261.749
Elapsed time for attention_key_query_prob (256x2048x134x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x134x2048): 72.968
Elapsed time for attention_prob_times_values (256x2048x2048x134): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x134): 94.590
Elapsed time for attention_linear_projection (4x8576x17152, b=2048): 0.0102
Throughput (in TFLOP/s) for attention_linear_projection (4x8576x17152, b=2048): 236.270
Elapsed time for mlp_h_to_4h (4x17152x34304, b=2048): 0.0372
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17152x34304, b=2048): 259.229
Elapsed time for mlp_4h_to_h (4x34304x17152, b=2048): 0.0395
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34304x17152, b=2048): 244.250

Attention duration (in seconds): 0.0448
Attention throughput (in TFLOP/s): 227.985
MLP duration (in seconds): 0.0767
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17280x25920, b=2048): 0.0282
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17280x25920, b=2048): 259.776
Elapsed time for attention_key_query_prob (256x2048x135x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x135x2048): 33.402
Elapsed time for attention_prob_times_values (256x2048x2048x135): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x135): 58.119
Elapsed time for attention_linear_projection (4x8640x17280, b=2048): 0.0100
Throughput (in TFLOP/s) for attention_linear_projection (4x8640x17280, b=2048): 244.998
Elapsed time for mlp_h_to_4h (4x17280x34560, b=2048): 0.0377
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17280x34560, b=2048): 259.838
Elapsed time for mlp_4h_to_h (4x34560x17280, b=2048): 0.0390
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34560x17280, b=2048): 250.999

Attention duration (in seconds): 0.0519
Attention throughput (in TFLOP/s): 199.695
MLP duration (in seconds): 0.0766
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17408x26112, b=2048): 0.0288
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17408x26112, b=2048): 258.347
Elapsed time for attention_key_query_prob (256x2048x136x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x136x2048): 100.145
Elapsed time for attention_prob_times_values (256x2048x2048x136): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x136): 127.073
Elapsed time for attention_linear_projection (4x8704x17408, b=2048): 0.0101
Throughput (in TFLOP/s) for attention_linear_projection (4x8704x17408, b=2048): 246.445
Elapsed time for mlp_h_to_4h (4x17408x34816, b=2048): 0.0383
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17408x34816, b=2048): 259.195
Elapsed time for mlp_4h_to_h (4x34816x17408, b=2048): 0.0398
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34816x17408, b=2048): 249.759

Attention duration (in seconds): 0.0441
Attention throughput (in TFLOP/s): 238.332
MLP duration (in seconds): 0.0781
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17536x26304, b=2048): 0.0295
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17536x26304, b=2048): 256.315
Elapsed time for attention_key_query_prob (256x2048x137x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x137x2048): 33.758
Elapsed time for attention_prob_times_values (256x2048x2048x137): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x137): 58.730
Elapsed time for attention_linear_projection (4x8768x17536, b=2048): 0.0103
Throughput (in TFLOP/s) for attention_linear_projection (4x8768x17536, b=2048): 244.753
Elapsed time for mlp_h_to_4h (4x17536x35072, b=2048): 0.0389
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17536x35072, b=2048): 258.758
Elapsed time for mlp_4h_to_h (4x35072x17536, b=2048): 0.0405
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35072x17536, b=2048): 248.981

Attention duration (in seconds): 0.0535
Attention throughput (in TFLOP/s): 199.338
MLP duration (in seconds): 0.0794
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1329
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17664x26496, b=2048): 0.0301
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17664x26496, b=2048): 254.908
Elapsed time for attention_key_query_prob (256x2048x138x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x138x2048): 74.596
Elapsed time for attention_prob_times_values (256x2048x2048x138): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x138): 94.632
Elapsed time for attention_linear_projection (4x8832x17664, b=2048): 0.0106
Throughput (in TFLOP/s) for attention_linear_projection (4x8832x17664, b=2048): 240.560
Elapsed time for mlp_h_to_4h (4x17664x35328, b=2048): 0.0394
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17664x35328, b=2048): 259.562
Elapsed time for mlp_4h_to_h (4x35328x17664, b=2048): 0.0408
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35328x17664, b=2048): 250.330

Attention duration (in seconds): 0.0478
Attention throughput (in TFLOP/s): 226.239
MLP duration (in seconds): 0.0802
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1280
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17792x26688, b=2048): 0.0303
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17792x26688, b=2048): 256.739
Elapsed time for attention_key_query_prob (256x2048x139x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x139x2048): 35.510
Elapsed time for attention_prob_times_values (256x2048x2048x139): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x139): 60.612
Elapsed time for attention_linear_projection (4x8896x17792, b=2048): 0.0107
Throughput (in TFLOP/s) for attention_linear_projection (4x8896x17792, b=2048): 242.698
Elapsed time for mlp_h_to_4h (4x17792x35584, b=2048): 0.0486
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17792x35584, b=2048): 213.420
Elapsed time for mlp_4h_to_h (4x35584x17792, b=2048): 0.0415
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35584x17792, b=2048): 250.020

Attention duration (in seconds): 0.0543
Attention throughput (in TFLOP/s): 201.958
MLP duration (in seconds): 0.0901
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1444
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17920x26880, b=2048): 0.0314
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17920x26880, b=2048): 251.734
Elapsed time for attention_key_query_prob (256x2048x140x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x140x2048): 75.723
Elapsed time for attention_prob_times_values (256x2048x2048x140): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x140): 95.192
Elapsed time for attention_linear_projection (4x8960x17920, b=2048): 0.0107
Throughput (in TFLOP/s) for attention_linear_projection (4x8960x17920, b=2048): 245.163
Elapsed time for mlp_h_to_4h (4x17920x35840, b=2048): 0.0404
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17920x35840, b=2048): 260.184
Elapsed time for mlp_4h_to_h (4x35840x17920, b=2048): 0.0427
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35840x17920, b=2048): 246.558

Attention duration (in seconds): 0.0492
Attention throughput (in TFLOP/s): 226.053
MLP duration (in seconds): 0.0831
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1323
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18048x27072, b=2048): 0.0309
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18048x27072, b=2048): 259.184
Elapsed time for attention_key_query_prob (256x2048x141x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x141x2048): 35.121
Elapsed time for attention_prob_times_values (256x2048x2048x141): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x141): 59.580
Elapsed time for attention_linear_projection (4x9024x18048, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x9024x18048, b=2048): 247.168
Elapsed time for mlp_h_to_4h (4x18048x36096, b=2048): 0.0416
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18048x36096, b=2048): 256.324
Elapsed time for mlp_4h_to_h (4x36096x18048, b=2048): 0.0428
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36096x18048, b=2048): 249.525

Attention duration (in seconds): 0.0554
Attention throughput (in TFLOP/s): 203.648
MLP duration (in seconds): 0.0844
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1398
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18176x27264, b=2048): 0.0319
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18176x27264, b=2048): 254.294
Elapsed time for attention_key_query_prob (256x2048x142x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x142x2048): 75.875
Elapsed time for attention_prob_times_values (256x2048x2048x142): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x142): 96.421
Elapsed time for attention_linear_projection (4x9088x18176, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x9088x18176, b=2048): 245.508
Elapsed time for mlp_h_to_4h (4x18176x36352, b=2048): 0.0591
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18176x36352, b=2048): 183.252
Elapsed time for mlp_4h_to_h (4x36352x18176, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36352x18176, b=2048): 249.367

Attention duration (in seconds): 0.0501
Attention throughput (in TFLOP/s): 228.099
MLP duration (in seconds): 0.1025
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1526
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18304x27456, b=2048): 0.0321
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18304x27456, b=2048): 256.313
Elapsed time for attention_key_query_prob (256x2048x143x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x143x2048): 35.250
Elapsed time for attention_prob_times_values (256x2048x2048x143): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x143): 59.524
Elapsed time for attention_linear_projection (4x9152x18304, b=2048): 0.0114
Throughput (in TFLOP/s) for attention_linear_projection (4x9152x18304, b=2048): 241.226
Elapsed time for mlp_h_to_4h (4x18304x36608, b=2048): 0.0422
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18304x36608, b=2048): 259.896
Elapsed time for mlp_4h_to_h (4x36608x18304, b=2048): 0.0452
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36608x18304, b=2048): 242.795

Attention duration (in seconds): 0.0574
Attention throughput (in TFLOP/s): 202.057
MLP duration (in seconds): 0.0875
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1448
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18432x27648, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18432x27648, b=2048): 257.454
Elapsed time for attention_key_query_prob (256x2048x144x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x144x2048): 111.334
Elapsed time for attention_prob_times_values (256x2048x2048x144): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x144): 135.617
Elapsed time for attention_linear_projection (4x9216x18432, b=2048): 0.0114
Throughput (in TFLOP/s) for attention_linear_projection (4x9216x18432, b=2048): 243.306
Elapsed time for mlp_h_to_4h (4x18432x36864, b=2048): 0.0429
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18432x36864, b=2048): 259.201
Elapsed time for mlp_4h_to_h (4x36864x18432, b=2048): 0.0449
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36864x18432, b=2048): 248.213

Attention duration (in seconds): 0.0489
Attention throughput (in TFLOP/s): 240.173
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1367
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18560x27840, b=2048): 0.0510
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18560x27840, b=2048): 166.114
Elapsed time for attention_key_query_prob (256x2048x145x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x145x2048): 57.097
Elapsed time for attention_prob_times_values (256x2048x2048x145): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x145): 59.317
Elapsed time for attention_linear_projection (4x9280x18560, b=2048): 0.0116
Throughput (in TFLOP/s) for attention_linear_projection (4x9280x18560, b=2048): 242.452
Elapsed time for mlp_h_to_4h (4x18560x37120, b=2048): 0.0440
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18560x37120, b=2048): 256.445
Elapsed time for mlp_4h_to_h (4x37120x18560, b=2048): 0.0456
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37120x18560, b=2048): 247.516

Attention duration (in seconds): 0.0733
Attention throughput (in TFLOP/s): 162.477
MLP duration (in seconds): 0.0896
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1629
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18688x28032, b=2048): 0.0333
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18688x28032, b=2048): 257.647
Elapsed time for attention_key_query_prob (256x2048x146x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x146x2048): 77.910
Elapsed time for attention_prob_times_values (256x2048x2048x146): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x146): 97.614
Elapsed time for attention_linear_projection (4x9344x18688, b=2048): 0.0119
Throughput (in TFLOP/s) for attention_linear_projection (4x9344x18688, b=2048): 241.106
Elapsed time for mlp_h_to_4h (4x18688x37376, b=2048): 0.0441
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18688x37376, b=2048): 259.410
Elapsed time for mlp_4h_to_h (4x37376x18688, b=2048): 0.0460
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37376x18688, b=2048): 249.023

Attention duration (in seconds): 0.0524
Attention throughput (in TFLOP/s): 230.296
MLP duration (in seconds): 0.0901
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1425
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18816x28224, b=2048): 0.0337
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18816x28224, b=2048): 258.246
Elapsed time for attention_key_query_prob (256x2048x147x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x147x2048): 57.666
Elapsed time for attention_prob_times_values (256x2048x2048x147): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x147): 59.964
Elapsed time for attention_linear_projection (4x9408x18816, b=2048): 0.0121
Throughput (in TFLOP/s) for attention_linear_projection (4x9408x18816, b=2048): 239.427
Elapsed time for mlp_h_to_4h (4x18816x37632, b=2048): 0.0454
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18816x37632, b=2048): 255.564
Elapsed time for mlp_4h_to_h (4x37632x18816, b=2048): 0.0466
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37632x18816, b=2048): 248.868

Attention duration (in seconds): 0.0565
Attention throughput (in TFLOP/s): 216.335
MLP duration (in seconds): 0.0920
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1486
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18944x28416, b=2048): 0.0343
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18944x28416, b=2048): 256.964
Elapsed time for attention_key_query_prob (256x2048x148x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x148x2048): 77.306
Elapsed time for attention_prob_times_values (256x2048x2048x148): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x148): 98.870
Elapsed time for attention_linear_projection (4x9472x18944, b=2048): 0.0122
Throughput (in TFLOP/s) for attention_linear_projection (4x9472x18944, b=2048): 240.279
Elapsed time for mlp_h_to_4h (4x18944x37888, b=2048): 0.0452
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18944x37888, b=2048): 259.938
Elapsed time for mlp_4h_to_h (4x37888x18944, b=2048): 0.0476
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37888x18944, b=2048): 246.890

Attention duration (in seconds): 0.0539
Attention throughput (in TFLOP/s): 230.036
MLP duration (in seconds): 0.0929
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1468
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19072x28608, b=2048): 0.0346
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19072x28608, b=2048): 258.286
Elapsed time for attention_key_query_prob (256x2048x149x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x149x2048): 58.046
Elapsed time for attention_prob_times_values (256x2048x2048x149): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x149): 60.237
Elapsed time for attention_linear_projection (4x9536x19072, b=2048): 0.0123
Throughput (in TFLOP/s) for attention_linear_projection (4x9536x19072, b=2048): 242.417
Elapsed time for mlp_h_to_4h (4x19072x38144, b=2048): 0.0460
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19072x38144, b=2048): 259.308
Elapsed time for mlp_4h_to_h (4x38144x19072, b=2048): 0.0480
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38144x19072, b=2048): 248.412

Attention duration (in seconds): 0.0577
Attention throughput (in TFLOP/s): 217.561
MLP duration (in seconds): 0.0939
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1517
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19200x28800, b=2048): 0.0353
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19200x28800, b=2048): 257.008
Elapsed time for attention_key_query_prob (256x2048x150x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x150x2048): 80.169
Elapsed time for attention_prob_times_values (256x2048x2048x150): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x150): 100.147
Elapsed time for attention_linear_projection (4x9600x19200, b=2048): 0.0126
Throughput (in TFLOP/s) for attention_linear_projection (4x9600x19200, b=2048): 240.476
Elapsed time for mlp_h_to_4h (4x19200x38400, b=2048): 0.0464
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19200x38400, b=2048): 260.594
Elapsed time for mlp_4h_to_h (4x38400x19200, b=2048): 0.0485
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38400x19200, b=2048): 248.879

Attention duration (in seconds): 0.0550
Attention throughput (in TFLOP/s): 231.161
MLP duration (in seconds): 0.0949
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1499
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19328x28992, b=2048): 0.0356
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19328x28992, b=2048): 257.905
Elapsed time for attention_key_query_prob (256x2048x151x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x151x2048): 58.310
Elapsed time for attention_prob_times_values (256x2048x2048x151): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x151): 59.711
Elapsed time for attention_linear_projection (4x9664x19328, b=2048): 0.0128
Throughput (in TFLOP/s) for attention_linear_projection (4x9664x19328, b=2048): 239.873
Elapsed time for mlp_h_to_4h (4x19328x38656, b=2048): 0.0472
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19328x38656, b=2048): 259.521
Elapsed time for mlp_4h_to_h (4x38656x19328, b=2048): 0.0498
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38656x19328, b=2048): 245.592

Attention duration (in seconds): 0.0593
Attention throughput (in TFLOP/s): 217.189
MLP duration (in seconds): 0.0970
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1564
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19456x29184, b=2048): 0.0359
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19456x29184, b=2048): 259.233
Elapsed time for attention_key_query_prob (256x2048x152x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x152x2048): 106.760
Elapsed time for attention_prob_times_values (256x2048x2048x152): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x152): 141.071
Elapsed time for attention_linear_projection (4x9728x19456, b=2048): 0.0128
Throughput (in TFLOP/s) for attention_linear_projection (4x9728x19456, b=2048): 242.770
Elapsed time for mlp_h_to_4h (4x19456x38912, b=2048): 0.0479
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19456x38912, b=2048): 258.840
Elapsed time for mlp_4h_to_h (4x38912x19456, b=2048): 0.0501
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38912x19456, b=2048): 247.704

Attention duration (in seconds): 0.0540
Attention throughput (in TFLOP/s): 241.652
MLP duration (in seconds): 0.0980
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1520
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19584x29376, b=2048): 0.0366
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19584x29376, b=2048): 257.727
Elapsed time for attention_key_query_prob (256x2048x153x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x153x2048): 58.658
Elapsed time for attention_prob_times_values (256x2048x2048x153): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x153): 59.936
Elapsed time for attention_linear_projection (4x9792x19584, b=2048): 0.0131
Throughput (in TFLOP/s) for attention_linear_projection (4x9792x19584, b=2048): 240.187
Elapsed time for mlp_h_to_4h (4x19584x39168, b=2048): 0.0489
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19584x39168, b=2048): 256.810
Elapsed time for mlp_4h_to_h (4x39168x19584, b=2048): 0.0504
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39168x19584, b=2048): 249.545

Attention duration (in seconds): 0.0607
Attention throughput (in TFLOP/s): 217.738
MLP duration (in seconds): 0.0993
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1600
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19712x29568, b=2048): 0.0373
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19712x29568, b=2048): 255.797
Elapsed time for attention_key_query_prob (256x2048x154x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x154x2048): 81.985
Elapsed time for attention_prob_times_values (256x2048x2048x154): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x154): 102.091
Elapsed time for attention_linear_projection (4x9856x19712, b=2048): 0.0134
Throughput (in TFLOP/s) for attention_linear_projection (4x9856x19712, b=2048): 237.050
Elapsed time for mlp_h_to_4h (4x19712x39424, b=2048): 0.0491
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19712x39424, b=2048): 259.376
Elapsed time for mlp_4h_to_h (4x39424x19712, b=2048): 0.0509
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39424x19712, b=2048): 250.365

Attention duration (in seconds): 0.0580
Attention throughput (in TFLOP/s): 230.798
MLP duration (in seconds): 0.0999
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1580
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19840x29760, b=2048): 0.0376
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19840x29760, b=2048): 257.412
Elapsed time for attention_key_query_prob (256x2048x155x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x155x2048): 59.427
Elapsed time for attention_prob_times_values (256x2048x2048x155): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x155): 62.781
Elapsed time for attention_linear_projection (4x9920x19840, b=2048): 0.0134
Throughput (in TFLOP/s) for attention_linear_projection (4x9920x19840, b=2048): 240.327
Elapsed time for mlp_h_to_4h (4x19840x39680, b=2048): 0.0497
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19840x39680, b=2048): 259.303
Elapsed time for mlp_4h_to_h (4x39680x19840, b=2048): 0.0580
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39680x19840, b=2048): 222.428

Attention duration (in seconds): 0.0619
Attention throughput (in TFLOP/s): 219.124
MLP duration (in seconds): 0.1077
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1696
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19968x29952, b=2048): 0.0383
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19968x29952, b=2048): 255.762
Elapsed time for attention_key_query_prob (256x2048x156x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x156x2048): 82.596
Elapsed time for attention_prob_times_values (256x2048x2048x156): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x156): 103.348
Elapsed time for attention_linear_projection (4x9984x19968, b=2048): 0.0373
Throughput (in TFLOP/s) for attention_linear_projection (4x9984x19968, b=2048): 87.546
Elapsed time for mlp_h_to_4h (4x19968x39936, b=2048): 0.0509
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19968x39936, b=2048): 256.639
Elapsed time for mlp_4h_to_h (4x39936x19968, b=2048): 0.0681
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39936x19968, b=2048): 191.840

Attention duration (in seconds): 0.0829
Attention throughput (in TFLOP/s): 165.645
MLP duration (in seconds): 0.1190
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20096x30144, b=2048): 0.0386
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20096x30144, b=2048): 256.899
Elapsed time for attention_key_query_prob (256x2048x157x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x157x2048): 59.888
Elapsed time for attention_prob_times_values (256x2048x2048x157): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x157): 63.457
Elapsed time for attention_linear_projection (4x10048x20096, b=2048): 0.0139
Throughput (in TFLOP/s) for attention_linear_projection (4x10048x20096, b=2048): 237.228
Elapsed time for mlp_h_to_4h (4x20096x40192, b=2048): 0.0518
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20096x40192, b=2048): 255.713
Elapsed time for mlp_4h_to_h (4x40192x20096, b=2048): 0.0535
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40192x20096, b=2048): 247.336

Attention duration (in seconds): 0.0635
Attention throughput (in TFLOP/s): 218.940
MLP duration (in seconds): 0.1053
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1688
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20224x30336, b=2048): 0.0388
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20224x30336, b=2048): 258.941
Elapsed time for attention_key_query_prob (256x2048x158x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x158x2048): 83.976
Elapsed time for attention_prob_times_values (256x2048x2048x158): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x158): 104.604
Elapsed time for attention_linear_projection (4x10112x20224, b=2048): 0.0139
Throughput (in TFLOP/s) for attention_linear_projection (4x10112x20224, b=2048): 240.832
Elapsed time for mlp_h_to_4h (4x20224x40448, b=2048): 0.0515
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20224x40448, b=2048): 260.155
Elapsed time for mlp_4h_to_h (4x40448x20224, b=2048): 0.0538
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40448x20224, b=2048): 249.217

Attention duration (in seconds): 0.0600
Attention throughput (in TFLOP/s): 234.622
MLP duration (in seconds): 0.1053
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1653
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20352x30528, b=2048): 0.0397
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20352x30528, b=2048): 256.690
Elapsed time for attention_key_query_prob (256x2048x159x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x159x2048): 60.648
Elapsed time for attention_prob_times_values (256x2048x2048x159): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x159): 63.160
Elapsed time for attention_linear_projection (4x10176x20352, b=2048): 0.0141
Throughput (in TFLOP/s) for attention_linear_projection (4x10176x20352, b=2048): 240.400
Elapsed time for mlp_h_to_4h (4x20352x40704, b=2048): 0.0523
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20352x40704, b=2048): 259.486
Elapsed time for mlp_4h_to_h (4x40704x20352, b=2048): 0.0545
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40704x20352, b=2048): 249.216

Attention duration (in seconds): 0.0648
Attention throughput (in TFLOP/s): 219.968
MLP duration (in seconds): 0.1068
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1716
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20480x30720, b=2048): 0.0404
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20480x30720, b=2048): 255.140
Elapsed time for attention_key_query_prob (256x2048x160x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x160x2048): 125.328
Elapsed time for attention_prob_times_values (256x2048x2048x160): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x160): 149.917
Elapsed time for attention_linear_projection (4x10240x20480, b=2048): 0.0143
Throughput (in TFLOP/s) for attention_linear_projection (4x10240x20480, b=2048): 240.814
Elapsed time for mlp_h_to_4h (4x20480x40960, b=2048): 0.0532
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20480x40960, b=2048): 258.446
Elapsed time for mlp_4h_to_h (4x40960x20480, b=2048): 0.0553
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40960x20480, b=2048): 248.622

Attention duration (in seconds): 0.0597
Attention throughput (in TFLOP/s): 241.716
MLP duration (in seconds): 0.1085
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1682
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20608x30912, b=2048): 0.0411
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20608x30912, b=2048): 253.903
Elapsed time for attention_key_query_prob (256x2048x161x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x161x2048): 58.013
Elapsed time for attention_prob_times_values (256x2048x2048x161): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x161): 63.431
Elapsed time for attention_linear_projection (4x10304x20608, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_linear_projection (4x10304x20608, b=2048): 240.887
Elapsed time for mlp_h_to_4h (4x20608x41216, b=2048): 0.0542
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20608x41216, b=2048): 256.807
Elapsed time for mlp_4h_to_h (4x41216x20608, b=2048): 0.0564
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41216x20608, b=2048): 246.582

Attention duration (in seconds): 0.0670
Attention throughput (in TFLOP/s): 218.156
MLP duration (in seconds): 0.1106
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1776
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20736x31104, b=2048): 0.0405
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20736x31104, b=2048): 260.958
Elapsed time for attention_key_query_prob (256x2048x162x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x162x2048): 80.108
Elapsed time for attention_prob_times_values (256x2048x2048x162): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x162): 106.485
Elapsed time for attention_linear_projection (4x10368x20736, b=2048): 0.0146
Throughput (in TFLOP/s) for attention_linear_projection (4x10368x20736, b=2048): 241.841
Elapsed time for mlp_h_to_4h (4x20736x41472, b=2048): 0.0547
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20736x41472, b=2048): 257.810
Elapsed time for mlp_4h_to_h (4x41472x20736, b=2048): 0.0583
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41472x20736, b=2048): 241.676

Attention duration (in seconds): 0.0627
Attention throughput (in TFLOP/s): 235.930
MLP duration (in seconds): 0.1130
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1756
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20864x31296, b=2048): 0.0413
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20864x31296, b=2048): 259.265
Elapsed time for attention_key_query_prob (256x2048x163x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x163x2048): 58.118
Elapsed time for attention_prob_times_values (256x2048x2048x163): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x163): 65.485
Elapsed time for attention_linear_projection (4x10432x20864, b=2048): 0.0148
Throughput (in TFLOP/s) for attention_linear_projection (4x10432x20864, b=2048): 241.176
Elapsed time for mlp_h_to_4h (4x20864x41728, b=2048): 0.0555
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20864x41728, b=2048): 256.900
Elapsed time for mlp_4h_to_h (4x41728x20864, b=2048): 0.0583
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41728x20864, b=2048): 244.677

Attention duration (in seconds): 0.0674
Attention throughput (in TFLOP/s): 221.964
MLP duration (in seconds): 0.1138
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1812
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20992x31488, b=2048): 0.0416
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20992x31488, b=2048): 260.506
Elapsed time for attention_key_query_prob (256x2048x164x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x164x2048): 80.738
Elapsed time for attention_prob_times_values (256x2048x2048x164): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x164): 107.275
Elapsed time for attention_linear_projection (4x10496x20992, b=2048): 0.0150
Throughput (in TFLOP/s) for attention_linear_projection (4x10496x20992, b=2048): 241.412
Elapsed time for mlp_h_to_4h (4x20992x41984, b=2048): 0.0559
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20992x41984, b=2048): 258.461
Elapsed time for mlp_4h_to_h (4x41984x20992, b=2048): 0.0586
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41984x20992, b=2048): 246.600

Attention duration (in seconds): 0.0642
Attention throughput (in TFLOP/s): 235.997
MLP duration (in seconds): 0.1144
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21120x31680, b=2048): 0.0423
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21120x31680, b=2048): 258.883
Elapsed time for attention_key_query_prob (256x2048x165x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x165x2048): 58.650
Elapsed time for attention_prob_times_values (256x2048x2048x165): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x165): 66.009
Elapsed time for attention_linear_projection (4x10560x21120, b=2048): 0.0151
Throughput (in TFLOP/s) for attention_linear_projection (4x10560x21120, b=2048): 241.344
Elapsed time for mlp_h_to_4h (4x21120x42240, b=2048): 0.0561
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21120x42240, b=2048): 260.478
Elapsed time for mlp_4h_to_h (4x42240x21120, b=2048): 0.0589
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42240x21120, b=2048): 248.233

Attention duration (in seconds): 0.0689
Attention throughput (in TFLOP/s): 222.442
MLP duration (in seconds): 0.1150
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1839
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21248x31872, b=2048): 0.0431
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21248x31872, b=2048): 257.303
Elapsed time for attention_key_query_prob (256x2048x166x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x166x2048): 81.758
Elapsed time for attention_prob_times_values (256x2048x2048x166): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x166): 104.144
Elapsed time for attention_linear_projection (4x10624x21248, b=2048): 0.0155
Throughput (in TFLOP/s) for attention_linear_projection (4x10624x21248, b=2048): 238.396
Elapsed time for mlp_h_to_4h (4x21248x42496, b=2048): 0.0572
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21248x42496, b=2048): 258.592
Elapsed time for mlp_4h_to_h (4x42496x21248, b=2048): 0.0601
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42496x21248, b=2048): 246.087

Attention duration (in seconds): 0.0664
Attention throughput (in TFLOP/s): 233.470
MLP duration (in seconds): 0.1173
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1837
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21376x32064, b=2048): 0.0439
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21376x32064, b=2048): 255.884
Elapsed time for attention_key_query_prob (256x2048x167x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x167x2048): 59.179
Elapsed time for attention_prob_times_values (256x2048x2048x167): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x167): 64.708
Elapsed time for attention_linear_projection (4x10688x21376, b=2048): 0.0153
Throughput (in TFLOP/s) for attention_linear_projection (4x10688x21376, b=2048): 244.638
Elapsed time for mlp_h_to_4h (4x21376x42752, b=2048): 0.0576
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21376x42752, b=2048): 259.862
Elapsed time for mlp_4h_to_h (4x42752x21376, b=2048): 0.0604
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42752x21376, b=2048): 247.842

Attention duration (in seconds): 0.0708
Attention throughput (in TFLOP/s): 221.646
MLP duration (in seconds): 0.1180
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1888
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21504x32256, b=2048): 0.0437
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21504x32256, b=2048): 260.186
Elapsed time for attention_key_query_prob (256x2048x168x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x168x2048): 111.561
Elapsed time for attention_prob_times_values (256x2048x2048x168): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x168): 154.110
Elapsed time for attention_linear_projection (4x10752x21504, b=2048): 0.0158
Throughput (in TFLOP/s) for attention_linear_projection (4x10752x21504, b=2048): 239.154
Elapsed time for mlp_h_to_4h (4x21504x43008, b=2048): 0.0590
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21504x43008, b=2048): 256.986
Elapsed time for mlp_4h_to_h (4x43008x21504, b=2048): 0.0625
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43008x21504, b=2048): 242.299

Attention duration (in seconds): 0.0651
Attention throughput (in TFLOP/s): 243.869
MLP duration (in seconds): 0.1215
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1866
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21632x32448, b=2048): 0.0444
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21632x32448, b=2048): 258.723
Elapsed time for attention_key_query_prob (256x2048x169x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x169x2048): 59.674
Elapsed time for attention_prob_times_values (256x2048x2048x169): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x169): 64.739
Elapsed time for attention_linear_projection (4x10816x21632, b=2048): 0.0159
Throughput (in TFLOP/s) for attention_linear_projection (4x10816x21632, b=2048): 241.574
Elapsed time for mlp_h_to_4h (4x21632x43264, b=2048): 0.0598
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21632x43264, b=2048): 256.411
Elapsed time for mlp_4h_to_h (4x43264x21632, b=2048): 0.0624
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43264x21632, b=2048): 245.701

Attention duration (in seconds): 0.0720
Attention throughput (in TFLOP/s): 223.029
MLP duration (in seconds): 0.1222
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1942
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21760x32640, b=2048): 0.0448
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21760x32640, b=2048): 259.668
Elapsed time for attention_key_query_prob (256x2048x170x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x170x2048): 83.705
Elapsed time for attention_prob_times_values (256x2048x2048x170): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x170): 111.168
Elapsed time for attention_linear_projection (4x10880x21760, b=2048): 0.0164
Throughput (in TFLOP/s) for attention_linear_projection (4x10880x21760, b=2048): 236.341
Elapsed time for mlp_h_to_4h (4x21760x43520, b=2048): 0.0599
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21760x43520, b=2048): 259.184
Elapsed time for mlp_4h_to_h (4x43520x21760, b=2048): 0.0627
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43520x21760, b=2048): 247.476

Attention duration (in seconds): 0.0689
Attention throughput (in TFLOP/s): 235.885
MLP duration (in seconds): 0.1226
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1914
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21888x32832, b=2048): 0.0461
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21888x32832, b=2048): 255.446
Elapsed time for attention_key_query_prob (256x2048x171x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x171x2048): 60.146
Elapsed time for attention_prob_times_values (256x2048x2048x171): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x171): 68.476
Elapsed time for attention_linear_projection (4x10944x21888, b=2048): 0.0162
Throughput (in TFLOP/s) for attention_linear_projection (4x10944x21888, b=2048): 241.721
Elapsed time for mlp_h_to_4h (4x21888x43776, b=2048): 0.0606
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21888x43776, b=2048): 259.099
Elapsed time for mlp_4h_to_h (4x43776x21888, b=2048): 0.0631
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43776x21888, b=2048): 248.687

Attention duration (in seconds): 0.0738
Attention throughput (in TFLOP/s): 222.682
MLP duration (in seconds): 0.1237
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1975
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22016x33024, b=2048): 0.0464
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22016x33024, b=2048): 256.839
Elapsed time for attention_key_query_prob (256x2048x172x2048): 0.0184
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x172x2048): 20.043
Elapsed time for attention_prob_times_values (256x2048x2048x172): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x172): 112.861
Elapsed time for attention_linear_projection (4x11008x22016, b=2048): 0.0166
Throughput (in TFLOP/s) for attention_linear_projection (4x11008x22016, b=2048): 239.427
Elapsed time for mlp_h_to_4h (4x22016x44032, b=2048): 0.0616
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22016x44032, b=2048): 257.676
Elapsed time for mlp_4h_to_h (4x44032x22016, b=2048): 0.0644
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44032x22016, b=2048): 246.808

Attention duration (in seconds): 0.0847
Attention throughput (in TFLOP/s): 196.320
MLP duration (in seconds): 0.1260
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22144x33216, b=2048): 0.0467
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22144x33216, b=2048): 258.121
Elapsed time for attention_key_query_prob (256x2048x173x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x173x2048): 60.587
Elapsed time for attention_prob_times_values (256x2048x2048x173): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x173): 68.848
Elapsed time for attention_linear_projection (4x11072x22144, b=2048): 0.0168
Throughput (in TFLOP/s) for attention_linear_projection (4x11072x22144, b=2048): 239.132
Elapsed time for mlp_h_to_4h (4x22144x44288, b=2048): 0.1132
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22144x44288, b=2048): 141.932
Elapsed time for mlp_4h_to_h (4x44288x22144, b=2048): 0.0654
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44288x22144, b=2048): 245.813

Attention duration (in seconds): 0.0750
Attention throughput (in TFLOP/s): 224.106
MLP duration (in seconds): 0.1786
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2536
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22272x33408, b=2048): 0.0472
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22272x33408, b=2048): 258.430
Elapsed time for attention_key_query_prob (256x2048x174x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x174x2048): 84.955
Elapsed time for attention_prob_times_values (256x2048x2048x174): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x174): 112.412
Elapsed time for attention_linear_projection (4x11136x22272, b=2048): 0.0168
Throughput (in TFLOP/s) for attention_linear_projection (4x11136x22272, b=2048): 241.946
Elapsed time for mlp_h_to_4h (4x22272x44544, b=2048): 0.0643
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22272x44544, b=2048): 252.668
Elapsed time for mlp_4h_to_h (4x44544x22272, b=2048): 0.0653
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44544x22272, b=2048): 248.755

Attention duration (in seconds): 0.0717
Attention throughput (in TFLOP/s): 237.155
MLP duration (in seconds): 0.1297
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22400x33600, b=2048): 0.0473
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22400x33600, b=2048): 260.486
Elapsed time for attention_key_query_prob (256x2048x175x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x175x2048): 60.841
Elapsed time for attention_prob_times_values (256x2048x2048x175): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x175): 67.523
Elapsed time for attention_linear_projection (4x11200x22400, b=2048): 0.0174
Throughput (in TFLOP/s) for attention_linear_projection (4x11200x22400, b=2048): 236.581
Elapsed time for mlp_h_to_4h (4x22400x44800, b=2048): 0.0631
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22400x44800, b=2048): 260.392
Elapsed time for mlp_4h_to_h (4x44800x22400, b=2048): 0.0772
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44800x22400, b=2048): 213.025

Attention duration (in seconds): 0.0765
Attention throughput (in TFLOP/s): 224.877
MLP duration (in seconds): 0.1403
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22528x33792, b=2048): 0.0481
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22528x33792, b=2048): 259.230
Elapsed time for attention_key_query_prob (256x2048x176x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x176x2048): 124.618
Elapsed time for attention_prob_times_values (256x2048x2048x176): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x176): 161.828
Elapsed time for attention_linear_projection (4x11264x22528, b=2048): 0.0171
Throughput (in TFLOP/s) for attention_linear_projection (4x11264x22528, b=2048): 242.706
Elapsed time for mlp_h_to_4h (4x22528x45056, b=2048): 0.0650
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22528x45056, b=2048): 255.709
Elapsed time for mlp_4h_to_h (4x45056x22528, b=2048): 0.1058
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45056x22528, b=2048): 157.146

Attention duration (in seconds): 0.0706
Attention throughput (in TFLOP/s): 246.218
MLP duration (in seconds): 0.1709
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2415
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22656x33984, b=2048): 0.0491
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22656x33984, b=2048): 256.846
Elapsed time for attention_key_query_prob (256x2048x177x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x177x2048): 61.219
Elapsed time for attention_prob_times_values (256x2048x2048x177): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x177): 67.862
Elapsed time for attention_linear_projection (4x11328x22656, b=2048): 0.0177
Throughput (in TFLOP/s) for attention_linear_projection (4x11328x22656, b=2048): 238.069
Elapsed time for mlp_h_to_4h (4x22656x45312, b=2048): 0.0648
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22656x45312, b=2048): 259.502
Elapsed time for mlp_4h_to_h (4x45312x22656, b=2048): 0.0687
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45312x22656, b=2048): 244.812

Attention duration (in seconds): 0.0786
Attention throughput (in TFLOP/s): 223.700
MLP duration (in seconds): 0.1335
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22784x34176, b=2048): 0.0493
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22784x34176, b=2048): 259.006
Elapsed time for attention_key_query_prob (256x2048x178x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x178x2048): 86.240
Elapsed time for attention_prob_times_values (256x2048x2048x178): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x178): 114.676
Elapsed time for attention_linear_projection (4x11392x22784, b=2048): 0.0181
Throughput (in TFLOP/s) for attention_linear_projection (4x11392x22784, b=2048): 234.889
Elapsed time for mlp_h_to_4h (4x22784x45568, b=2048): 0.0660
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22784x45568, b=2048): 257.838
Elapsed time for mlp_4h_to_h (4x45568x22784, b=2048): 0.0687
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45568x22784, b=2048): 247.772

Attention duration (in seconds): 0.0751
Attention throughput (in TFLOP/s): 236.597
MLP duration (in seconds): 0.1346
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22912x34368, b=2048): 0.0496
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22912x34368, b=2048): 260.005
Elapsed time for attention_key_query_prob (256x2048x179x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x179x2048): 60.841
Elapsed time for attention_prob_times_values (256x2048x2048x179): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x179): 68.913
Elapsed time for attention_linear_projection (4x11456x22912, b=2048): 0.0181
Throughput (in TFLOP/s) for attention_linear_projection (4x11456x22912, b=2048): 237.441
Elapsed time for mlp_h_to_4h (4x22912x45824, b=2048): 0.0674
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22912x45824, b=2048): 255.170
Elapsed time for mlp_4h_to_h (4x45824x22912, b=2048): 0.0699
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45824x22912, b=2048): 246.085

Attention duration (in seconds): 0.0796
Attention throughput (in TFLOP/s): 225.684
MLP duration (in seconds): 0.1373
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23040x34560, b=2048): 0.0500
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23040x34560, b=2048): 261.171
Elapsed time for attention_key_query_prob (256x2048x180x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x180x2048): 87.190
Elapsed time for attention_prob_times_values (256x2048x2048x180): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x180): 115.165
Elapsed time for attention_linear_projection (4x11520x23040, b=2048): 0.0181
Throughput (in TFLOP/s) for attention_linear_projection (4x11520x23040, b=2048): 239.755
Elapsed time for mlp_h_to_4h (4x23040x46080, b=2048): 0.0800
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23040x46080, b=2048): 217.328
Elapsed time for mlp_4h_to_h (4x46080x23040, b=2048): 0.0699
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46080x23040, b=2048): 248.737

Attention duration (in seconds): 0.0759
Attention throughput (in TFLOP/s): 239.428
MLP duration (in seconds): 0.1500
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23168x34752, b=2048): 0.0509
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23168x34752, b=2048): 259.399
Elapsed time for attention_key_query_prob (256x2048x181x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x181x2048): 61.570
Elapsed time for attention_prob_times_values (256x2048x2048x181): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x181): 70.877
Elapsed time for attention_linear_projection (4x11584x23168, b=2048): 0.0184
Throughput (in TFLOP/s) for attention_linear_projection (4x11584x23168, b=2048): 239.334
Elapsed time for mlp_h_to_4h (4x23168x46336, b=2048): 0.0689
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23168x46336, b=2048): 255.257
Elapsed time for mlp_4h_to_h (4x46336x23168, b=2048): 0.0718
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46336x23168, b=2048): 244.877

Attention duration (in seconds): 0.0810
Attention throughput (in TFLOP/s): 226.674
MLP duration (in seconds): 0.1407
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23296x34944, b=2048): 0.0516
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23296x34944, b=2048): 258.498
Elapsed time for attention_key_query_prob (256x2048x182x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x182x2048): 88.017
Elapsed time for attention_prob_times_values (256x2048x2048x182): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x182): 116.768
Elapsed time for attention_linear_projection (4x11648x23296, b=2048): 0.0187
Throughput (in TFLOP/s) for attention_linear_projection (4x11648x23296, b=2048): 237.262
Elapsed time for mlp_h_to_4h (4x23296x46592, b=2048): 0.0690
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23296x46592, b=2048): 257.914
Elapsed time for mlp_4h_to_h (4x46592x23296, b=2048): 0.0726
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46592x23296, b=2048): 245.060

Attention duration (in seconds): 0.0781
Attention throughput (in TFLOP/s): 237.641
MLP duration (in seconds): 0.1415
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23424x35136, b=2048): 0.0519
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23424x35136, b=2048): 259.605
Elapsed time for attention_key_query_prob (256x2048x183x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x183x2048): 61.537
Elapsed time for attention_prob_times_values (256x2048x2048x183): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x183): 69.394
Elapsed time for attention_linear_projection (4x11712x23424, b=2048): 0.0187
Throughput (in TFLOP/s) for attention_linear_projection (4x11712x23424, b=2048): 239.981
Elapsed time for mlp_h_to_4h (4x23424x46848, b=2048): 0.0695
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23424x46848, b=2048): 258.847
Elapsed time for mlp_4h_to_h (4x46848x23424, b=2048): 0.0731
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46848x23424, b=2048): 246.019

Attention duration (in seconds): 0.0827
Attention throughput (in TFLOP/s): 226.849
MLP duration (in seconds): 0.1425
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23552x35328, b=2048): 0.0536
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23552x35328, b=2048): 254.567
Elapsed time for attention_key_query_prob (256x2048x184x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x184x2048): 114.440
Elapsed time for attention_prob_times_values (256x2048x2048x184): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x184): 165.484
Elapsed time for attention_linear_projection (4x11776x23552, b=2048): 0.0192
Throughput (in TFLOP/s) for attention_linear_projection (4x11776x23552, b=2048): 236.531
Elapsed time for mlp_h_to_4h (4x23552x47104, b=2048): 0.0712
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23552x47104, b=2048): 255.275
Elapsed time for mlp_4h_to_h (4x47104x23552, b=2048): 0.0746
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47104x23552, b=2048): 243.812

Attention duration (in seconds): 0.0786
Attention throughput (in TFLOP/s): 241.297
MLP duration (in seconds): 0.1458
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23680x35520, b=2048): 0.0538
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23680x35520, b=2048): 256.260
Elapsed time for attention_key_query_prob (256x2048x185x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x185x2048): 62.181
Elapsed time for attention_prob_times_values (256x2048x2048x185): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x185): 69.639
Elapsed time for attention_linear_projection (4x11840x23680, b=2048): 0.0189
Throughput (in TFLOP/s) for attention_linear_projection (4x11840x23680, b=2048): 243.190
Elapsed time for mlp_h_to_4h (4x23680x47360, b=2048): 0.0725
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23680x47360, b=2048): 253.313
Elapsed time for mlp_4h_to_h (4x47360x23680, b=2048): 0.0739
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47360x23680, b=2048): 248.529

Attention duration (in seconds): 0.0848
Attention throughput (in TFLOP/s): 226.157
MLP duration (in seconds): 0.1465
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2312
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23808x35712, b=2048): 0.0538
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23808x35712, b=2048): 258.796
Elapsed time for attention_key_query_prob (256x2048x186x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x186x2048): 89.696
Elapsed time for attention_prob_times_values (256x2048x2048x186): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x186): 118.073
Elapsed time for attention_linear_projection (4x11904x23808, b=2048): 0.0191
Throughput (in TFLOP/s) for attention_linear_projection (4x11904x23808, b=2048): 242.850
Elapsed time for mlp_h_to_4h (4x23808x47616, b=2048): 0.0725
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23808x47616, b=2048): 256.168
Elapsed time for mlp_4h_to_h (4x47616x23808, b=2048): 0.0747
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47616x23808, b=2048): 248.787

Attention duration (in seconds): 0.0808
Attention throughput (in TFLOP/s): 239.807
MLP duration (in seconds): 0.1472
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23936x35904, b=2048): 0.0543
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23936x35904, b=2048): 259.094
Elapsed time for attention_key_query_prob (256x2048x187x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x187x2048): 62.683
Elapsed time for attention_prob_times_values (256x2048x2048x187): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x187): 73.147
Elapsed time for attention_linear_projection (4x11968x23936, b=2048): 0.0192
Throughput (in TFLOP/s) for attention_linear_projection (4x11968x23936, b=2048): 244.005
Elapsed time for mlp_h_to_4h (4x23936x47872, b=2048): 0.0729
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23936x47872, b=2048): 257.399
Elapsed time for mlp_4h_to_h (4x47872x23936, b=2048): 0.0767
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47872x23936, b=2048): 244.678

Attention duration (in seconds): 0.0855
Attention throughput (in TFLOP/s): 229.034
MLP duration (in seconds): 0.1497
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2351
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24064x36096, b=2048): 0.0549
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24064x36096, b=2048): 259.056
Elapsed time for attention_key_query_prob (256x2048x188x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x188x2048): 87.916
Elapsed time for attention_prob_times_values (256x2048x2048x188): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x188): 120.070
Elapsed time for attention_linear_projection (4x12032x24064, b=2048): 0.0197
Throughput (in TFLOP/s) for attention_linear_projection (4x12032x24064, b=2048): 241.280
Elapsed time for mlp_h_to_4h (4x24064x48128, b=2048): 0.0750
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24064x48128, b=2048): 253.020
Elapsed time for mlp_4h_to_h (4x48128x24064, b=2048): 0.0772
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48128x24064, b=2048): 245.794

Attention duration (in seconds): 0.0826
Attention throughput (in TFLOP/s): 239.641
MLP duration (in seconds): 0.1522
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2347
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24192x36288, b=2048): 0.0562
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24192x36288, b=2048): 255.955
Elapsed time for attention_key_query_prob (256x2048x189x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x189x2048): 63.637
Elapsed time for attention_prob_times_values (256x2048x2048x189): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x189): 74.316
Elapsed time for attention_linear_projection (4x12096x24192, b=2048): 0.0202
Throughput (in TFLOP/s) for attention_linear_projection (4x12096x24192, b=2048): 237.147
Elapsed time for mlp_h_to_4h (4x24192x48384, b=2048): 0.0746
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24192x48384, b=2048): 256.909
Elapsed time for mlp_4h_to_h (4x48384x24192, b=2048): 0.0783
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48384x24192, b=2048): 244.820

Attention duration (in seconds): 0.0883
Attention throughput (in TFLOP/s): 226.507
MLP duration (in seconds): 0.1530
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2412
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24320x36480, b=2048): 0.0559
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24320x36480, b=2048): 259.993
Elapsed time for attention_key_query_prob (256x2048x190x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x190x2048): 91.821
Elapsed time for attention_prob_times_values (256x2048x2048x190): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x190): 121.270
Elapsed time for attention_linear_projection (4x12160x24320, b=2048): 0.0198
Throughput (in TFLOP/s) for attention_linear_projection (4x12160x24320, b=2048): 244.173
Elapsed time for mlp_h_to_4h (4x24320x48640, b=2048): 0.0766
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24320x48640, b=2048): 252.973
Elapsed time for mlp_4h_to_h (4x48640x24320, b=2048): 0.0785
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48640x24320, b=2048): 247.014

Attention duration (in seconds): 0.0836
Attention throughput (in TFLOP/s): 241.707
MLP duration (in seconds): 0.1551
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2386
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24448x36672, b=2048): 0.0568
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24448x36672, b=2048): 258.764
Elapsed time for attention_key_query_prob (256x2048x191x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x191x2048): 63.931
Elapsed time for attention_prob_times_values (256x2048x2048x191): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x191): 73.624
Elapsed time for attention_linear_projection (4x12224x24448, b=2048): 0.0200
Throughput (in TFLOP/s) for attention_linear_projection (4x12224x24448, b=2048): 245.364
Elapsed time for mlp_h_to_4h (4x24448x48896, b=2048): 0.0762
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24448x48896, b=2048): 257.132
Elapsed time for mlp_4h_to_h (4x48896x24448, b=2048): 0.0790
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48896x24448, b=2048): 248.009

Attention duration (in seconds): 0.0887
Attention throughput (in TFLOP/s): 230.031
MLP duration (in seconds): 0.1551
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2438
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24576x36864, b=2048): 0.0582
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24576x36864, b=2048): 254.861
Elapsed time for attention_key_query_prob (256x2048x192x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x192x2048): 138.450
Elapsed time for attention_prob_times_values (256x2048x2048x192): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x192): 175.429
Elapsed time for attention_linear_projection (4x12288x24576, b=2048): 0.0206
Throughput (in TFLOP/s) for attention_linear_projection (4x12288x24576, b=2048): 240.312
Elapsed time for mlp_h_to_4h (4x24576x49152, b=2048): 0.0772
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24576x49152, b=2048): 256.326
Elapsed time for mlp_4h_to_h (4x49152x24576, b=2048): 0.0906
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49152x24576, b=2048): 218.533

Attention duration (in seconds): 0.0842
Attention throughput (in TFLOP/s): 244.964
MLP duration (in seconds): 0.1678
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2519
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24704x37056, b=2048): 0.0575
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24704x37056, b=2048): 260.833
Elapsed time for attention_key_query_prob (256x2048x193x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x193x2048): 61.814
Elapsed time for attention_prob_times_values (256x2048x2048x193): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x193): 73.555
Elapsed time for attention_linear_projection (4x12352x24704, b=2048): 0.0200
Throughput (in TFLOP/s) for attention_linear_projection (4x12352x24704, b=2048): 249.635
Elapsed time for mlp_h_to_4h (4x24704x49408, b=2048): 0.0787
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24704x49408, b=2048): 254.251
Elapsed time for mlp_4h_to_h (4x49408x24704, b=2048): 0.0805
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49408x24704, b=2048): 248.409

Attention duration (in seconds): 0.0899
Attention throughput (in TFLOP/s): 231.747
MLP duration (in seconds): 0.1592
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2490
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24832x37248, b=2048): 0.0657
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24832x37248, b=2048): 230.760
Elapsed time for attention_key_query_prob (256x2048x194x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x194x2048): 87.964
Elapsed time for attention_prob_times_values (256x2048x2048x194): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x194): 121.923
Elapsed time for attention_linear_projection (4x12416x24832, b=2048): 0.0216
Throughput (in TFLOP/s) for attention_linear_projection (4x12416x24832, b=2048): 234.076
Elapsed time for mlp_h_to_4h (4x24832x49664, b=2048): 0.0807
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24832x49664, b=2048): 250.370
Elapsed time for mlp_4h_to_h (4x49664x24832, b=2048): 0.0814
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49664x24832, b=2048): 248.282

Attention duration (in seconds): 0.0954
Attention throughput (in TFLOP/s): 220.523
MLP duration (in seconds): 0.1621
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2575
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24960x37440, b=2048): 0.0588
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24960x37440, b=2048): 260.519
Elapsed time for attention_key_query_prob (256x2048x195x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x195x2048): 61.702
Elapsed time for attention_prob_times_values (256x2048x2048x195): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x195): 75.907
Elapsed time for attention_linear_projection (4x12480x24960, b=2048): 0.0200
Throughput (in TFLOP/s) for attention_linear_projection (4x12480x24960, b=2048): 255.563
Elapsed time for mlp_h_to_4h (4x24960x49920, b=2048): 0.0797
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24960x49920, b=2048): 256.061
Elapsed time for mlp_4h_to_h (4x49920x24960, b=2048): 0.0830
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49920x24960, b=2048): 245.866

Attention duration (in seconds): 0.0910
Attention throughput (in TFLOP/s): 233.425
MLP duration (in seconds): 0.1628
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2538
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25088x37632, b=2048): 0.0603
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25088x37632, b=2048): 256.433
Elapsed time for attention_key_query_prob (256x2048x196x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x196x2048): 88.341
Elapsed time for attention_prob_times_values (256x2048x2048x196): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x196): 121.752
Elapsed time for attention_linear_projection (4x12544x25088, b=2048): 0.0204
Throughput (in TFLOP/s) for attention_linear_projection (4x12544x25088, b=2048): 252.652
Elapsed time for mlp_h_to_4h (4x25088x50176, b=2048): 0.0797
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25088x50176, b=2048): 258.918
Elapsed time for mlp_4h_to_h (4x50176x25088, b=2048): 0.0833
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50176x25088, b=2048): 247.497

Attention duration (in seconds): 0.0890
Attention throughput (in TFLOP/s): 241.328
MLP duration (in seconds): 0.1630
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2519
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25216x37824, b=2048): 0.0604
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25216x37824, b=2048): 258.859
Elapsed time for attention_key_query_prob (256x2048x197x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x197x2048): 62.230
Elapsed time for attention_prob_times_values (256x2048x2048x197): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x197): 75.995
Elapsed time for attention_linear_projection (4x12608x25216, b=2048): 0.0205
Throughput (in TFLOP/s) for attention_linear_projection (4x12608x25216, b=2048): 253.593
Elapsed time for mlp_h_to_4h (4x25216x50432, b=2048): 0.0800
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25216x50432, b=2048): 260.453
Elapsed time for mlp_4h_to_h (4x50432x25216, b=2048): 0.0848
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50432x25216, b=2048): 245.744

Attention duration (in seconds): 0.0933
Attention throughput (in TFLOP/s): 232.454
MLP duration (in seconds): 0.1648
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2581
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25344x38016, b=2048): 0.0606
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25344x38016, b=2048): 260.671
Elapsed time for attention_key_query_prob (256x2048x198x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x198x2048): 89.176
Elapsed time for attention_prob_times_values (256x2048x2048x198): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x198): 120.299
Elapsed time for attention_linear_projection (4x12672x25344, b=2048): 0.0206
Throughput (in TFLOP/s) for attention_linear_projection (4x12672x25344, b=2048): 255.126
Elapsed time for mlp_h_to_4h (4x25344x50688, b=2048): 0.0821
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25344x50688, b=2048): 256.343
Elapsed time for mlp_4h_to_h (4x50688x25344, b=2048): 0.0850
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50688x25344, b=2048): 247.576

Attention duration (in seconds): 0.0895
Attention throughput (in TFLOP/s): 244.710
MLP duration (in seconds): 0.1671
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2566
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25472x38208, b=2048): 0.0617
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25472x38208, b=2048): 258.624
Elapsed time for attention_key_query_prob (256x2048x199x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x199x2048): 61.947
Elapsed time for attention_prob_times_values (256x2048x2048x199): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x199): 73.711
Elapsed time for attention_linear_projection (4x12736x25472, b=2048): 0.0205
Throughput (in TFLOP/s) for attention_linear_projection (4x12736x25472, b=2048): 259.051
Elapsed time for mlp_h_to_4h (4x25472x50944, b=2048): 0.0827
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25472x50944, b=2048): 256.984
Elapsed time for mlp_4h_to_h (4x50944x25472, b=2048): 0.0856
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50944x25472, b=2048): 248.510

Attention duration (in seconds): 0.0949
Attention throughput (in TFLOP/s): 233.114
MLP duration (in seconds): 0.1683
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2632
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25600x38400, b=2048): 0.0657
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25600x38400, b=2048): 245.277
Elapsed time for attention_key_query_prob (256x2048x200x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x200x2048): 123.091
Elapsed time for attention_prob_times_values (256x2048x2048x200): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x200): 175.493
Elapsed time for attention_linear_projection (4x12800x25600, b=2048): 0.0210
Throughput (in TFLOP/s) for attention_linear_projection (4x12800x25600, b=2048): 256.233
Elapsed time for mlp_h_to_4h (4x25600x51200, b=2048): 0.0833
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25600x51200, b=2048): 257.864
Elapsed time for mlp_4h_to_h (4x51200x25600, b=2048): 0.0872
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51200x25600, b=2048): 246.302

Attention duration (in seconds): 0.0926
Attention throughput (in TFLOP/s): 241.306
MLP duration (in seconds): 0.1705
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2630
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25728x38592, b=2048): 0.0651
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25728x38592, b=2048): 250.044
Elapsed time for attention_key_query_prob (256x2048x201x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x201x2048): 62.756
Elapsed time for attention_prob_times_values (256x2048x2048x201): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x201): 73.306
Elapsed time for attention_linear_projection (4x12864x25728, b=2048): 0.0210
Throughput (in TFLOP/s) for attention_linear_projection (4x12864x25728, b=2048): 257.635
Elapsed time for mlp_h_to_4h (4x25728x51456, b=2048): 0.0851
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25728x51456, b=2048): 254.743
Elapsed time for mlp_4h_to_h (4x51456x25728, b=2048): 0.0884
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51456x25728, b=2048): 245.399

Attention duration (in seconds): 0.0989
Attention throughput (in TFLOP/s): 228.106
MLP duration (in seconds): 0.1735
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2724
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25856x38784, b=2048): 0.0636
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25856x38784, b=2048): 258.425
Elapsed time for attention_key_query_prob (256x2048x202x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x202x2048): 89.681
Elapsed time for attention_prob_times_values (256x2048x2048x202): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x202): 120.350
Elapsed time for attention_linear_projection (4x12928x25856, b=2048): 0.0215
Throughput (in TFLOP/s) for attention_linear_projection (4x12928x25856, b=2048): 255.291
Elapsed time for mlp_h_to_4h (4x25856x51712, b=2048): 0.0844
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25856x51712, b=2048): 259.570
Elapsed time for mlp_4h_to_h (4x51712x25856, b=2048): 0.0896
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51712x25856, b=2048): 244.591

Attention duration (in seconds): 0.0935
Attention throughput (in TFLOP/s): 243.649
MLP duration (in seconds): 0.1740
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2674
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25984x38976, b=2048): 0.0642
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25984x38976, b=2048): 258.395
Elapsed time for attention_key_query_prob (256x2048x203x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x203x2048): 63.537
Elapsed time for attention_prob_times_values (256x2048x2048x203): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x203): 76.310
Elapsed time for attention_linear_projection (4x12992x25984, b=2048): 0.0219
Throughput (in TFLOP/s) for attention_linear_projection (4x12992x25984, b=2048): 252.893
Elapsed time for mlp_h_to_4h (4x25984x51968, b=2048): 0.0876
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25984x51968, b=2048): 252.521
Elapsed time for mlp_4h_to_h (4x51968x25984, b=2048): 0.0893
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51968x25984, b=2048): 247.778

Attention duration (in seconds): 0.0987
Attention throughput (in TFLOP/s): 233.081
MLP duration (in seconds): 0.1769
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2756
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26112x39168, b=2048): 0.0643
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26112x39168, b=2048): 260.742
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 91.004
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 121.333
Elapsed time for attention_linear_projection (4x13056x26112, b=2048): 0.0217
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x26112, b=2048): 257.583
Elapsed time for mlp_h_to_4h (4x26112x52224, b=2048): 0.0888
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26112x52224, b=2048): 251.555
Elapsed time for mlp_4h_to_h (4x52224x26112, b=2048): 0.0908
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x26112, b=2048): 246.118

Attention duration (in seconds): 0.0944
Attention throughput (in TFLOP/s): 246.024
MLP duration (in seconds): 0.1796
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2740
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26240x39360, b=2048): 0.1167
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26240x39360, b=2048): 145.062
Elapsed time for attention_key_query_prob (256x2048x205x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x205x2048): 64.103
Elapsed time for attention_prob_times_values (256x2048x2048x205): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x205): 76.357
Elapsed time for attention_linear_projection (4x13120x26240, b=2048): 0.0221
Throughput (in TFLOP/s) for attention_linear_projection (4x13120x26240, b=2048): 255.483
Elapsed time for mlp_h_to_4h (4x26240x52480, b=2048): 0.0934
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26240x52480, b=2048): 241.593
Elapsed time for mlp_4h_to_h (4x52480x26240, b=2048): 0.0915
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52480x26240, b=2048): 246.710

Attention duration (in seconds): 0.1514
Attention throughput (in TFLOP/s): 154.878
MLP duration (in seconds): 0.1848
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3362
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26368x39552, b=2048): 0.0656
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26368x39552, b=2048): 260.523
Elapsed time for attention_key_query_prob (256x2048x206x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x206x2048): 90.875
Elapsed time for attention_prob_times_values (256x2048x2048x206): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x206): 122.733
Elapsed time for attention_linear_projection (4x13184x26368, b=2048): 0.0223
Throughput (in TFLOP/s) for attention_linear_projection (4x13184x26368, b=2048): 255.753
Elapsed time for mlp_h_to_4h (4x26368x52736, b=2048): 0.0886
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26368x52736, b=2048): 257.041
Elapsed time for mlp_4h_to_h (4x52736x26368, b=2048): 0.0921
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52736x26368, b=2048): 247.361

Attention duration (in seconds): 0.0963
Attention throughput (in TFLOP/s): 245.691
MLP duration (in seconds): 0.1807
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2771
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26496x39744, b=2048): 0.0685
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26496x39744, b=2048): 251.730
Elapsed time for attention_key_query_prob (256x2048x207x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x207x2048): 64.017
Elapsed time for attention_prob_times_values (256x2048x2048x207): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x207): 75.498
Elapsed time for attention_linear_projection (4x13248x26496, b=2048): 0.0224
Throughput (in TFLOP/s) for attention_linear_projection (4x13248x26496, b=2048): 256.850
Elapsed time for mlp_h_to_4h (4x26496x52992, b=2048): 0.0892
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26496x52992, b=2048): 257.948
Elapsed time for mlp_4h_to_h (4x52992x26496, b=2048): 0.0935
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52992x26496, b=2048): 245.915

Attention duration (in seconds): 0.1038
Attention throughput (in TFLOP/s): 230.272
MLP duration (in seconds): 0.1827
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2865
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26624x39936, b=2048): 0.0679
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26624x39936, b=2048): 256.733
Elapsed time for attention_key_query_prob (256x2048x208x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x208x2048): 136.136
Elapsed time for attention_prob_times_values (256x2048x2048x208): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x208): 187.237
Elapsed time for attention_linear_projection (4x13312x26624, b=2048): 0.0228
Throughput (in TFLOP/s) for attention_linear_projection (4x13312x26624, b=2048): 254.461
Elapsed time for mlp_h_to_4h (4x26624x53248, b=2048): 0.0899
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26624x53248, b=2048): 258.244
Elapsed time for mlp_4h_to_h (4x53248x26624, b=2048): 0.0945
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53248x26624, b=2048): 245.771

Attention duration (in seconds): 0.0963
Attention throughput (in TFLOP/s): 250.367
MLP duration (in seconds): 0.1845
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2808
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26752x40128, b=2048): 0.0682
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26752x40128, b=2048): 257.730
Elapsed time for attention_key_query_prob (256x2048x209x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x209x2048): 64.416
Elapsed time for attention_prob_times_values (256x2048x2048x209): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x209): 75.828
Elapsed time for attention_linear_projection (4x13376x26752, b=2048): 0.0226
Throughput (in TFLOP/s) for attention_linear_projection (4x13376x26752, b=2048): 259.033
Elapsed time for mlp_h_to_4h (4x26752x53504, b=2048): 0.0902
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26752x53504, b=2048): 259.999
Elapsed time for mlp_4h_to_h (4x53504x26752, b=2048): 0.0943
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53504x26752, b=2048): 248.647

Attention duration (in seconds): 0.1038
Attention throughput (in TFLOP/s): 234.657
MLP duration (in seconds): 0.1845
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2883
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26880x40320, b=2048): 0.0682
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26880x40320, b=2048): 260.459
Elapsed time for attention_key_query_prob (256x2048x210x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x210x2048): 92.576
Elapsed time for attention_prob_times_values (256x2048x2048x210): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x210): 124.474
Elapsed time for attention_linear_projection (4x13440x26880, b=2048): 0.0233
Throughput (in TFLOP/s) for attention_linear_projection (4x13440x26880, b=2048): 253.623
Elapsed time for mlp_h_to_4h (4x26880x53760, b=2048): 0.1054
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26880x53760, b=2048): 224.704
Elapsed time for mlp_4h_to_h (4x53760x26880, b=2048): 0.1016
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53760x26880, b=2048): 233.032

Attention duration (in seconds): 0.1000
Attention throughput (in TFLOP/s): 245.760
MLP duration (in seconds): 0.2070
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27008x40512, b=2048): 0.0699
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27008x40512, b=2048): 256.331
Elapsed time for attention_key_query_prob (256x2048x211x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x211x2048): 64.650
Elapsed time for attention_prob_times_values (256x2048x2048x211): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x211): 77.610
Elapsed time for attention_linear_projection (4x13504x27008, b=2048): 0.0235
Throughput (in TFLOP/s) for attention_linear_projection (4x13504x27008, b=2048): 254.448
Elapsed time for mlp_h_to_4h (4x27008x54016, b=2048): 0.0932
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27008x54016, b=2048): 256.569
Elapsed time for mlp_4h_to_h (4x54016x27008, b=2048): 0.0969
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54016x27008, b=2048): 246.582

Attention duration (in seconds): 0.1063
Attention throughput (in TFLOP/s): 233.453
MLP duration (in seconds): 0.1901
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2964
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27136x40704, b=2048): 0.0704
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27136x40704, b=2048): 257.217
Elapsed time for attention_key_query_prob (256x2048x212x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x212x2048): 93.568
Elapsed time for attention_prob_times_values (256x2048x2048x212): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x212): 125.668
Elapsed time for attention_linear_projection (4x13568x27136, b=2048): 0.0236
Throughput (in TFLOP/s) for attention_linear_projection (4x13568x27136, b=2048): 255.374
Elapsed time for mlp_h_to_4h (4x27136x54272, b=2048): 0.0938
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27136x54272, b=2048): 257.288
Elapsed time for mlp_4h_to_h (4x54272x27136, b=2048): 0.0973
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54272x27136, b=2048): 247.968

Attention duration (in seconds): 0.1025
Attention throughput (in TFLOP/s): 244.370
MLP duration (in seconds): 0.1911
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2936
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27264x40896, b=2048): 0.0709
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27264x40896, b=2048): 257.684
Elapsed time for attention_key_query_prob (256x2048x213x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x213x2048): 65.181
Elapsed time for attention_prob_times_values (256x2048x2048x213): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x213): 78.266
Elapsed time for attention_linear_projection (4x13632x27264, b=2048): 0.0238
Throughput (in TFLOP/s) for attention_linear_projection (4x13632x27264, b=2048): 256.340
Elapsed time for mlp_h_to_4h (4x27264x54528, b=2048): 0.0954
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27264x54528, b=2048): 255.422
Elapsed time for mlp_4h_to_h (4x54528x27264, b=2048): 0.0992
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54528x27264, b=2048): 245.608

Attention duration (in seconds): 0.1075
Attention throughput (in TFLOP/s): 235.069
MLP duration (in seconds): 0.1945
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27392x41088, b=2048): 0.0714
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27392x41088, b=2048): 258.173
Elapsed time for attention_key_query_prob (256x2048x214x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x214x2048): 94.883
Elapsed time for attention_prob_times_values (256x2048x2048x214): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x214): 126.812
Elapsed time for attention_linear_projection (4x13696x27392, b=2048): 0.0242
Throughput (in TFLOP/s) for attention_linear_projection (4x13696x27392, b=2048): 254.244
Elapsed time for mlp_h_to_4h (4x27392x54784, b=2048): 0.0954
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27392x54784, b=2048): 257.775
Elapsed time for mlp_4h_to_h (4x54784x27392, b=2048): 0.1108
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54784x27392, b=2048): 221.838

Attention duration (in seconds): 0.1041
Attention throughput (in TFLOP/s): 245.086
MLP duration (in seconds): 0.2062
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27520x41280, b=2048): 0.0716
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27520x41280, b=2048): 259.881
Elapsed time for attention_key_query_prob (256x2048x215x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x215x2048): 65.746
Elapsed time for attention_prob_times_values (256x2048x2048x215): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x215): 77.354
Elapsed time for attention_linear_projection (4x13760x27520, b=2048): 0.0243
Throughput (in TFLOP/s) for attention_linear_projection (4x13760x27520, b=2048): 255.124
Elapsed time for mlp_h_to_4h (4x27520x55040, b=2048): 0.0959
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27520x55040, b=2048): 258.670
Elapsed time for mlp_4h_to_h (4x55040x27520, b=2048): 0.1011
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55040x27520, b=2048): 245.534

Attention duration (in seconds): 0.1089
Attention throughput (in TFLOP/s): 236.302
MLP duration (in seconds): 0.1970
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27648x41472, b=2048): 0.0722
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27648x41472, b=2048): 260.083
Elapsed time for attention_key_query_prob (256x2048x216x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x216x2048): 125.028
Elapsed time for attention_prob_times_values (256x2048x2048x216): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x216): 190.312
Elapsed time for attention_linear_projection (4x13824x27648, b=2048): 0.0245
Throughput (in TFLOP/s) for attention_linear_projection (4x13824x27648, b=2048): 255.944
Elapsed time for mlp_h_to_4h (4x27648x55296, b=2048): 0.0979
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27648x55296, b=2048): 255.851
Elapsed time for mlp_4h_to_h (4x55296x27648, b=2048): 0.1008
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55296x27648, b=2048): 248.508

Attention duration (in seconds): 0.1028
Attention throughput (in TFLOP/s): 252.573
MLP duration (in seconds): 0.1987
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27776x41664, b=2048): 0.0737
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27776x41664, b=2048): 257.440
Elapsed time for attention_key_query_prob (256x2048x217x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x217x2048): 66.205
Elapsed time for attention_prob_times_values (256x2048x2048x217): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x217): 77.574
Elapsed time for attention_linear_projection (4x13888x27776, b=2048): 0.0265
Throughput (in TFLOP/s) for attention_linear_projection (4x13888x27776, b=2048): 238.562
Elapsed time for mlp_h_to_4h (4x27776x55552, b=2048): 0.0991
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27776x55552, b=2048): 255.117
Elapsed time for mlp_4h_to_h (4x55552x27776, b=2048): 0.1022
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55552x27776, b=2048): 247.268

Attention duration (in seconds): 0.1132
Attention throughput (in TFLOP/s): 231.583
MLP duration (in seconds): 0.2013
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27904x41856, b=2048): 0.0736
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27904x41856, b=2048): 260.015
Elapsed time for attention_key_query_prob (256x2048x218x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x218x2048): 96.461
Elapsed time for attention_prob_times_values (256x2048x2048x218): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x218): 127.820
Elapsed time for attention_linear_projection (4x13952x27904, b=2048): 0.0247
Throughput (in TFLOP/s) for attention_linear_projection (4x13952x27904, b=2048): 258.018
Elapsed time for mlp_h_to_4h (4x27904x55808, b=2048): 0.0995
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27904x55808, b=2048): 256.515
Elapsed time for mlp_4h_to_h (4x55808x27904, b=2048): 0.1027
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55808x27904, b=2048): 248.484

Attention duration (in seconds): 0.1068
Attention throughput (in TFLOP/s): 247.591
MLP duration (in seconds): 0.2021
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28032x42048, b=2048): 0.0749
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28032x42048, b=2048): 257.967
Elapsed time for attention_key_query_prob (256x2048x219x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x219x2048): 66.605
Elapsed time for attention_prob_times_values (256x2048x2048x219): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x219): 80.833
Elapsed time for attention_linear_projection (4x14016x28032, b=2048): 0.0311
Throughput (in TFLOP/s) for attention_linear_projection (4x14016x28032, b=2048): 206.997
Elapsed time for mlp_h_to_4h (4x28032x56064, b=2048): 0.1011
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28032x56064, b=2048): 254.583
Elapsed time for mlp_4h_to_h (4x56064x28032, b=2048): 0.1034
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56064x28032, b=2048): 249.073

Attention duration (in seconds): 0.1188
Attention throughput (in TFLOP/s): 224.587
MLP duration (in seconds): 0.2045
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28160x42240, b=2048): 0.0818
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28160x42240, b=2048): 238.368
Elapsed time for attention_key_query_prob (256x2048x220x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x220x2048): 97.165
Elapsed time for attention_prob_times_values (256x2048x2048x220): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x220): 129.043
Elapsed time for attention_linear_projection (4x14080x28160, b=2048): 0.0253
Throughput (in TFLOP/s) for attention_linear_projection (4x14080x28160, b=2048): 256.953
Elapsed time for mlp_h_to_4h (4x28160x56320, b=2048): 0.1014
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28160x56320, b=2048): 256.155
Elapsed time for mlp_4h_to_h (4x56320x28160, b=2048): 0.1302
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56320x28160, b=2048): 199.536

Attention duration (in seconds): 0.1156
Attention throughput (in TFLOP/s): 233.029
MLP duration (in seconds): 0.2317
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3472
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28288x42432, b=2048): 0.0765
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28288x42432, b=2048): 256.942
Elapsed time for attention_key_query_prob (256x2048x221x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x221x2048): 67.202
Elapsed time for attention_prob_times_values (256x2048x2048x221): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x221): 81.645
Elapsed time for attention_linear_projection (4x14144x28288, b=2048): 0.0254
Throughput (in TFLOP/s) for attention_linear_projection (4x14144x28288, b=2048): 257.835
Elapsed time for mlp_h_to_4h (4x28288x56576, b=2048): 0.1023
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28288x56576, b=2048): 256.248
Elapsed time for mlp_4h_to_h (4x56576x28288, b=2048): 0.1185
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56576x28288, b=2048): 221.335

Attention duration (in seconds): 0.1148
Attention throughput (in TFLOP/s): 236.598
MLP duration (in seconds): 0.2208
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3356
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28416x42624, b=2048): 0.0772
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28416x42624, b=2048): 257.127
Elapsed time for attention_key_query_prob (256x2048x222x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x222x2048): 98.193
Elapsed time for attention_prob_times_values (256x2048x2048x222): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x222): 130.445
Elapsed time for attention_linear_projection (4x14208x28416, b=2048): 0.0256
Throughput (in TFLOP/s) for attention_linear_projection (4x14208x28416, b=2048): 258.791
Elapsed time for mlp_h_to_4h (4x28416x56832, b=2048): 0.1030
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28416x56832, b=2048): 256.890
Elapsed time for mlp_4h_to_h (4x56832x28416, b=2048): 0.1074
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56832x28416, b=2048): 246.446

Attention duration (in seconds): 0.1112
Attention throughput (in TFLOP/s): 246.411
MLP duration (in seconds): 0.2104
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28544x42816, b=2048): 0.0779
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28544x42816, b=2048): 257.035
Elapsed time for attention_key_query_prob (256x2048x223x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x223x2048): 68.506
Elapsed time for attention_prob_times_values (256x2048x2048x223): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x223): 81.087
Elapsed time for attention_linear_projection (4x14272x28544, b=2048): 0.0257
Throughput (in TFLOP/s) for attention_linear_projection (4x14272x28544, b=2048): 259.734
Elapsed time for mlp_h_to_4h (4x28544x57088, b=2048): 0.1044
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28544x57088, b=2048): 255.696
Elapsed time for mlp_4h_to_h (4x57088x28544, b=2048): 0.1247
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57088x28544, b=2048): 214.050

Attention duration (in seconds): 0.1165
Attention throughput (in TFLOP/s): 237.398
MLP duration (in seconds): 0.2291
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3456
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28672x43008, b=2048): 0.0783
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28672x43008, b=2048): 257.938
Elapsed time for attention_key_query_prob (256x2048x224x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x224x2048): 150.344
Elapsed time for attention_prob_times_values (256x2048x2048x224): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x224): 199.054
Elapsed time for attention_linear_projection (4x14336x28672, b=2048): 0.0268
Throughput (in TFLOP/s) for attention_linear_projection (4x14336x28672, b=2048): 251.331
Elapsed time for mlp_h_to_4h (4x28672x57344, b=2048): 0.1054
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28672x57344, b=2048): 255.560
Elapsed time for mlp_4h_to_h (4x57344x28672, b=2048): 0.1083
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57344x28672, b=2048): 248.766

Attention duration (in seconds): 0.1107
Attention throughput (in TFLOP/s): 251.946
MLP duration (in seconds): 0.2137
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28800x43200, b=2048): 0.0797
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28800x43200, b=2048): 255.719
Elapsed time for attention_key_query_prob (256x2048x225x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x225x2048): 66.665
Elapsed time for attention_prob_times_values (256x2048x2048x225): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x225): 81.479
Elapsed time for attention_linear_projection (4x14400x28800, b=2048): 0.0263
Throughput (in TFLOP/s) for attention_linear_projection (4x14400x28800, b=2048): 258.659
Elapsed time for mlp_h_to_4h (4x28800x57600, b=2048): 0.1066
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28800x57600, b=2048): 254.930
Elapsed time for mlp_4h_to_h (4x57600x28800, b=2048): 0.1101
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57600x28800, b=2048): 246.903

Attention duration (in seconds): 0.1192
Attention throughput (in TFLOP/s): 236.197
MLP duration (in seconds): 0.2167
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3359
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28928x43392, b=2048): 0.0799
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28928x43392, b=2048): 257.412
Elapsed time for attention_key_query_prob (256x2048x226x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x226x2048): 94.303
Elapsed time for attention_prob_times_values (256x2048x2048x226): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x226): 132.098
Elapsed time for attention_linear_projection (4x14464x28928, b=2048): 0.0271
Throughput (in TFLOP/s) for attention_linear_projection (4x14464x28928, b=2048): 253.132
Elapsed time for mlp_h_to_4h (4x28928x57856, b=2048): 0.1072
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28928x57856, b=2048): 255.874
Elapsed time for mlp_4h_to_h (4x57856x28928, b=2048): 0.1108
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57856x28928, b=2048): 247.454

Attention duration (in seconds): 0.1158
Attention throughput (in TFLOP/s): 245.186
MLP duration (in seconds): 0.2180
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3338
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29056x43584, b=2048): 0.0814
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29056x43584, b=2048): 254.990
Elapsed time for attention_key_query_prob (256x2048x227x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x227x2048): 66.473
Elapsed time for attention_prob_times_values (256x2048x2048x227): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x227): 83.577
Elapsed time for attention_linear_projection (4x14528x29056, b=2048): 0.0272
Throughput (in TFLOP/s) for attention_linear_projection (4x14528x29056, b=2048): 254.253
Elapsed time for mlp_h_to_4h (4x29056x58112, b=2048): 0.1110
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29056x58112, b=2048): 249.146
Elapsed time for mlp_4h_to_h (4x58112x29056, b=2048): 0.1121
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58112x29056, b=2048): 246.688

Attention duration (in seconds): 0.1217
Attention throughput (in TFLOP/s): 235.256
MLP duration (in seconds): 0.2232
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3449
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29184x43776, b=2048): 0.0816
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29184x43776, b=2048): 256.646
Elapsed time for attention_key_query_prob (256x2048x228x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x228x2048): 94.533
Elapsed time for attention_prob_times_values (256x2048x2048x228): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x228): 133.180
Elapsed time for attention_linear_projection (4x14592x29184, b=2048): 0.0267
Throughput (in TFLOP/s) for attention_linear_projection (4x14592x29184, b=2048): 261.420
Elapsed time for mlp_h_to_4h (4x29184x58368, b=2048): 0.1094
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29184x58368, b=2048): 255.016
Elapsed time for mlp_4h_to_h (4x58368x29184, b=2048): 0.1249
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58368x29184, b=2048): 223.361

Attention duration (in seconds): 0.1171
Attention throughput (in TFLOP/s): 246.688
MLP duration (in seconds): 0.2344
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3515
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29312x43968, b=2048): 0.0820
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29312x43968, b=2048): 257.550
Elapsed time for attention_key_query_prob (256x2048x229x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x229x2048): 66.634
Elapsed time for attention_prob_times_values (256x2048x2048x229): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x229): 84.142
Elapsed time for attention_linear_projection (4x14656x29312, b=2048): 0.0274
Throughput (in TFLOP/s) for attention_linear_projection (4x14656x29312, b=2048): 256.790
Elapsed time for mlp_h_to_4h (4x29312x58624, b=2048): 0.1103
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29312x58624, b=2048): 255.271
Elapsed time for mlp_4h_to_h (4x58624x29312, b=2048): 0.1232
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58624x29312, b=2048): 228.462

Attention duration (in seconds): 0.1226
Attention throughput (in TFLOP/s): 237.624
MLP duration (in seconds): 0.2335
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3561
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29440x44160, b=2048): 0.0827
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29440x44160, b=2048): 257.469
Elapsed time for attention_key_query_prob (256x2048x230x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x230x2048): 95.596
Elapsed time for attention_prob_times_values (256x2048x2048x230): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x230): 133.897
Elapsed time for attention_linear_projection (4x14720x29440, b=2048): 0.0280
Throughput (in TFLOP/s) for attention_linear_projection (4x14720x29440, b=2048): 253.877
Elapsed time for mlp_h_to_4h (4x29440x58880, b=2048): 0.1123
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29440x58880, b=2048): 252.954
Elapsed time for mlp_4h_to_h (4x58880x29440, b=2048): 0.1149
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58880x29440, b=2048): 247.227

Attention duration (in seconds): 0.1196
Attention throughput (in TFLOP/s): 245.820
MLP duration (in seconds): 0.2272
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3467
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29568x44352, b=2048): 0.0836
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29568x44352, b=2048): 257.116
Elapsed time for attention_key_query_prob (256x2048x231x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x231x2048): 66.722
Elapsed time for attention_prob_times_values (256x2048x2048x231): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x231): 82.905
Elapsed time for attention_linear_projection (4x14784x29568, b=2048): 0.0278
Throughput (in TFLOP/s) for attention_linear_projection (4x14784x29568, b=2048): 258.005
Elapsed time for mlp_h_to_4h (4x29568x59136, b=2048): 0.1126
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29568x59136, b=2048): 254.411
Elapsed time for mlp_4h_to_h (4x59136x29568, b=2048): 0.1171
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59136x29568, b=2048): 244.695

Attention duration (in seconds): 0.1247
Attention throughput (in TFLOP/s): 237.610
MLP duration (in seconds): 0.2297
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3544
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29696x44544, b=2048): 0.0845
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29696x44544, b=2048): 256.441
Elapsed time for attention_key_query_prob (256x2048x232x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x232x2048): 131.965
Elapsed time for attention_prob_times_values (256x2048x2048x232): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x232): 202.861
Elapsed time for attention_linear_projection (4x14848x29696, b=2048): 0.0279
Throughput (in TFLOP/s) for attention_linear_projection (4x14848x29696, b=2048): 258.870
Elapsed time for mlp_h_to_4h (4x29696x59392, b=2048): 0.1174
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29696x59392, b=2048): 246.067
Elapsed time for mlp_4h_to_h (4x59392x29696, b=2048): 0.1270
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59392x29696, b=2048): 227.474

Attention duration (in seconds): 0.1186
Attention throughput (in TFLOP/s): 251.943
MLP duration (in seconds): 0.2445
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3631
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29824x44736, b=2048): 0.0860
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29824x44736, b=2048): 254.243
Elapsed time for attention_key_query_prob (256x2048x233x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x233x2048): 67.525
Elapsed time for attention_prob_times_values (256x2048x2048x233): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x233): 83.390
Elapsed time for attention_linear_projection (4x14912x29824, b=2048): 0.0284
Throughput (in TFLOP/s) for attention_linear_projection (4x14912x29824, b=2048): 256.936
Elapsed time for mlp_h_to_4h (4x29824x59648, b=2048): 0.1148
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29824x59648, b=2048): 253.912
Elapsed time for mlp_4h_to_h (4x59648x29824, b=2048): 0.1173
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59648x29824, b=2048): 248.477

Attention duration (in seconds): 0.1277
Attention throughput (in TFLOP/s): 235.985
MLP duration (in seconds): 0.2321
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3598
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29952x44928, b=2048): 0.0874
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29952x44928, b=2048): 252.289
Elapsed time for attention_key_query_prob (256x2048x234x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x234x2048): 97.973
Elapsed time for attention_prob_times_values (256x2048x2048x234): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x234): 135.874
Elapsed time for attention_linear_projection (4x14976x29952, b=2048): 0.0285
Throughput (in TFLOP/s) for attention_linear_projection (4x14976x29952, b=2048): 257.940
Elapsed time for mlp_h_to_4h (4x29952x59904, b=2048): 0.1158
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29952x59904, b=2048): 253.937
Elapsed time for mlp_4h_to_h (4x59904x29952, b=2048): 0.1189
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59904x29952, b=2048): 247.236

Attention duration (in seconds): 0.1247
Attention throughput (in TFLOP/s): 243.781
MLP duration (in seconds): 0.2347
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3594
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30080x45120, b=2048): 0.0862
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30080x45120, b=2048): 257.991
Elapsed time for attention_key_query_prob (256x2048x235x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x235x2048): 67.923
Elapsed time for attention_prob_times_values (256x2048x2048x235): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x235): 86.732
Elapsed time for attention_linear_projection (4x15040x30080, b=2048): 0.0290
Throughput (in TFLOP/s) for attention_linear_projection (4x15040x30080, b=2048): 255.653
Elapsed time for mlp_h_to_4h (4x30080x60160, b=2048): 0.1160
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30080x60160, b=2048): 255.580
Elapsed time for mlp_4h_to_h (4x60160x30080, b=2048): 0.1213
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60160x30080, b=2048): 244.448

Attention duration (in seconds): 0.1284
Attention throughput (in TFLOP/s): 238.709
MLP duration (in seconds): 0.2373
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3657
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30208x45312, b=2048): 0.0870
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30208x45312, b=2048): 257.912
Elapsed time for attention_key_query_prob (256x2048x236x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x236x2048): 98.099
Elapsed time for attention_prob_times_values (256x2048x2048x236): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x236): 138.518
Elapsed time for attention_linear_projection (4x15104x30208, b=2048): 0.0288
Throughput (in TFLOP/s) for attention_linear_projection (4x15104x30208, b=2048): 259.702
Elapsed time for mlp_h_to_4h (4x30208x60416, b=2048): 0.1225
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30208x60416, b=2048): 244.123
Elapsed time for mlp_4h_to_h (4x60416x30208, b=2048): 0.1492
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60416x30208, b=2048): 200.400

Attention duration (in seconds): 0.1246
Attention throughput (in TFLOP/s): 248.191
MLP duration (in seconds): 0.2717
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3963
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30336x45504, b=2048): 0.0876
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30336x45504, b=2048): 258.094
Elapsed time for attention_key_query_prob (256x2048x237x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x237x2048): 68.753
Elapsed time for attention_prob_times_values (256x2048x2048x237): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x237): 87.252
Elapsed time for attention_linear_projection (4x15168x30336, b=2048): 0.0293
Throughput (in TFLOP/s) for attention_linear_projection (4x15168x30336, b=2048): 257.296
Elapsed time for mlp_h_to_4h (4x30336x60672, b=2048): 0.1176
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30336x60672, b=2048): 256.337
Elapsed time for mlp_4h_to_h (4x60672x30336, b=2048): 0.1215
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60672x30336, b=2048): 248.242

Attention duration (in seconds): 0.1302
Attention throughput (in TFLOP/s): 239.490
MLP duration (in seconds): 0.2391
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3693
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30464x45696, b=2048): 0.0894
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30464x45696, b=2048): 255.067
Elapsed time for attention_key_query_prob (256x2048x238x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x238x2048): 98.525
Elapsed time for attention_prob_times_values (256x2048x2048x238): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x238): 139.302
Elapsed time for attention_linear_projection (4x15232x30464, b=2048): 0.0297
Throughput (in TFLOP/s) for attention_linear_projection (4x15232x30464, b=2048): 255.833
Elapsed time for mlp_h_to_4h (4x30464x60928, b=2048): 0.1187
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30464x60928, b=2048): 256.093
Elapsed time for mlp_4h_to_h (4x60928x30464, b=2048): 0.1226
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60928x30464, b=2048): 248.145

Attention duration (in seconds): 0.1280
Attention throughput (in TFLOP/s): 245.582
MLP duration (in seconds): 0.2413
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3693
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30592x45888, b=2048): 0.0903
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30592x45888, b=2048): 254.784
Elapsed time for attention_key_query_prob (256x2048x239x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x239x2048): 68.949
Elapsed time for attention_prob_times_values (256x2048x2048x239): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x239): 86.475
Elapsed time for attention_linear_projection (4x15296x30592, b=2048): 0.0296
Throughput (in TFLOP/s) for attention_linear_projection (4x15296x30592, b=2048): 259.291
Elapsed time for mlp_h_to_4h (4x30592x61184, b=2048): 0.1206
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30592x61184, b=2048): 254.311
Elapsed time for mlp_4h_to_h (4x61184x30592, b=2048): 0.1367
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61184x30592, b=2048): 224.386

Attention duration (in seconds): 0.1332
Attention throughput (in TFLOP/s): 237.902
MLP duration (in seconds): 0.2573
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3905
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30720x46080, b=2048): 0.0906
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30720x46080, b=2048): 256.063
Elapsed time for attention_key_query_prob (256x2048x240x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x240x2048): 147.056
Elapsed time for attention_prob_times_values (256x2048x2048x240): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x240): 208.339
Elapsed time for attention_linear_projection (4x15360x30720, b=2048): 0.0304
Throughput (in TFLOP/s) for attention_linear_projection (4x15360x30720, b=2048): 253.998
Elapsed time for mlp_h_to_4h (4x30720x61440, b=2048): 0.1221
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30720x61440, b=2048): 253.203
Elapsed time for mlp_4h_to_h (4x61440x30720, b=2048): 0.1247
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61440x30720, b=2048): 248.042

Attention duration (in seconds): 0.1270
Attention throughput (in TFLOP/s): 251.630
MLP duration (in seconds): 0.2468
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3738
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30848x46272, b=2048): 0.0909
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30848x46272, b=2048): 257.339
Elapsed time for attention_key_query_prob (256x2048x241x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x241x2048): 69.448
Elapsed time for attention_prob_times_values (256x2048x2048x241): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x241): 87.017
Elapsed time for attention_linear_projection (4x15424x30848, b=2048): 0.0361
Throughput (in TFLOP/s) for attention_linear_projection (4x15424x30848, b=2048): 216.128
Elapsed time for mlp_h_to_4h (4x30848x61696, b=2048): 0.1214
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30848x61696, b=2048): 256.814
Elapsed time for mlp_4h_to_h (4x61696x30848, b=2048): 0.1259
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61696x30848, b=2048): 247.681

Attention duration (in seconds): 0.1403
Attention throughput (in TFLOP/s): 229.553
MLP duration (in seconds): 0.2473
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3877
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30976x46464, b=2048): 0.0913
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30976x46464, b=2048): 258.348
Elapsed time for attention_key_query_prob (256x2048x242x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x242x2048): 97.860
Elapsed time for attention_prob_times_values (256x2048x2048x242): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x242): 142.234
Elapsed time for attention_linear_projection (4x15488x30976, b=2048): 0.0303
Throughput (in TFLOP/s) for attention_linear_projection (4x15488x30976, b=2048): 259.093
Elapsed time for mlp_h_to_4h (4x30976x61952, b=2048): 0.1235
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30976x61952, b=2048): 254.587
Elapsed time for mlp_4h_to_h (4x61952x30976, b=2048): 0.1267
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61952x30976, b=2048): 248.082

Attention duration (in seconds): 0.1306
Attention throughput (in TFLOP/s): 248.745
MLP duration (in seconds): 0.2502
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3808
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31104x46656, b=2048): 0.0923
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31104x46656, b=2048): 257.612
Elapsed time for attention_key_query_prob (256x2048x243x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x243x2048): 69.599
Elapsed time for attention_prob_times_values (256x2048x2048x243): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x243): 89.780
Elapsed time for attention_linear_projection (4x15552x31104, b=2048): 0.0309
Throughput (in TFLOP/s) for attention_linear_projection (4x15552x31104, b=2048): 256.635
Elapsed time for mlp_h_to_4h (4x31104x62208, b=2048): 0.1229
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31104x62208, b=2048): 257.921
Elapsed time for mlp_4h_to_h (4x62208x31104, b=2048): 0.1276
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62208x31104, b=2048): 248.440

Attention duration (in seconds): 0.1365
Attention throughput (in TFLOP/s): 239.915
MLP duration (in seconds): 0.2505
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3870
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31232x46848, b=2048): 0.0929
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31232x46848, b=2048): 257.919
Elapsed time for attention_key_query_prob (256x2048x244x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x244x2048): 101.890
Elapsed time for attention_prob_times_values (256x2048x2048x244): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x244): 141.590
Elapsed time for attention_linear_projection (4x15616x31232, b=2048): 0.0310
Throughput (in TFLOP/s) for attention_linear_projection (4x15616x31232, b=2048): 257.909
Elapsed time for mlp_h_to_4h (4x31232x62464, b=2048): 0.1244
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31232x62464, b=2048): 256.886
Elapsed time for mlp_4h_to_h (4x62464x31232, b=2048): 0.1572
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62464x31232, b=2048): 203.294

Attention duration (in seconds): 0.1328
Attention throughput (in TFLOP/s): 248.631
MLP duration (in seconds): 0.2817
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31360x47040, b=2048): 0.0956
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31360x47040, b=2048): 252.763
Elapsed time for attention_key_query_prob (256x2048x245x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x245x2048): 71.140
Elapsed time for attention_prob_times_values (256x2048x2048x245): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x245): 89.991
Elapsed time for attention_linear_projection (4x15680x31360, b=2048): 0.0313
Throughput (in TFLOP/s) for attention_linear_projection (4x15680x31360, b=2048): 257.154
Elapsed time for mlp_h_to_4h (4x31360x62720, b=2048): 0.1262
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31360x62720, b=2048): 255.360
Elapsed time for mlp_4h_to_h (4x62720x31360, b=2048): 0.1304
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62720x31360, b=2048): 247.060

Attention duration (in seconds): 0.1402
Attention throughput (in TFLOP/s): 237.374
MLP duration (in seconds): 0.2566
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3968
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31488x47232, b=2048): 0.0951
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31488x47232, b=2048): 256.113
Elapsed time for attention_key_query_prob (256x2048x246x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x246x2048): 97.409
Elapsed time for attention_prob_times_values (256x2048x2048x246): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x246): 140.399
Elapsed time for attention_linear_projection (4x15744x31488, b=2048): 0.0313
Throughput (in TFLOP/s) for attention_linear_projection (4x15744x31488, b=2048): 259.364
Elapsed time for mlp_h_to_4h (4x31488x62976, b=2048): 0.1333
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31488x62976, b=2048): 243.769
Elapsed time for mlp_4h_to_h (4x62976x31488, b=2048): 0.1551
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62976x31488, b=2048): 209.425

Attention duration (in seconds): 0.1356
Attention throughput (in TFLOP/s): 247.308
MLP duration (in seconds): 0.2884
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31616x47424, b=2048): 0.0971
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31616x47424, b=2048): 252.977
Elapsed time for attention_key_query_prob (256x2048x247x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x247x2048): 70.344
Elapsed time for attention_prob_times_values (256x2048x2048x247): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x247): 89.919
Elapsed time for attention_linear_projection (4x15808x31616, b=2048): 0.0322
Throughput (in TFLOP/s) for attention_linear_projection (4x15808x31616, b=2048): 254.504
Elapsed time for mlp_h_to_4h (4x31616x63232, b=2048): 0.1288
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31616x63232, b=2048): 254.232
Elapsed time for mlp_4h_to_h (4x63232x31616, b=2048): 0.1319
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63232x31616, b=2048): 248.365

Attention duration (in seconds): 0.1427
Attention throughput (in TFLOP/s): 236.932
MLP duration (in seconds): 0.2607
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31744x47616, b=2048): 0.0973
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31744x47616, b=2048): 254.463
Elapsed time for attention_key_query_prob (256x2048x248x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x248x2048): 129.495
Elapsed time for attention_prob_times_values (256x2048x2048x248): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x248): 213.718
Elapsed time for attention_linear_projection (4x15872x31744, b=2048): 0.0327
Throughput (in TFLOP/s) for attention_linear_projection (4x15872x31744, b=2048): 252.715
Elapsed time for mlp_h_to_4h (4x31744x63488, b=2048): 0.1306
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31744x63488, b=2048): 252.850
Elapsed time for mlp_4h_to_h (4x63488x31744, b=2048): 0.1344
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63488x31744, b=2048): 245.754

Attention duration (in seconds): 0.1366
Attention throughput (in TFLOP/s): 249.539
MLP duration (in seconds): 0.2650
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31872x47808, b=2048): 0.0974
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31872x47808, b=2048): 256.279
Elapsed time for attention_key_query_prob (256x2048x249x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x249x2048): 70.724
Elapsed time for attention_prob_times_values (256x2048x2048x249): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x249): 90.768
Elapsed time for attention_linear_projection (4x15936x31872, b=2048): 0.0321
Throughput (in TFLOP/s) for attention_linear_projection (4x15936x31872, b=2048): 259.189
Elapsed time for mlp_h_to_4h (4x31872x63744, b=2048): 0.1304
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31872x63744, b=2048): 255.173
Elapsed time for mlp_4h_to_h (4x63744x31872, b=2048): 0.1843
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63744x31872, b=2048): 180.610

Attention duration (in seconds): 0.1430
Attention throughput (in TFLOP/s): 240.300
MLP duration (in seconds): 0.3147
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4577
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32000x48000, b=2048): 0.0980
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32000x48000, b=2048): 256.841
Elapsed time for attention_key_query_prob (256x2048x250x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x250x2048): 103.990
Elapsed time for attention_prob_times_values (256x2048x2048x250): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x250): 146.003
Elapsed time for attention_linear_projection (4x16000x32000, b=2048): 0.0326
Throughput (in TFLOP/s) for attention_linear_projection (4x16000x32000, b=2048): 257.476
Elapsed time for mlp_h_to_4h (4x32000x64000, b=2048): 0.1492
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32000x64000, b=2048): 224.960
Elapsed time for mlp_4h_to_h (4x64000x32000, b=2048): 0.1510
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64000x32000, b=2048): 222.167

Attention duration (in seconds): 0.1394
Attention throughput (in TFLOP/s): 248.405
MLP duration (in seconds): 0.3002
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4396
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32128x48192, b=2048): 0.0985
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32128x48192, b=2048): 257.541
Elapsed time for attention_key_query_prob (256x2048x251x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x251x2048): 72.131
Elapsed time for attention_prob_times_values (256x2048x2048x251): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x251): 91.936
Elapsed time for attention_linear_projection (4x16064x32128, b=2048): 0.0327
Throughput (in TFLOP/s) for attention_linear_projection (4x16064x32128, b=2048): 258.453
Elapsed time for mlp_h_to_4h (4x32128x64256, b=2048): 0.1332
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32128x64256, b=2048): 253.998
Elapsed time for mlp_4h_to_h (4x64256x32128, b=2048): 0.1371
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64256x32128, b=2048): 246.692

Attention duration (in seconds): 0.1446
Attention throughput (in TFLOP/s): 241.446
MLP duration (in seconds): 0.2703
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32256x48384, b=2048): 0.0998
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32256x48384, b=2048): 256.302
Elapsed time for attention_key_query_prob (256x2048x252x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x252x2048): 105.829
Elapsed time for attention_prob_times_values (256x2048x2048x252): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x252): 148.714
Elapsed time for attention_linear_projection (4x16128x32256, b=2048): 0.0336
Throughput (in TFLOP/s) for attention_linear_projection (4x16128x32256, b=2048): 253.351
Elapsed time for mlp_h_to_4h (4x32256x64512, b=2048): 0.1332
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32256x64512, b=2048): 255.949
Elapsed time for mlp_4h_to_h (4x64512x32256, b=2048): 0.1374
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64512x32256, b=2048): 248.107

Attention duration (in seconds): 0.1422
Attention throughput (in TFLOP/s): 247.437
MLP duration (in seconds): 0.2706
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32384x48576, b=2048): 0.1011
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32384x48576, b=2048): 254.939
Elapsed time for attention_key_query_prob (256x2048x253x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x253x2048): 73.199
Elapsed time for attention_prob_times_values (256x2048x2048x253): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x253): 94.451
Elapsed time for attention_linear_projection (4x16192x32384, b=2048): 0.0335
Throughput (in TFLOP/s) for attention_linear_projection (4x16192x32384, b=2048): 256.091
Elapsed time for mlp_h_to_4h (4x32384x64768, b=2048): 0.1361
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32384x64768, b=2048): 252.425
Elapsed time for mlp_4h_to_h (4x64768x32384, b=2048): 0.1387
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64768x32384, b=2048): 247.682

Attention duration (in seconds): 0.1478
Attention throughput (in TFLOP/s): 239.829
MLP duration (in seconds): 0.2749
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32512x48768, b=2048): 0.1013
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32512x48768, b=2048): 256.324
Elapsed time for attention_key_query_prob (256x2048x254x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x254x2048): 107.108
Elapsed time for attention_prob_times_values (256x2048x2048x254): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x254): 150.317
Elapsed time for attention_linear_projection (4x16256x32512, b=2048): 0.0340
Throughput (in TFLOP/s) for attention_linear_projection (4x16256x32512, b=2048): 254.992
Elapsed time for mlp_h_to_4h (4x32512x65024, b=2048): 0.1369
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32512x65024, b=2048): 252.962
Elapsed time for mlp_4h_to_h (4x65024x32512, b=2048): 0.1398
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65024x32512, b=2048): 247.704

Attention duration (in seconds): 0.1440
Attention throughput (in TFLOP/s): 248.063
MLP duration (in seconds): 0.2768
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32640x48960, b=2048): 0.1023
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32640x48960, b=2048): 255.909
Elapsed time for attention_key_query_prob (256x2048x255x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x255x2048): 73.241
Elapsed time for attention_prob_times_values (256x2048x2048x255): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x255): 91.275
Elapsed time for attention_linear_projection (4x16320x32640, b=2048): 0.0341
Throughput (in TFLOP/s) for attention_linear_projection (4x16320x32640, b=2048): 256.245
Elapsed time for mlp_h_to_4h (4x32640x65280, b=2048): 0.1390
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32640x65280, b=2048): 251.081
Elapsed time for mlp_4h_to_h (4x65280x32640, b=2048): 0.1407
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65280x32640, b=2048): 248.123

Attention duration (in seconds): 0.1498
Attention throughput (in TFLOP/s): 240.279
MLP duration (in seconds): 0.2797
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4296
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8192x6144, b=2048): 0.0034
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8192x6144, b=2048): 242.874
Elapsed time for attention_key_query_prob (128x2048x64x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x64x2048): 59.687
Elapsed time for attention_prob_times_values (128x2048x2048x64): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x64): 78.838
Elapsed time for attention_linear_projection (4x2048x8192, b=2048): 0.0013
Throughput (in TFLOP/s) for attention_linear_projection (4x2048x8192, b=2048): 216.511
Elapsed time for mlp_h_to_4h (4x8192x8192, b=2048): 0.0043
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8192x8192, b=2048): 254.958
Elapsed time for mlp_4h_to_h (4x8192x8192, b=2048): 0.0042
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8192x8192, b=2048): 261.968

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 184.954
MLP duration (in seconds): 0.0085
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8320x6240, b=2048): 0.0034
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8320x6240, b=2048): 252.580
Elapsed time for attention_key_query_prob (128x2048x65x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x65x2048): 26.763
Elapsed time for attention_prob_times_values (128x2048x2048x65): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x65): 38.645
Elapsed time for attention_linear_projection (4x2080x8320, b=2048): 0.0014
Throughput (in TFLOP/s) for attention_linear_projection (4x2080x8320, b=2048): 202.388
Elapsed time for mlp_h_to_4h (4x8320x8320, b=2048): 0.0045
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8320x8320, b=2048): 250.430
Elapsed time for mlp_4h_to_h (4x8320x8320, b=2048): 0.0045
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8320x8320, b=2048): 252.223

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 138.713
MLP duration (in seconds): 0.0090
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8448x6336, b=2048): 0.0034
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8448x6336, b=2048): 256.973
Elapsed time for attention_key_query_prob (128x2048x66x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x66x2048): 19.642
Elapsed time for attention_prob_times_values (128x2048x2048x66): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x66): 68.080
Elapsed time for attention_linear_projection (4x2112x8448, b=2048): 0.0014
Throughput (in TFLOP/s) for attention_linear_projection (4x2112x8448, b=2048): 215.333
Elapsed time for mlp_h_to_4h (4x8448x8448, b=2048): 0.0047
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8448x8448, b=2048): 246.342
Elapsed time for mlp_4h_to_h (4x8448x8448, b=2048): 0.0046
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8448x8448, b=2048): 254.286

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 139.188
MLP duration (in seconds): 0.0093
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8576x6432, b=2048): 0.0037
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8576x6432, b=2048): 245.905
Elapsed time for attention_key_query_prob (128x2048x67x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x67x2048): 27.739
Elapsed time for attention_prob_times_values (128x2048x2048x67): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x67): 39.976
Elapsed time for attention_linear_projection (4x2144x8576, b=2048): 0.0015
Throughput (in TFLOP/s) for attention_linear_projection (4x2144x8576, b=2048): 203.633
Elapsed time for mlp_h_to_4h (4x8576x8576, b=2048): 0.0047
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8576x8576, b=2048): 259.055
Elapsed time for mlp_4h_to_h (4x8576x8576, b=2048): 0.0047
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8576x8576, b=2048): 258.499

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 141.279
MLP duration (in seconds): 0.0093
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8704x6528, b=2048): 0.0037
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8704x6528, b=2048): 249.178
Elapsed time for attention_key_query_prob (128x2048x68x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x68x2048): 19.102
Elapsed time for attention_prob_times_values (128x2048x2048x68): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x68): 49.299
Elapsed time for attention_linear_projection (4x2176x8704, b=2048): 0.0014
Throughput (in TFLOP/s) for attention_linear_projection (4x2176x8704, b=2048): 214.634
Elapsed time for mlp_h_to_4h (4x8704x8704, b=2048): 0.0049
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8704x8704, b=2048): 254.145
Elapsed time for mlp_4h_to_h (4x8704x8704, b=2048): 0.0049
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8704x8704, b=2048): 251.603

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 132.308
MLP duration (in seconds): 0.0098
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8832x6624, b=2048): 0.0038
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8832x6624, b=2048): 250.299
Elapsed time for attention_key_query_prob (128x2048x69x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x69x2048): 27.029
Elapsed time for attention_prob_times_values (128x2048x2048x69): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x69): 41.202
Elapsed time for attention_linear_projection (4x2208x8832, b=2048): 0.0016
Throughput (in TFLOP/s) for attention_linear_projection (4x2208x8832, b=2048): 205.885
Elapsed time for mlp_h_to_4h (4x8832x8832, b=2048): 0.0050
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8832x8832, b=2048): 257.316
Elapsed time for mlp_4h_to_h (4x8832x8832, b=2048): 0.0050
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8832x8832, b=2048): 255.989

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 143.761
MLP duration (in seconds): 0.0100
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8960x6720, b=2048): 0.0039
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8960x6720, b=2048): 253.814
Elapsed time for attention_key_query_prob (128x2048x70x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x70x2048): 20.144
Elapsed time for attention_prob_times_values (128x2048x2048x70): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x70): 67.549
Elapsed time for attention_linear_projection (4x2240x8960, b=2048): 0.0015
Throughput (in TFLOP/s) for attention_linear_projection (4x2240x8960, b=2048): 216.519
Elapsed time for mlp_h_to_4h (4x8960x8960, b=2048): 0.0050
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8960x8960, b=2048): 261.254
Elapsed time for mlp_4h_to_h (4x8960x8960, b=2048): 0.0051
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8960x8960, b=2048): 258.174

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 143.000
MLP duration (in seconds): 0.0101
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9088x6816, b=2048): 0.0039
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9088x6816, b=2048): 256.957
Elapsed time for attention_key_query_prob (128x2048x71x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x71x2048): 27.764
Elapsed time for attention_prob_times_values (128x2048x2048x71): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x71): 41.897
Elapsed time for attention_linear_projection (4x2272x9088, b=2048): 0.0016
Throughput (in TFLOP/s) for attention_linear_projection (4x2272x9088, b=2048): 208.022
Elapsed time for mlp_h_to_4h (4x9088x9088, b=2048): 0.0053
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9088x9088, b=2048): 253.457
Elapsed time for mlp_4h_to_h (4x9088x9088, b=2048): 0.0053
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9088x9088, b=2048): 253.672

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 148.467
MLP duration (in seconds): 0.0107
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9216x6912, b=2048): 0.0040
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9216x6912, b=2048): 260.240
Elapsed time for attention_key_query_prob (128x2048x72x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x72x2048): 66.665
Elapsed time for attention_prob_times_values (128x2048x2048x72): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x72): 82.341
Elapsed time for attention_linear_projection (4x2304x9216, b=2048): 0.0016
Throughput (in TFLOP/s) for attention_linear_projection (4x2304x9216, b=2048): 218.144
Elapsed time for mlp_h_to_4h (4x9216x9216, b=2048): 0.0054
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9216x9216, b=2048): 257.178
Elapsed time for mlp_4h_to_h (4x9216x9216, b=2048): 0.0054
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9216x9216, b=2048): 256.275

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 200.705
MLP duration (in seconds): 0.0108
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9344x7008, b=2048): 0.0043
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9344x7008, b=2048): 252.423
Elapsed time for attention_key_query_prob (128x2048x73x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x73x2048): 27.868
Elapsed time for attention_prob_times_values (128x2048x2048x73): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x73): 42.708
Elapsed time for attention_linear_projection (4x2336x9344, b=2048): 0.0017
Throughput (in TFLOP/s) for attention_linear_projection (4x2336x9344, b=2048): 209.086
Elapsed time for mlp_h_to_4h (4x9344x9344, b=2048): 0.0055
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9344x9344, b=2048): 260.413
Elapsed time for mlp_4h_to_h (4x9344x9344, b=2048): 0.0055
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9344x9344, b=2048): 259.883

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 149.619
MLP duration (in seconds): 0.0110
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9472x7104, b=2048): 0.0046
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9472x7104, b=2048): 239.751
Elapsed time for attention_key_query_prob (128x2048x74x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x74x2048): 50.541
Elapsed time for attention_prob_times_values (128x2048x2048x74): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x74): 72.925
Elapsed time for attention_linear_projection (4x2368x9472, b=2048): 0.0017
Throughput (in TFLOP/s) for attention_linear_projection (4x2368x9472, b=2048): 221.459
Elapsed time for mlp_h_to_4h (4x9472x9472, b=2048): 0.0060
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9472x9472, b=2048): 245.068
Elapsed time for mlp_4h_to_h (4x9472x9472, b=2048): 0.0060
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9472x9472, b=2048): 245.879

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 182.619
MLP duration (in seconds): 0.0120
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9600x7200, b=2048): 0.0047
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9600x7200, b=2048): 240.928
Elapsed time for attention_key_query_prob (128x2048x75x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x75x2048): 29.451
Elapsed time for attention_prob_times_values (128x2048x2048x75): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x75): 44.257
Elapsed time for attention_linear_projection (4x2400x9600, b=2048): 0.0018
Throughput (in TFLOP/s) for attention_linear_projection (4x2400x9600, b=2048): 210.433
Elapsed time for mlp_h_to_4h (4x9600x9600, b=2048): 0.0064
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9600x9600, b=2048): 237.661
Elapsed time for mlp_4h_to_h (4x9600x9600, b=2048): 0.0064
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9600x9600, b=2048): 234.710

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 151.246
MLP duration (in seconds): 0.0128
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9728x7296, b=2048): 0.0047
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9728x7296, b=2048): 246.720
Elapsed time for attention_key_query_prob (128x2048x76x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x76x2048): 51.208
Elapsed time for attention_prob_times_values (128x2048x2048x76): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x76): 73.167
Elapsed time for attention_linear_projection (4x2432x9728, b=2048): 0.0017
Throughput (in TFLOP/s) for attention_linear_projection (4x2432x9728, b=2048): 221.801
Elapsed time for mlp_h_to_4h (4x9728x9728, b=2048): 0.0065
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9728x9728, b=2048): 239.846
Elapsed time for mlp_4h_to_h (4x9728x9728, b=2048): 0.0066
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9728x9728, b=2048): 234.105

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 186.884
MLP duration (in seconds): 0.0131
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9856x7392, b=2048): 0.0049
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9856x7392, b=2048): 242.040
Elapsed time for attention_key_query_prob (128x2048x77x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x77x2048): 29.835
Elapsed time for attention_prob_times_values (128x2048x2048x77): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x77): 45.569
Elapsed time for attention_linear_projection (4x2464x9856, b=2048): 0.0019
Throughput (in TFLOP/s) for attention_linear_projection (4x2464x9856, b=2048): 204.118
Elapsed time for mlp_h_to_4h (4x9856x9856, b=2048): 0.0066
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9856x9856, b=2048): 242.022
Elapsed time for mlp_4h_to_h (4x9856x9856, b=2048): 0.0067
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9856x9856, b=2048): 239.152

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 153.221
MLP duration (in seconds): 0.0132
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9984x7488, b=2048): 0.0050
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9984x7488, b=2048): 245.554
Elapsed time for attention_key_query_prob (128x2048x78x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x78x2048): 52.181
Elapsed time for attention_prob_times_values (128x2048x2048x78): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x78): 76.599
Elapsed time for attention_linear_projection (4x2496x9984, b=2048): 0.0019
Throughput (in TFLOP/s) for attention_linear_projection (4x2496x9984, b=2048): 215.029
Elapsed time for mlp_h_to_4h (4x9984x9984, b=2048): 0.0069
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9984x9984, b=2048): 238.003
Elapsed time for mlp_4h_to_h (4x9984x9984, b=2048): 0.0067
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9984x9984, b=2048): 243.988

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 187.855
MLP duration (in seconds): 0.0136
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10112x7584, b=2048): 0.0052
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10112x7584, b=2048): 240.664
Elapsed time for attention_key_query_prob (128x2048x79x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x79x2048): 29.915
Elapsed time for attention_prob_times_values (128x2048x2048x79): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x79): 46.344
Elapsed time for attention_linear_projection (4x2528x10112, b=2048): 0.0020
Throughput (in TFLOP/s) for attention_linear_projection (4x2528x10112, b=2048): 211.572
Elapsed time for mlp_h_to_4h (4x10112x10112, b=2048): 0.0074
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10112x10112, b=2048): 226.654
Elapsed time for mlp_4h_to_h (4x10112x10112, b=2048): 0.0069
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10112x10112, b=2048): 241.204

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 155.478
MLP duration (in seconds): 0.0143
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10240x7680, b=2048): 0.0053
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10240x7680, b=2048): 244.263
Elapsed time for attention_key_query_prob (128x2048x80x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x80x2048): 78.001
Elapsed time for attention_prob_times_values (128x2048x2048x80): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x80): 91.537
Elapsed time for attention_linear_projection (4x2560x10240, b=2048): 0.0020
Throughput (in TFLOP/s) for attention_linear_projection (4x2560x10240, b=2048): 216.780
Elapsed time for mlp_h_to_4h (4x10240x10240, b=2048): 0.0071
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10240x10240, b=2048): 242.031
Elapsed time for mlp_4h_to_h (4x10240x10240, b=2048): 0.0072
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10240x10240, b=2048): 238.949

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 203.291
MLP duration (in seconds): 0.0143
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10368x7776, b=2048): 0.0054
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10368x7776, b=2048): 243.496
Elapsed time for attention_key_query_prob (128x2048x81x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x81x2048): 30.093
Elapsed time for attention_prob_times_values (128x2048x2048x81): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x81): 47.094
Elapsed time for attention_linear_projection (4x2592x10368, b=2048): 0.0021
Throughput (in TFLOP/s) for attention_linear_projection (4x2592x10368, b=2048): 209.763
Elapsed time for mlp_h_to_4h (4x10368x10368, b=2048): 0.0073
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10368x10368, b=2048): 240.369
Elapsed time for mlp_4h_to_h (4x10368x10368, b=2048): 0.0073
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10368x10368, b=2048): 240.213

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 157.834
MLP duration (in seconds): 0.0147
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10496x7872, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10496x7872, b=2048): 245.776
Elapsed time for attention_key_query_prob (128x2048x82x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x82x2048): 53.669
Elapsed time for attention_prob_times_values (128x2048x2048x82): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x82): 80.615
Elapsed time for attention_linear_projection (4x2624x10496, b=2048): 0.0021
Throughput (in TFLOP/s) for attention_linear_projection (4x2624x10496, b=2048): 217.720
Elapsed time for mlp_h_to_4h (4x10496x10496, b=2048): 0.0074
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10496x10496, b=2048): 242.358
Elapsed time for mlp_4h_to_h (4x10496x10496, b=2048): 0.0075
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10496x10496, b=2048): 240.503

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 192.088
MLP duration (in seconds): 0.0150
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10624x7968, b=2048): 0.0062
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10624x7968, b=2048): 222.083
Elapsed time for attention_key_query_prob (128x2048x83x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x83x2048): 31.057
Elapsed time for attention_prob_times_values (128x2048x2048x83): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x83): 48.482
Elapsed time for attention_linear_projection (4x2656x10624, b=2048): 0.0022
Throughput (in TFLOP/s) for attention_linear_projection (4x2656x10624, b=2048): 209.744
Elapsed time for mlp_h_to_4h (4x10624x10624, b=2048): 0.0076
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10624x10624, b=2048): 241.826
Elapsed time for mlp_4h_to_h (4x10624x10624, b=2048): 0.0076
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10624x10624, b=2048): 242.113

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 154.098
MLP duration (in seconds): 0.0153
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10752x8064, b=2048): 0.0058
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10752x8064, b=2048): 246.556
Elapsed time for attention_key_query_prob (128x2048x84x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x84x2048): 56.429
Elapsed time for attention_prob_times_values (128x2048x2048x84): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x84): 82.383
Elapsed time for attention_linear_projection (4x2688x10752, b=2048): 0.0022
Throughput (in TFLOP/s) for attention_linear_projection (4x2688x10752, b=2048): 220.138
Elapsed time for mlp_h_to_4h (4x10752x10752, b=2048): 0.0079
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10752x10752, b=2048): 238.254
Elapsed time for mlp_4h_to_h (4x10752x10752, b=2048): 0.0080
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10752x10752, b=2048): 235.444

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 195.597
MLP duration (in seconds): 0.0160
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10880x8160, b=2048): 0.0059
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10880x8160, b=2048): 247.003
Elapsed time for attention_key_query_prob (128x2048x85x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x85x2048): 31.559
Elapsed time for attention_prob_times_values (128x2048x2048x85): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x85): 49.625
Elapsed time for attention_linear_projection (4x2720x10880, b=2048): 0.0023
Throughput (in TFLOP/s) for attention_linear_projection (4x2720x10880, b=2048): 212.481
Elapsed time for mlp_h_to_4h (4x10880x10880, b=2048): 0.0081
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10880x10880, b=2048): 240.264
Elapsed time for mlp_4h_to_h (4x10880x10880, b=2048): 0.0082
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10880x10880, b=2048): 237.445

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 164.469
MLP duration (in seconds): 0.0162
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0291
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11008x8256, b=2048): 0.0062
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11008x8256, b=2048): 240.641
Elapsed time for attention_key_query_prob (128x2048x86x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x86x2048): 57.877
Elapsed time for attention_prob_times_values (128x2048x2048x86): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x86): 84.510
Elapsed time for attention_linear_projection (4x2752x11008, b=2048): 0.0023
Throughput (in TFLOP/s) for attention_linear_projection (4x2752x11008, b=2048): 220.528
Elapsed time for mlp_h_to_4h (4x11008x11008, b=2048): 0.0084
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11008x11008, b=2048): 235.164
Elapsed time for mlp_4h_to_h (4x11008x11008, b=2048): 0.0084
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11008x11008, b=2048): 236.211

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 195.032
MLP duration (in seconds): 0.0168
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0280
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11136x8352, b=2048): 0.0065
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11136x8352, b=2048): 235.621
Elapsed time for attention_key_query_prob (128x2048x87x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x87x2048): 31.333
Elapsed time for attention_prob_times_values (128x2048x2048x87): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x87): 50.720
Elapsed time for attention_linear_projection (4x2784x11136, b=2048): 0.0023
Throughput (in TFLOP/s) for attention_linear_projection (4x2784x11136, b=2048): 217.574
Elapsed time for mlp_h_to_4h (4x11136x11136, b=2048): 0.0128
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11136x11136, b=2048): 158.888
Elapsed time for mlp_4h_to_h (4x11136x11136, b=2048): 0.0086
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11136x11136, b=2048): 235.954

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 162.833
MLP duration (in seconds): 0.0214
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0350
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11264x8448, b=2048): 0.0066
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11264x8448, b=2048): 236.321
Elapsed time for attention_key_query_prob (128x2048x88x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x88x2048): 76.347
Elapsed time for attention_prob_times_values (128x2048x2048x88): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x88): 85.395
Elapsed time for attention_linear_projection (4x2816x11264, b=2048): 0.0023
Throughput (in TFLOP/s) for attention_linear_projection (4x2816x11264, b=2048): 222.287
Elapsed time for mlp_h_to_4h (4x11264x11264, b=2048): 0.0088
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11264x11264, b=2048): 237.555
Elapsed time for mlp_4h_to_h (4x11264x11264, b=2048): 0.0087
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11264x11264, b=2048): 237.600

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 201.053
MLP duration (in seconds): 0.0175
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0288
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11392x8544, b=2048): 0.0068
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11392x8544, b=2048): 234.584
Elapsed time for attention_key_query_prob (128x2048x89x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x89x2048): 32.254
Elapsed time for attention_prob_times_values (128x2048x2048x89): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x89): 49.048
Elapsed time for attention_linear_projection (4x2848x11392, b=2048): 0.0025
Throughput (in TFLOP/s) for attention_linear_projection (4x2848x11392, b=2048): 212.036
Elapsed time for mlp_h_to_4h (4x11392x11392, b=2048): 0.0091
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11392x11392, b=2048): 233.487
Elapsed time for mlp_4h_to_h (4x11392x11392, b=2048): 0.0091
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11392x11392, b=2048): 232.919

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 163.012
MLP duration (in seconds): 0.0182
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0325
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11520x8640, b=2048): 0.0069
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11520x8640, b=2048): 235.451
Elapsed time for attention_key_query_prob (128x2048x90x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x90x2048): 59.319
Elapsed time for attention_prob_times_values (128x2048x2048x90): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x90): 87.885
Elapsed time for attention_linear_projection (4x2880x11520, b=2048): 0.0025
Throughput (in TFLOP/s) for attention_linear_projection (4x2880x11520, b=2048): 219.394
Elapsed time for mlp_h_to_4h (4x11520x11520, b=2048): 0.0091
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11520x11520, b=2048): 238.438
Elapsed time for mlp_4h_to_h (4x11520x11520, b=2048): 0.0092
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11520x11520, b=2048): 235.501

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 195.147
MLP duration (in seconds): 0.0184
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11648x8736, b=2048): 0.0071
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11648x8736, b=2048): 233.736
Elapsed time for attention_key_query_prob (128x2048x91x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x91x2048): 32.776
Elapsed time for attention_prob_times_values (128x2048x2048x91): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x91): 51.254
Elapsed time for attention_linear_projection (4x2912x11648, b=2048): 0.0026
Throughput (in TFLOP/s) for attention_linear_projection (4x2912x11648, b=2048): 215.784
Elapsed time for mlp_h_to_4h (4x11648x11648, b=2048): 0.0095
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11648x11648, b=2048): 234.308
Elapsed time for mlp_4h_to_h (4x11648x11648, b=2048): 0.0096
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11648x11648, b=2048): 232.080

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 165.688
MLP duration (in seconds): 0.0191
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0337
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11776x8832, b=2048): 0.0072
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11776x8832, b=2048): 236.663
Elapsed time for attention_key_query_prob (128x2048x92x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x92x2048): 61.556
Elapsed time for attention_prob_times_values (128x2048x2048x92): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x92): 89.469
Elapsed time for attention_linear_projection (4x2944x11776, b=2048): 0.0026
Throughput (in TFLOP/s) for attention_linear_projection (4x2944x11776, b=2048): 220.267
Elapsed time for mlp_h_to_4h (4x11776x11776, b=2048): 0.0095
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11776x11776, b=2048): 239.149
Elapsed time for mlp_4h_to_h (4x11776x11776, b=2048): 0.0095
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11776x11776, b=2048): 239.287

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 197.760
MLP duration (in seconds): 0.0190
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11904x8928, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11904x8928, b=2048): 234.317
Elapsed time for attention_key_query_prob (128x2048x93x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x93x2048): 32.450
Elapsed time for attention_prob_times_values (128x2048x2048x93): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x93): 54.338
Elapsed time for attention_linear_projection (4x2976x11904, b=2048): 0.0027
Throughput (in TFLOP/s) for attention_linear_projection (4x2976x11904, b=2048): 217.811
Elapsed time for mlp_h_to_4h (4x11904x11904, b=2048): 0.0098
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11904x11904, b=2048): 236.008
Elapsed time for mlp_4h_to_h (4x11904x11904, b=2048): 0.0100
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11904x11904, b=2048): 233.059

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 167.970
MLP duration (in seconds): 0.0198
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0348
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12032x9024, b=2048): 0.0079
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12032x9024, b=2048): 225.752
Elapsed time for attention_key_query_prob (128x2048x94x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x94x2048): 62.828
Elapsed time for attention_prob_times_values (128x2048x2048x94): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x94): 67.995
Elapsed time for attention_linear_projection (4x3008x12032, b=2048): 0.0027
Throughput (in TFLOP/s) for attention_linear_projection (4x3008x12032, b=2048): 223.320
Elapsed time for mlp_h_to_4h (4x12032x12032, b=2048): 0.0098
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12032x12032, b=2048): 241.321
Elapsed time for mlp_4h_to_h (4x12032x12032, b=2048): 0.0098
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12032x12032, b=2048): 241.210

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 188.885
MLP duration (in seconds): 0.0197
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0333
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12160x9120, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12160x9120, b=2048): 244.527
Elapsed time for attention_key_query_prob (128x2048x95x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x95x2048): 34.835
Elapsed time for attention_prob_times_values (128x2048x2048x95): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x95): 54.978
Elapsed time for attention_linear_projection (4x3040x12160, b=2048): 0.0028
Throughput (in TFLOP/s) for attention_linear_projection (4x3040x12160, b=2048): 219.181
Elapsed time for mlp_h_to_4h (4x12160x12160, b=2048): 0.0103
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12160x12160, b=2048): 235.498
Elapsed time for mlp_4h_to_h (4x12160x12160, b=2048): 0.0102
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12160x12160, b=2048): 238.421

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 175.373
MLP duration (in seconds): 0.0204
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0354
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12288x9216, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12288x9216, b=2048): 250.829
Elapsed time for attention_key_query_prob (128x2048x96x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x96x2048): 92.938
Elapsed time for attention_prob_times_values (128x2048x2048x96): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x96): 85.545
Elapsed time for attention_linear_projection (4x3072x12288, b=2048): 0.0028
Throughput (in TFLOP/s) for attention_linear_projection (4x3072x12288, b=2048): 222.839
Elapsed time for mlp_h_to_4h (4x12288x12288, b=2048): 0.0100
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12288x12288, b=2048): 247.367
Elapsed time for mlp_4h_to_h (4x12288x12288, b=2048): 0.0100
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12288x12288, b=2048): 247.320

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 214.633
MLP duration (in seconds): 0.0200
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0325
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12416x9312, b=2048): 0.0079
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12416x9312, b=2048): 240.479
Elapsed time for attention_key_query_prob (128x2048x97x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x97x2048): 31.519
Elapsed time for attention_prob_times_values (128x2048x2048x97): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x97): 55.835
Elapsed time for attention_linear_projection (4x3104x12416, b=2048): 0.0029
Throughput (in TFLOP/s) for attention_linear_projection (4x3104x12416, b=2048): 215.264
Elapsed time for mlp_h_to_4h (4x12416x12416, b=2048): 0.0104
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12416x12416, b=2048): 243.564
Elapsed time for mlp_4h_to_h (4x12416x12416, b=2048): 0.0105
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12416x12416, b=2048): 240.692

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 171.087
MLP duration (in seconds): 0.0209
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0368
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12544x9408, b=2048): 0.0079
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12544x9408, b=2048): 243.437
Elapsed time for attention_key_query_prob (128x2048x98x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x98x2048): 58.057
Elapsed time for attention_prob_times_values (128x2048x2048x98): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x98): 94.609
Elapsed time for attention_linear_projection (4x3136x12544, b=2048): 0.0029
Throughput (in TFLOP/s) for attention_linear_projection (4x3136x12544, b=2048): 224.693
Elapsed time for mlp_h_to_4h (4x12544x12544, b=2048): 0.0107
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12544x12544, b=2048): 240.945
Elapsed time for mlp_4h_to_h (4x12544x12544, b=2048): 0.0110
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12544x12544, b=2048): 235.017

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 203.011
MLP duration (in seconds): 0.0217
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0354
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12672x9504, b=2048): 0.0090
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12672x9504, b=2048): 219.995
Elapsed time for attention_key_query_prob (128x2048x99x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x99x2048): 31.177
Elapsed time for attention_prob_times_values (128x2048x2048x99): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x99): 57.508
Elapsed time for attention_linear_projection (4x3168x12672, b=2048): 0.0029
Throughput (in TFLOP/s) for attention_linear_projection (4x3168x12672, b=2048): 223.887
Elapsed time for mlp_h_to_4h (4x12672x12672, b=2048): 0.0107
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12672x12672, b=2048): 246.922
Elapsed time for mlp_4h_to_h (4x12672x12672, b=2048): 0.0106
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12672x12672, b=2048): 247.260

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 165.657
MLP duration (in seconds): 0.0213
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0385
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12800x9600, b=2048): 0.0087
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12800x9600, b=2048): 231.629
Elapsed time for attention_key_query_prob (128x2048x100x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x100x2048): 59.110
Elapsed time for attention_prob_times_values (128x2048x2048x100): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x100): 96.977
Elapsed time for attention_linear_projection (4x3200x12800, b=2048): 0.0030
Throughput (in TFLOP/s) for attention_linear_projection (4x3200x12800, b=2048): 224.498
Elapsed time for mlp_h_to_4h (4x12800x12800, b=2048): 0.0109
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12800x12800, b=2048): 246.206
Elapsed time for mlp_4h_to_h (4x12800x12800, b=2048): 0.0108
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12800x12800, b=2048): 249.143

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 198.503
MLP duration (in seconds): 0.0217
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0363
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12928x9696, b=2048): 0.0086
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12928x9696, b=2048): 239.623
Elapsed time for attention_key_query_prob (128x2048x101x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x101x2048): 31.170
Elapsed time for attention_prob_times_values (128x2048x2048x101): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x101): 58.541
Elapsed time for attention_linear_projection (4x3232x12928, b=2048): 0.0032
Throughput (in TFLOP/s) for attention_linear_projection (4x3232x12928, b=2048): 213.815
Elapsed time for mlp_h_to_4h (4x12928x12928, b=2048): 0.0115
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12928x12928, b=2048): 238.829
Elapsed time for mlp_4h_to_h (4x12928x12928, b=2048): 0.0118
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12928x12928, b=2048): 232.982

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 172.777
MLP duration (in seconds): 0.0232
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0403
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x9792, b=2048): 0.0086
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x9792, b=2048): 242.530
Elapsed time for attention_key_query_prob (128x2048x102x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x102x2048): 61.021
Elapsed time for attention_prob_times_values (128x2048x2048x102): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x102): 99.366
Elapsed time for attention_linear_projection (4x3264x13056, b=2048): 0.0031
Throughput (in TFLOP/s) for attention_linear_projection (4x3264x13056, b=2048): 224.644
Elapsed time for mlp_h_to_4h (4x13056x13056, b=2048): 0.0113
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x13056, b=2048): 246.654
Elapsed time for mlp_4h_to_h (4x13056x13056, b=2048): 0.0115
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13056x13056, b=2048): 243.612

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 205.706
MLP duration (in seconds): 0.0228
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0374
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13184x9888, b=2048): 0.0094
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13184x9888, b=2048): 226.465
Elapsed time for attention_key_query_prob (128x2048x103x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x103x2048): 30.879
Elapsed time for attention_prob_times_values (128x2048x2048x103): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x103): 59.024
Elapsed time for attention_linear_projection (4x3296x13184, b=2048): 0.0032
Throughput (in TFLOP/s) for attention_linear_projection (4x3296x13184, b=2048): 223.298
Elapsed time for mlp_h_to_4h (4x13184x13184, b=2048): 0.0116
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13184x13184, b=2048): 245.846
Elapsed time for mlp_4h_to_h (4x13184x13184, b=2048): 0.0117
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13184x13184, b=2048): 243.089

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 169.794
MLP duration (in seconds): 0.0233
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0414
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13312x9984, b=2048): 0.0093
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13312x9984, b=2048): 234.989
Elapsed time for attention_key_query_prob (128x2048x104x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x104x2048): 84.134
Elapsed time for attention_prob_times_values (128x2048x2048x104): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x104): 115.108
Elapsed time for attention_linear_projection (4x3328x13312, b=2048): 0.0032
Throughput (in TFLOP/s) for attention_linear_projection (4x3328x13312, b=2048): 228.527
Elapsed time for mlp_h_to_4h (4x13312x13312, b=2048): 0.0118
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13312x13312, b=2048): 245.440
Elapsed time for mlp_4h_to_h (4x13312x13312, b=2048): 0.0120
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13312x13312, b=2048): 241.732

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 212.123
MLP duration (in seconds): 0.0238
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0386
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13440x10080, b=2048): 0.0090
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13440x10080, b=2048): 245.472
Elapsed time for attention_key_query_prob (128x2048x105x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x105x2048): 30.526
Elapsed time for attention_prob_times_values (128x2048x2048x105): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x105): 59.866
Elapsed time for attention_linear_projection (4x3360x13440, b=2048): 0.0033
Throughput (in TFLOP/s) for attention_linear_projection (4x3360x13440, b=2048): 224.597
Elapsed time for mlp_h_to_4h (4x13440x13440, b=2048): 0.0120
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13440x13440, b=2048): 246.168
Elapsed time for mlp_4h_to_h (4x13440x13440, b=2048): 0.0120
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13440x13440, b=2048): 246.193

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 177.802
MLP duration (in seconds): 0.0240
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0420
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13568x10176, b=2048): 0.0094
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13568x10176, b=2048): 241.731
Elapsed time for attention_key_query_prob (128x2048x106x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x106x2048): 63.541
Elapsed time for attention_prob_times_values (128x2048x2048x106): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x106): 102.398
Elapsed time for attention_linear_projection (4x3392x13568, b=2048): 0.0033
Throughput (in TFLOP/s) for attention_linear_projection (4x3392x13568, b=2048): 229.261
Elapsed time for mlp_h_to_4h (4x13568x13568, b=2048): 0.0123
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13568x13568, b=2048): 245.248
Elapsed time for mlp_4h_to_h (4x13568x13568, b=2048): 0.0127
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13568x13568, b=2048): 237.329

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 208.607
MLP duration (in seconds): 0.0250
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0406
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13696x10272, b=2048): 0.0096
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13696x10272, b=2048): 240.308
Elapsed time for attention_key_query_prob (128x2048x107x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x107x2048): 31.618
Elapsed time for attention_prob_times_values (128x2048x2048x107): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x107): 61.820
Elapsed time for attention_linear_projection (4x3424x13696, b=2048): 0.0034
Throughput (in TFLOP/s) for attention_linear_projection (4x3424x13696, b=2048): 225.704
Elapsed time for mlp_h_to_4h (4x13696x13696, b=2048): 0.0127
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13696x13696, b=2048): 241.290
Elapsed time for mlp_4h_to_h (4x13696x13696, b=2048): 0.0129
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13696x13696, b=2048): 238.231

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 178.660
MLP duration (in seconds): 0.0256
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0441
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13824x10368, b=2048): 0.0098
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13824x10368, b=2048): 240.451
Elapsed time for attention_key_query_prob (128x2048x108x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x108x2048): 63.906
Elapsed time for attention_prob_times_values (128x2048x2048x108): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x108): 104.577
Elapsed time for attention_linear_projection (4x3456x13824, b=2048): 0.0034
Throughput (in TFLOP/s) for attention_linear_projection (4x3456x13824, b=2048): 230.023
Elapsed time for mlp_h_to_4h (4x13824x13824, b=2048): 0.0131
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13824x13824, b=2048): 239.718
Elapsed time for mlp_4h_to_h (4x13824x13824, b=2048): 0.0129
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13824x13824, b=2048): 242.942

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 208.976
MLP duration (in seconds): 0.0259
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0420
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13952x10464, b=2048): 0.0095
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13952x10464, b=2048): 251.092
Elapsed time for attention_key_query_prob (128x2048x109x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x109x2048): 32.396
Elapsed time for attention_prob_times_values (128x2048x2048x109): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x109): 62.943
Elapsed time for attention_linear_projection (4x3488x13952, b=2048): 0.0036
Throughput (in TFLOP/s) for attention_linear_projection (4x3488x13952, b=2048): 222.280
Elapsed time for mlp_h_to_4h (4x13952x13952, b=2048): 0.0129
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13952x13952, b=2048): 247.879
Elapsed time for mlp_4h_to_h (4x13952x13952, b=2048): 0.0130
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13952x13952, b=2048): 244.835

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 184.196
MLP duration (in seconds): 0.0259
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0445
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14080x10560, b=2048): 0.0098
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14080x10560, b=2048): 247.608
Elapsed time for attention_key_query_prob (128x2048x110x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x110x2048): 65.030
Elapsed time for attention_prob_times_values (128x2048x2048x110): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x110): 106.012
Elapsed time for attention_linear_projection (4x3520x14080, b=2048): 0.0035
Throughput (in TFLOP/s) for attention_linear_projection (4x3520x14080, b=2048): 229.474
Elapsed time for mlp_h_to_4h (4x14080x14080, b=2048): 0.0133
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14080x14080, b=2048): 243.889
Elapsed time for mlp_4h_to_h (4x14080x14080, b=2048): 0.0133
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14080x14080, b=2048): 245.047

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 213.664
MLP duration (in seconds): 0.0266
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0429
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14208x10656, b=2048): 0.0102
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14208x10656, b=2048): 242.974
Elapsed time for attention_key_query_prob (128x2048x111x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x111x2048): 32.622
Elapsed time for attention_prob_times_values (128x2048x2048x111): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x111): 62.991
Elapsed time for attention_linear_projection (4x3552x14208, b=2048): 0.0036
Throughput (in TFLOP/s) for attention_linear_projection (4x3552x14208, b=2048): 228.056
Elapsed time for mlp_h_to_4h (4x14208x14208, b=2048): 0.0136
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14208x14208, b=2048): 243.150
Elapsed time for mlp_4h_to_h (4x14208x14208, b=2048): 0.0135
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14208x14208, b=2048): 245.717

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 182.957
MLP duration (in seconds): 0.0271
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0464
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14336x10752, b=2048): 0.0103
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14336x10752, b=2048): 245.310
Elapsed time for attention_key_query_prob (128x2048x112x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x112x2048): 94.193
Elapsed time for attention_prob_times_values (128x2048x2048x112): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x112): 123.568
Elapsed time for attention_linear_projection (4x3584x14336, b=2048): 0.0037
Throughput (in TFLOP/s) for attention_linear_projection (4x3584x14336, b=2048): 230.592
Elapsed time for mlp_h_to_4h (4x14336x14336, b=2048): 0.0136
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14336x14336, b=2048): 247.339
Elapsed time for mlp_4h_to_h (4x14336x14336, b=2048): 0.0136
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14336x14336, b=2048): 247.699

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 222.763
MLP duration (in seconds): 0.0272
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0434
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14464x10848, b=2048): 0.0103
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14464x10848, b=2048): 250.504
Elapsed time for attention_key_query_prob (128x2048x113x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x113x2048): 33.358
Elapsed time for attention_prob_times_values (128x2048x2048x113): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x113): 63.885
Elapsed time for attention_linear_projection (4x3616x14464, b=2048): 0.0038
Throughput (in TFLOP/s) for attention_linear_projection (4x3616x14464, b=2048): 225.043
Elapsed time for mlp_h_to_4h (4x14464x14464, b=2048): 0.0139
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14464x14464, b=2048): 246.796
Elapsed time for mlp_4h_to_h (4x14464x14464, b=2048): 0.0137
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14464x14464, b=2048): 249.287

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 187.198
MLP duration (in seconds): 0.0276
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0472
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14592x10944, b=2048): 0.0106
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14592x10944, b=2048): 246.582
Elapsed time for attention_key_query_prob (128x2048x114x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x114x2048): 67.165
Elapsed time for attention_prob_times_values (128x2048x2048x114): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x114): 109.446
Elapsed time for attention_linear_projection (4x3648x14592, b=2048): 0.0038
Throughput (in TFLOP/s) for attention_linear_projection (4x3648x14592, b=2048): 232.257
Elapsed time for mlp_h_to_4h (4x14592x14592, b=2048): 0.0146
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14592x14592, b=2048): 239.530
Elapsed time for mlp_4h_to_h (4x14592x14592, b=2048): 0.0144
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14592x14592, b=2048): 242.283

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 215.719
MLP duration (in seconds): 0.0290
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0463
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14720x11040, b=2048): 0.0111
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14720x11040, b=2048): 239.436
Elapsed time for attention_key_query_prob (128x2048x115x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x115x2048): 34.087
Elapsed time for attention_prob_times_values (128x2048x2048x115): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x115): 66.187
Elapsed time for attention_linear_projection (4x3680x14720, b=2048): 0.0039
Throughput (in TFLOP/s) for attention_linear_projection (4x3680x14720, b=2048): 228.951
Elapsed time for mlp_h_to_4h (4x14720x14720, b=2048): 0.0145
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14720x14720, b=2048): 244.576
Elapsed time for mlp_4h_to_h (4x14720x14720, b=2048): 0.0144
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14720x14720, b=2048): 247.170

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 185.359
MLP duration (in seconds): 0.0289
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0494
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14848x11136, b=2048): 0.0112
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14848x11136, b=2048): 241.495
Elapsed time for attention_key_query_prob (128x2048x116x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x116x2048): 68.857
Elapsed time for attention_prob_times_values (128x2048x2048x116): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x116): 111.509
Elapsed time for attention_linear_projection (4x3712x14848, b=2048): 0.0039
Throughput (in TFLOP/s) for attention_linear_projection (4x3712x14848, b=2048): 232.065
Elapsed time for mlp_h_to_4h (4x14848x14848, b=2048): 0.0147
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14848x14848, b=2048): 246.123
Elapsed time for mlp_4h_to_h (4x14848x14848, b=2048): 0.0147
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14848x14848, b=2048): 246.283

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 214.094
MLP duration (in seconds): 0.0293
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0474
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14976x11232, b=2048): 0.0111
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14976x11232, b=2048): 247.354
Elapsed time for attention_key_query_prob (128x2048x117x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x117x2048): 35.079
Elapsed time for attention_prob_times_values (128x2048x2048x117): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x117): 67.030
Elapsed time for attention_linear_projection (4x3744x14976, b=2048): 0.0041
Throughput (in TFLOP/s) for attention_linear_projection (4x3744x14976, b=2048): 225.658
Elapsed time for mlp_h_to_4h (4x14976x14976, b=2048): 0.0150
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14976x14976, b=2048): 244.809
Elapsed time for mlp_4h_to_h (4x14976x14976, b=2048): 0.0152
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14976x14976, b=2048): 241.889

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 189.947
MLP duration (in seconds): 0.0302
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0509
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15104x11328, b=2048): 0.0114
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15104x11328, b=2048): 246.015
Elapsed time for attention_key_query_prob (128x2048x118x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x118x2048): 69.787
Elapsed time for attention_prob_times_values (128x2048x2048x118): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x118): 111.926
Elapsed time for attention_linear_projection (4x3776x15104, b=2048): 0.0040
Throughput (in TFLOP/s) for attention_linear_projection (4x3776x15104, b=2048): 232.528
Elapsed time for mlp_h_to_4h (4x15104x15104, b=2048): 0.0154
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15104x15104, b=2048): 243.357
Elapsed time for mlp_4h_to_h (4x15104x15104, b=2048): 0.0148
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15104x15104, b=2048): 252.949

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 217.370
MLP duration (in seconds): 0.0301
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0485
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15232x11424, b=2048): 0.0116
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15232x11424, b=2048): 245.149
Elapsed time for attention_key_query_prob (128x2048x119x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x119x2048): 33.853
Elapsed time for attention_prob_times_values (128x2048x2048x119): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x119): 67.294
Elapsed time for attention_linear_projection (4x3808x15232, b=2048): 0.0041
Throughput (in TFLOP/s) for attention_linear_projection (4x3808x15232, b=2048): 230.883
Elapsed time for mlp_h_to_4h (4x15232x15232, b=2048): 0.0155
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15232x15232, b=2048): 245.426
Elapsed time for mlp_4h_to_h (4x15232x15232, b=2048): 0.0155
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15232x15232, b=2048): 245.540

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 189.407
MLP duration (in seconds): 0.0310
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0524
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15360x11520, b=2048): 0.0120
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15360x11520, b=2048): 240.872
Elapsed time for attention_key_query_prob (128x2048x120x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x120x2048): 91.661
Elapsed time for attention_prob_times_values (128x2048x2048x120): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x120): 129.662
Elapsed time for attention_linear_projection (4x3840x15360, b=2048): 0.0087
Throughput (in TFLOP/s) for attention_linear_projection (4x3840x15360, b=2048): 110.990
Elapsed time for mlp_h_to_4h (4x15360x15360, b=2048): 0.0158
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15360x15360, b=2048): 245.231
Elapsed time for mlp_4h_to_h (4x15360x15360, b=2048): 0.0159
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15360x15360, b=2048): 242.964

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 178.167
MLP duration (in seconds): 0.0317
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0548
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15488x11616, b=2048): 0.0124
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15488x11616, b=2048): 236.866
Elapsed time for attention_key_query_prob (128x2048x121x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x121x2048): 34.652
Elapsed time for attention_prob_times_values (128x2048x2048x121): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x121): 68.442
Elapsed time for attention_linear_projection (4x3872x15488, b=2048): 0.0043
Throughput (in TFLOP/s) for attention_linear_projection (4x3872x15488, b=2048): 227.772
Elapsed time for mlp_h_to_4h (4x15488x15488, b=2048): 0.0162
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15488x15488, b=2048): 243.110
Elapsed time for mlp_4h_to_h (4x15488x15488, b=2048): 0.0164
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15488x15488, b=2048): 239.834

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 187.007
MLP duration (in seconds): 0.0326
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0550
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15616x11712, b=2048): 0.0122
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15616x11712, b=2048): 245.751
Elapsed time for attention_key_query_prob (128x2048x122x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x122x2048): 72.143
Elapsed time for attention_prob_times_values (128x2048x2048x122): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x122): 116.728
Elapsed time for attention_linear_projection (4x3904x15616, b=2048): 0.0043
Throughput (in TFLOP/s) for attention_linear_projection (4x3904x15616, b=2048): 234.350
Elapsed time for mlp_h_to_4h (4x15616x15616, b=2048): 0.0161
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15616x15616, b=2048): 248.162
Elapsed time for mlp_4h_to_h (4x15616x15616, b=2048): 0.0163
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15616x15616, b=2048): 244.977

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 219.524
MLP duration (in seconds): 0.0324
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0518
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15744x11808, b=2048): 0.0123
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15744x11808, b=2048): 247.344
Elapsed time for attention_key_query_prob (128x2048x123x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x123x2048): 36.381
Elapsed time for attention_prob_times_values (128x2048x2048x123): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x123): 70.173
Elapsed time for attention_linear_projection (4x3936x15744, b=2048): 0.0044
Throughput (in TFLOP/s) for attention_linear_projection (4x3936x15744, b=2048): 232.829
Elapsed time for mlp_h_to_4h (4x15744x15744, b=2048): 0.0249
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15744x15744, b=2048): 163.274
Elapsed time for mlp_4h_to_h (4x15744x15744, b=2048): 0.0169
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15744x15744, b=2048): 240.855

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 194.945
MLP duration (in seconds): 0.0417
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0639
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15872x11904, b=2048): 0.0127
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15872x11904, b=2048): 243.477
Elapsed time for attention_key_query_prob (128x2048x124x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x124x2048): 73.703
Elapsed time for attention_prob_times_values (128x2048x2048x124): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x124): 118.365
Elapsed time for attention_linear_projection (4x3968x15872, b=2048): 0.0044
Throughput (in TFLOP/s) for attention_linear_projection (4x3968x15872, b=2048): 235.139
Elapsed time for mlp_h_to_4h (4x15872x15872, b=2048): 0.0168
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15872x15872, b=2048): 245.694
Elapsed time for mlp_4h_to_h (4x15872x15872, b=2048): 0.0171
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15872x15872, b=2048): 240.673

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 219.317
MLP duration (in seconds): 0.0339
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0540
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16000x12000, b=2048): 0.0130
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16000x12000, b=2048): 242.099
Elapsed time for attention_key_query_prob (128x2048x125x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x125x2048): 37.156
Elapsed time for attention_prob_times_values (128x2048x2048x125): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x125): 71.695
Elapsed time for attention_linear_projection (4x4000x16000, b=2048): 0.0046
Throughput (in TFLOP/s) for attention_linear_projection (4x4000x16000, b=2048): 229.855
Elapsed time for mlp_h_to_4h (4x16000x16000, b=2048): 0.0167
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16000x16000, b=2048): 250.715
Elapsed time for mlp_4h_to_h (4x16000x16000, b=2048): 0.0169
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16000x16000, b=2048): 247.920

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 193.697
MLP duration (in seconds): 0.0336
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0567
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16128x12096, b=2048): 0.0130
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16128x12096, b=2048): 245.156
Elapsed time for attention_key_query_prob (128x2048x126x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x126x2048): 74.724
Elapsed time for attention_prob_times_values (128x2048x2048x126): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x126): 120.121
Elapsed time for attention_linear_projection (4x4032x16128, b=2048): 0.0045
Throughput (in TFLOP/s) for attention_linear_projection (4x4032x16128, b=2048): 236.077
Elapsed time for mlp_h_to_4h (4x16128x16128, b=2048): 0.0174
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16128x16128, b=2048): 244.351
Elapsed time for mlp_4h_to_h (4x16128x16128, b=2048): 0.0173
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16128x16128, b=2048): 246.460

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 221.220
MLP duration (in seconds): 0.0347
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0552
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16256x12192, b=2048): 0.0132
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16256x12192, b=2048): 246.743
Elapsed time for attention_key_query_prob (128x2048x127x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x127x2048): 36.865
Elapsed time for attention_prob_times_values (128x2048x2048x127): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x127): 71.971
Elapsed time for attention_linear_projection (4x4064x16256, b=2048): 0.0046
Throughput (in TFLOP/s) for attention_linear_projection (4x4064x16256, b=2048): 233.127
Elapsed time for mlp_h_to_4h (4x16256x16256, b=2048): 0.0174
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16256x16256, b=2048): 248.674
Elapsed time for mlp_4h_to_h (4x16256x16256, b=2048): 0.0176
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16256x16256, b=2048): 245.324

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 196.706
MLP duration (in seconds): 0.0351
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0585
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16384x12288, b=2048): 0.0136
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16384x12288, b=2048): 242.691
Elapsed time for attention_key_query_prob (128x2048x128x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x128x2048): 109.739
Elapsed time for attention_prob_times_values (128x2048x2048x128): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x128): 139.748
Elapsed time for attention_linear_projection (4x4096x16384, b=2048): 0.0047
Throughput (in TFLOP/s) for attention_linear_projection (4x4096x16384, b=2048): 235.278
Elapsed time for mlp_h_to_4h (4x16384x16384, b=2048): 0.0183
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16384x16384, b=2048): 240.314
Elapsed time for mlp_4h_to_h (4x16384x16384, b=2048): 0.0181
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16384x16384, b=2048): 243.586

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 227.940
MLP duration (in seconds): 0.0364
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0569
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16512x12384, b=2048): 0.0139
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16512x12384, b=2048): 241.200
Elapsed time for attention_key_query_prob (128x2048x129x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x129x2048): 34.090
Elapsed time for attention_prob_times_values (128x2048x2048x129): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x129): 55.262
Elapsed time for attention_linear_projection (4x4128x16512, b=2048): 0.0045
Throughput (in TFLOP/s) for attention_linear_projection (4x4128x16512, b=2048): 247.793
Elapsed time for mlp_h_to_4h (4x16512x16512, b=2048): 0.0184
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16512x16512, b=2048): 242.973
Elapsed time for mlp_4h_to_h (4x16512x16512, b=2048): 0.0179
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16512x16512, b=2048): 249.094

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 190.017
MLP duration (in seconds): 0.0363
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0613
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16640x12480, b=2048): 0.0139
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16640x12480, b=2048): 244.534
Elapsed time for attention_key_query_prob (128x2048x130x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x130x2048): 70.734
Elapsed time for attention_prob_times_values (128x2048x2048x130): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x130): 94.507
Elapsed time for attention_linear_projection (4x4160x16640, b=2048): 0.0045
Throughput (in TFLOP/s) for attention_linear_projection (4x4160x16640, b=2048): 250.774
Elapsed time for mlp_h_to_4h (4x16640x16640, b=2048): 0.0183
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16640x16640, b=2048): 248.131
Elapsed time for mlp_4h_to_h (4x16640x16640, b=2048): 0.0189
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16640x16640, b=2048): 240.586

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 220.029
MLP duration (in seconds): 0.0371
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0590
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16768x12576, b=2048): 0.0142
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16768x12576, b=2048): 243.652
Elapsed time for attention_key_query_prob (128x2048x131x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x131x2048): 34.507
Elapsed time for attention_prob_times_values (128x2048x2048x131): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x131): 58.142
Elapsed time for attention_linear_projection (4x4192x16768, b=2048): 0.0047
Throughput (in TFLOP/s) for attention_linear_projection (4x4192x16768, b=2048): 245.347
Elapsed time for mlp_h_to_4h (4x16768x16768, b=2048): 0.0185
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16768x16768, b=2048): 249.510
Elapsed time for mlp_4h_to_h (4x16768x16768, b=2048): 0.0192
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16768x16768, b=2048): 239.585

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 192.671
MLP duration (in seconds): 0.0377
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0631
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16896x12672, b=2048): 0.0143
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16896x12672, b=2048): 245.172
Elapsed time for attention_key_query_prob (128x2048x132x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x132x2048): 71.753
Elapsed time for attention_prob_times_values (128x2048x2048x132): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x132): 93.251
Elapsed time for attention_linear_projection (4x4224x16896, b=2048): 0.0047
Throughput (in TFLOP/s) for attention_linear_projection (4x4224x16896, b=2048): 249.132
Elapsed time for mlp_h_to_4h (4x16896x16896, b=2048): 0.0188
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16896x16896, b=2048): 248.668
Elapsed time for mlp_4h_to_h (4x16896x16896, b=2048): 0.0192
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16896x16896, b=2048): 243.640

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 220.507
MLP duration (in seconds): 0.0380
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0605
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17024x12768, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17024x12768, b=2048): 247.663
Elapsed time for attention_key_query_prob (128x2048x133x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x133x2048): 34.392
Elapsed time for attention_prob_times_values (128x2048x2048x133): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x133): 58.649
Elapsed time for attention_linear_projection (4x4256x17024, b=2048): 0.0048
Throughput (in TFLOP/s) for attention_linear_projection (4x4256x17024, b=2048): 246.950
Elapsed time for mlp_h_to_4h (4x17024x17024, b=2048): 0.0192
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17024x17024, b=2048): 247.358
Elapsed time for mlp_4h_to_h (4x17024x17024, b=2048): 0.0194
Throughput (in TFLOP/s) for mlp_4h_to_h (4x17024x17024, b=2048): 244.465

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 195.314
MLP duration (in seconds): 0.0386
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0644
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17152x12864, b=2048): 0.0149
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17152x12864, b=2048): 243.355
Elapsed time for attention_key_query_prob (128x2048x134x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x134x2048): 72.604
Elapsed time for attention_prob_times_values (128x2048x2048x134): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x134): 93.101
Elapsed time for attention_linear_projection (4x4288x17152, b=2048): 0.0048
Throughput (in TFLOP/s) for attention_linear_projection (4x4288x17152, b=2048): 252.569
Elapsed time for mlp_h_to_4h (4x17152x17152, b=2048): 0.0196
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17152x17152, b=2048): 245.870
Elapsed time for mlp_4h_to_h (4x17152x17152, b=2048): 0.0196
Throughput (in TFLOP/s) for mlp_4h_to_h (4x17152x17152, b=2048): 246.367

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 220.610
MLP duration (in seconds): 0.0392
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0623
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17280x12960, b=2048): 0.0146
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17280x12960, b=2048): 252.025
Elapsed time for attention_key_query_prob (128x2048x135x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x135x2048): 33.404
Elapsed time for attention_prob_times_values (128x2048x2048x135): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x135): 57.865
Elapsed time for attention_linear_projection (4x4320x17280, b=2048): 0.0049
Throughput (in TFLOP/s) for attention_linear_projection (4x4320x17280, b=2048): 250.544
Elapsed time for mlp_h_to_4h (4x17280x17280, b=2048): 0.0197
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17280x17280, b=2048): 247.967
Elapsed time for mlp_4h_to_h (4x17280x17280, b=2048): 0.0193
Throughput (in TFLOP/s) for mlp_4h_to_h (4x17280x17280, b=2048): 253.625

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 197.153
MLP duration (in seconds): 0.0390
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0653
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17408x13056, b=2048): 0.0150
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17408x13056, b=2048): 247.826
Elapsed time for attention_key_query_prob (128x2048x136x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x136x2048): 97.889
Elapsed time for attention_prob_times_values (128x2048x2048x136): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x136): 123.138
Elapsed time for attention_linear_projection (4x4352x17408, b=2048): 0.0049
Throughput (in TFLOP/s) for attention_linear_projection (4x4352x17408, b=2048): 252.066
Elapsed time for mlp_h_to_4h (4x17408x17408, b=2048): 0.0200
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17408x17408, b=2048): 247.871
Elapsed time for mlp_4h_to_h (4x17408x17408, b=2048): 0.0199
Throughput (in TFLOP/s) for mlp_4h_to_h (4x17408x17408, b=2048): 249.942

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 232.329
MLP duration (in seconds): 0.0399
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0625
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17536x13152, b=2048): 0.0157
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17536x13152, b=2048): 241.270
Elapsed time for attention_key_query_prob (128x2048x137x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x137x2048): 33.587
Elapsed time for attention_prob_times_values (128x2048x2048x137): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x137): 58.278
Elapsed time for attention_linear_projection (4x4384x17536, b=2048): 0.0050
Throughput (in TFLOP/s) for attention_linear_projection (4x4384x17536, b=2048): 251.237
Elapsed time for mlp_h_to_4h (4x17536x17536, b=2048): 0.0198
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17536x17536, b=2048): 254.578
Elapsed time for mlp_4h_to_h (4x17536x17536, b=2048): 0.0198
Throughput (in TFLOP/s) for mlp_4h_to_h (4x17536x17536, b=2048): 254.578

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 193.352
MLP duration (in seconds): 0.0396
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0672
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17664x13248, b=2048): 0.0159
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17664x13248, b=2048): 241.783
Elapsed time for attention_key_query_prob (128x2048x138x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x138x2048): 73.794
Elapsed time for attention_prob_times_values (128x2048x2048x138): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x138): 93.360
Elapsed time for attention_linear_projection (4x4416x17664, b=2048): 0.0050
Throughput (in TFLOP/s) for attention_linear_projection (4x4416x17664, b=2048): 255.003
Elapsed time for mlp_h_to_4h (4x17664x17664, b=2048): 0.0206
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17664x17664, b=2048): 247.668
Elapsed time for mlp_4h_to_h (4x17664x17664, b=2048): 0.0201
Throughput (in TFLOP/s) for mlp_4h_to_h (4x17664x17664, b=2048): 253.915

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 221.074
MLP duration (in seconds): 0.0408
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0652
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17792x13344, b=2048): 0.0157
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17792x13344, b=2048): 248.430
Elapsed time for attention_key_query_prob (128x2048x139x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x139x2048): 35.299
Elapsed time for attention_prob_times_values (128x2048x2048x139): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x139): 60.054
Elapsed time for attention_linear_projection (4x4448x17792, b=2048): 0.0052
Throughput (in TFLOP/s) for attention_linear_projection (4x4448x17792, b=2048): 250.052
Elapsed time for mlp_h_to_4h (4x17792x17792, b=2048): 0.0205
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17792x17792, b=2048): 252.886
Elapsed time for mlp_4h_to_h (4x17792x17792, b=2048): 0.0208
Throughput (in TFLOP/s) for mlp_4h_to_h (4x17792x17792, b=2048): 249.561

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 199.044
MLP duration (in seconds): 0.0413
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0688
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17920x13440, b=2048): 0.0162
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17920x13440, b=2048): 244.324
Elapsed time for attention_key_query_prob (128x2048x140x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x140x2048): 74.802
Elapsed time for attention_prob_times_values (128x2048x2048x140): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x140): 94.528
Elapsed time for attention_linear_projection (4x4480x17920, b=2048): 0.0052
Throughput (in TFLOP/s) for attention_linear_projection (4x4480x17920, b=2048): 253.663
Elapsed time for mlp_h_to_4h (4x17920x17920, b=2048): 0.0207
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17920x17920, b=2048): 254.145
Elapsed time for mlp_4h_to_h (4x17920x17920, b=2048): 0.0204
Throughput (in TFLOP/s) for mlp_4h_to_h (4x17920x17920, b=2048): 257.298

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 223.051
MLP duration (in seconds): 0.0412
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0661
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18048x13536, b=2048): 0.0163
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18048x13536, b=2048): 246.104
Elapsed time for attention_key_query_prob (128x2048x141x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x141x2048): 34.989
Elapsed time for attention_prob_times_values (128x2048x2048x141): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x141): 59.363
Elapsed time for attention_linear_projection (4x4512x18048, b=2048): 0.0053
Throughput (in TFLOP/s) for attention_linear_projection (4x4512x18048, b=2048): 250.324
Elapsed time for mlp_h_to_4h (4x18048x18048, b=2048): 0.0208
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18048x18048, b=2048): 256.392
Elapsed time for mlp_4h_to_h (4x18048x18048, b=2048): 0.0209
Throughput (in TFLOP/s) for mlp_4h_to_h (4x18048x18048, b=2048): 255.403

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 198.081
MLP duration (in seconds): 0.0417
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0702
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18176x13632, b=2048): 0.0164
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18176x13632, b=2048): 247.503
Elapsed time for attention_key_query_prob (128x2048x142x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x142x2048): 75.548
Elapsed time for attention_prob_times_values (128x2048x2048x142): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x142): 95.649
Elapsed time for attention_linear_projection (4x4544x18176, b=2048): 0.0054
Throughput (in TFLOP/s) for attention_linear_projection (4x4544x18176, b=2048): 252.780
Elapsed time for mlp_h_to_4h (4x18176x18176, b=2048): 0.0212
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18176x18176, b=2048): 255.367
Elapsed time for mlp_4h_to_h (4x18176x18176, b=2048): 0.0212
Throughput (in TFLOP/s) for mlp_4h_to_h (4x18176x18176, b=2048): 255.249

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 225.394
MLP duration (in seconds): 0.0424
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0678
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18304x13728, b=2048): 0.0168
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18304x13728, b=2048): 245.607
Elapsed time for attention_key_query_prob (128x2048x143x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x143x2048): 35.041
Elapsed time for attention_prob_times_values (128x2048x2048x143): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x143): 58.981
Elapsed time for attention_linear_projection (4x4576x18304, b=2048): 0.0054
Throughput (in TFLOP/s) for attention_linear_projection (4x4576x18304, b=2048): 253.831
Elapsed time for mlp_h_to_4h (4x18304x18304, b=2048): 0.0214
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18304x18304, b=2048): 256.996
Elapsed time for mlp_4h_to_h (4x18304x18304, b=2048): 0.0214
Throughput (in TFLOP/s) for mlp_4h_to_h (4x18304x18304, b=2048): 256.905

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 198.819
MLP duration (in seconds): 0.0427
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0719
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18432x13824, b=2048): 0.0178
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18432x13824, b=2048): 234.898
Elapsed time for attention_key_query_prob (128x2048x144x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x144x2048): 110.349
Elapsed time for attention_prob_times_values (128x2048x2048x144): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x144): 134.158
Elapsed time for attention_linear_projection (4x4608x18432, b=2048): 0.0054
Throughput (in TFLOP/s) for attention_linear_projection (4x4608x18432, b=2048): 256.658
Elapsed time for mlp_h_to_4h (4x18432x18432, b=2048): 0.0217
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18432x18432, b=2048): 255.988
Elapsed time for mlp_4h_to_h (4x18432x18432, b=2048): 0.0215
Throughput (in TFLOP/s) for mlp_4h_to_h (4x18432x18432, b=2048): 258.566

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 228.193
MLP duration (in seconds): 0.0433
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0690
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18560x13920, b=2048): 0.0177
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18560x13920, b=2048): 239.286
Elapsed time for attention_key_query_prob (128x2048x145x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x145x2048): 56.446
Elapsed time for attention_prob_times_values (128x2048x2048x145): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x145): 58.614
Elapsed time for attention_linear_projection (4x4640x18560, b=2048): 0.0056
Throughput (in TFLOP/s) for attention_linear_projection (4x4640x18560, b=2048): 250.392
Elapsed time for mlp_h_to_4h (4x18560x18560, b=2048): 0.0217
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18560x18560, b=2048): 260.155
Elapsed time for mlp_4h_to_h (4x18560x18560, b=2048): 0.0220
Throughput (in TFLOP/s) for mlp_4h_to_h (4x18560x18560, b=2048): 256.802

Attention duration (in seconds): 0.0287
Attention throughput (in TFLOP/s): 207.217
MLP duration (in seconds): 0.0437
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0724
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18688x14016, b=2048): 0.0176
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18688x14016, b=2048): 243.952
Elapsed time for attention_key_query_prob (128x2048x146x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x146x2048): 77.347
Elapsed time for attention_prob_times_values (128x2048x2048x146): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x146): 97.585
Elapsed time for attention_linear_projection (4x4672x18688, b=2048): 0.0056
Throughput (in TFLOP/s) for attention_linear_projection (4x4672x18688, b=2048): 254.159
Elapsed time for mlp_h_to_4h (4x18688x18688, b=2048): 0.0223
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18688x18688, b=2048): 256.656
Elapsed time for mlp_4h_to_h (4x18688x18688, b=2048): 0.0226
Throughput (in TFLOP/s) for mlp_4h_to_h (4x18688x18688, b=2048): 253.412

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 224.760
MLP duration (in seconds): 0.0449
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0717
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18816x14112, b=2048): 0.0181
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18816x14112, b=2048): 240.658
Elapsed time for attention_key_query_prob (128x2048x147x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x147x2048): 57.121
Elapsed time for attention_prob_times_values (128x2048x2048x147): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x147): 59.290
Elapsed time for attention_linear_projection (4x4704x18816, b=2048): 0.0058
Throughput (in TFLOP/s) for attention_linear_projection (4x4704x18816, b=2048): 251.995
Elapsed time for mlp_h_to_4h (4x18816x18816, b=2048): 0.0228
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18816x18816, b=2048): 254.797
Elapsed time for mlp_4h_to_h (4x18816x18816, b=2048): 0.0230
Throughput (in TFLOP/s) for mlp_4h_to_h (4x18816x18816, b=2048): 251.809

Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 209.050
MLP duration (in seconds): 0.0458
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0751
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18944x14208, b=2048): 0.0179
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18944x14208, b=2048): 246.897
Elapsed time for attention_key_query_prob (128x2048x148x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x148x2048): 78.296
Elapsed time for attention_prob_times_values (128x2048x2048x148): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x148): 97.861
Elapsed time for attention_linear_projection (4x4736x18944, b=2048): 0.0057
Throughput (in TFLOP/s) for attention_linear_projection (4x4736x18944, b=2048): 255.976
Elapsed time for mlp_h_to_4h (4x18944x18944, b=2048): 0.0226
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18944x18944, b=2048): 259.745
Elapsed time for mlp_4h_to_h (4x18944x18944, b=2048): 0.0231
Throughput (in TFLOP/s) for mlp_4h_to_h (4x18944x18944, b=2048): 254.439

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 227.376
MLP duration (in seconds): 0.0457
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0730
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19072x14304, b=2048): 0.0183
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19072x14304, b=2048): 244.874
Elapsed time for attention_key_query_prob (128x2048x149x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x149x2048): 57.664
Elapsed time for attention_prob_times_values (128x2048x2048x149): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x149): 59.957
Elapsed time for attention_linear_projection (4x4768x19072, b=2048): 0.0059
Throughput (in TFLOP/s) for attention_linear_projection (4x4768x19072, b=2048): 250.894
Elapsed time for mlp_h_to_4h (4x19072x19072, b=2048): 0.0236
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19072x19072, b=2048): 253.051
Elapsed time for mlp_4h_to_h (4x19072x19072, b=2048): 0.0232
Throughput (in TFLOP/s) for mlp_4h_to_h (4x19072x19072, b=2048): 256.328

Attention duration (in seconds): 0.0296
Attention throughput (in TFLOP/s): 211.902
MLP duration (in seconds): 0.0468
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0764
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19200x14400, b=2048): 0.0184
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19200x14400, b=2048): 246.412
Elapsed time for attention_key_query_prob (128x2048x150x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x150x2048): 79.757
Elapsed time for attention_prob_times_values (128x2048x2048x150): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x150): 98.850
Elapsed time for attention_linear_projection (4x4800x19200, b=2048): 0.0059
Throughput (in TFLOP/s) for attention_linear_projection (4x4800x19200, b=2048): 254.284
Elapsed time for mlp_h_to_4h (4x19200x19200, b=2048): 0.0234
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19200x19200, b=2048): 257.792
Elapsed time for mlp_4h_to_h (4x19200x19200, b=2048): 0.0239
Throughput (in TFLOP/s) for mlp_4h_to_h (4x19200x19200, b=2048): 252.905

Attention duration (in seconds): 0.0280
Attention throughput (in TFLOP/s): 227.455
MLP duration (in seconds): 0.0473
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0753
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19328x14496, b=2048): 0.0191
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19328x14496, b=2048): 240.200
Elapsed time for attention_key_query_prob (128x2048x151x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x151x2048): 57.577
Elapsed time for attention_prob_times_values (128x2048x2048x151): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x151): 58.493
Elapsed time for attention_linear_projection (4x4832x19328, b=2048): 0.0064
Throughput (in TFLOP/s) for attention_linear_projection (4x4832x19328, b=2048): 237.674
Elapsed time for mlp_h_to_4h (4x19328x19328, b=2048): 0.0242
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19328x19328, b=2048): 252.850
Elapsed time for mlp_4h_to_h (4x19328x19328, b=2048): 0.0239
Throughput (in TFLOP/s) for mlp_4h_to_h (4x19328x19328, b=2048): 256.048

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 206.986
MLP duration (in seconds): 0.0481
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0792
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19456x14592, b=2048): 0.0190
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19456x14592, b=2048): 244.539
Elapsed time for attention_key_query_prob (128x2048x152x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x152x2048): 103.829
Elapsed time for attention_prob_times_values (128x2048x2048x152): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x152): 138.994
Elapsed time for attention_linear_projection (4x4864x19456, b=2048): 0.0061
Throughput (in TFLOP/s) for attention_linear_projection (4x4864x19456, b=2048): 252.404
Elapsed time for mlp_h_to_4h (4x19456x19456, b=2048): 0.0245
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19456x19456, b=2048): 252.664
Elapsed time for mlp_4h_to_h (4x19456x19456, b=2048): 0.0239
Throughput (in TFLOP/s) for mlp_4h_to_h (4x19456x19456, b=2048): 258.957

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 233.905
MLP duration (in seconds): 0.0485
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0764
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19584x14688, b=2048): 0.0190
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19584x14688, b=2048): 247.491
Elapsed time for attention_key_query_prob (128x2048x153x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x153x2048): 58.281
Elapsed time for attention_prob_times_values (128x2048x2048x153): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x153): 59.514
Elapsed time for attention_linear_projection (4x4896x19584, b=2048): 0.0067
Throughput (in TFLOP/s) for attention_linear_projection (4x4896x19584, b=2048): 233.928
Elapsed time for mlp_h_to_4h (4x19584x19584, b=2048): 0.0247
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19584x19584, b=2048): 254.212
Elapsed time for mlp_4h_to_h (4x19584x19584, b=2048): 0.0244
Throughput (in TFLOP/s) for mlp_4h_to_h (4x19584x19584, b=2048): 257.322

Attention duration (in seconds): 0.0313
Attention throughput (in TFLOP/s): 211.007
MLP duration (in seconds): 0.0491
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0805
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19712x14784, b=2048): 0.0199
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19712x14784, b=2048): 239.782
Elapsed time for attention_key_query_prob (128x2048x154x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x154x2048): 81.595
Elapsed time for attention_prob_times_values (128x2048x2048x154): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x154): 101.486
Elapsed time for attention_linear_projection (4x4928x19712, b=2048): 0.0062
Throughput (in TFLOP/s) for attention_linear_projection (4x4928x19712, b=2048): 258.158
Elapsed time for mlp_h_to_4h (4x19712x19712, b=2048): 0.0248
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19712x19712, b=2048): 256.236
Elapsed time for mlp_4h_to_h (4x19712x19712, b=2048): 0.0331
Throughput (in TFLOP/s) for mlp_4h_to_h (4x19712x19712, b=2048): 192.331

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 225.232
MLP duration (in seconds): 0.0579
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0877
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19840x14880, b=2048): 0.0195
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19840x14880, b=2048): 247.908
Elapsed time for attention_key_query_prob (128x2048x155x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x155x2048): 58.853
Elapsed time for attention_prob_times_values (128x2048x2048x155): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x155): 62.177
Elapsed time for attention_linear_projection (4x4960x19840, b=2048): 0.0068
Throughput (in TFLOP/s) for attention_linear_projection (4x4960x19840, b=2048): 238.601
Elapsed time for mlp_h_to_4h (4x19840x19840, b=2048): 0.0251
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19840x19840, b=2048): 256.749
Elapsed time for mlp_4h_to_h (4x19840x19840, b=2048): 0.0250
Throughput (in TFLOP/s) for mlp_4h_to_h (4x19840x19840, b=2048): 257.609

Attention duration (in seconds): 0.0318
Attention throughput (in TFLOP/s): 213.455
MLP duration (in seconds): 0.0502
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0819
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19968x14976, b=2048): 0.0193
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19968x14976, b=2048): 253.421
Elapsed time for attention_key_query_prob (128x2048x156x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x156x2048): 81.731
Elapsed time for attention_prob_times_values (128x2048x2048x156): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x156): 101.688
Elapsed time for attention_linear_projection (4x4992x19968, b=2048): 0.0068
Throughput (in TFLOP/s) for attention_linear_projection (4x4992x19968, b=2048): 241.536
Elapsed time for mlp_h_to_4h (4x19968x19968, b=2048): 0.0257
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19968x19968, b=2048): 253.785
Elapsed time for mlp_4h_to_h (4x19968x19968, b=2048): 0.0254
Throughput (in TFLOP/s) for mlp_4h_to_h (4x19968x19968, b=2048): 256.933

Attention duration (in seconds): 0.0298
Attention throughput (in TFLOP/s): 230.523
MLP duration (in seconds): 0.0512
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0810
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20096x15072, b=2048): 0.0199
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20096x15072, b=2048): 249.844
Elapsed time for attention_key_query_prob (128x2048x157x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x157x2048): 59.382
Elapsed time for attention_prob_times_values (128x2048x2048x157): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x157): 63.007
Elapsed time for attention_linear_projection (4x5024x20096, b=2048): 0.0070
Throughput (in TFLOP/s) for attention_linear_projection (4x5024x20096, b=2048): 235.389
Elapsed time for mlp_h_to_4h (4x20096x20096, b=2048): 0.0259
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20096x20096, b=2048): 255.709
Elapsed time for mlp_4h_to_h (4x20096x20096, b=2048): 0.0256
Throughput (in TFLOP/s) for mlp_4h_to_h (4x20096x20096, b=2048): 258.527

Attention duration (in seconds): 0.0324
Attention throughput (in TFLOP/s): 214.596
MLP duration (in seconds): 0.0515
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0839
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20224x15168, b=2048): 0.0200
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20224x15168, b=2048): 251.015
Elapsed time for attention_key_query_prob (128x2048x158x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x158x2048): 83.098
Elapsed time for attention_prob_times_values (128x2048x2048x158): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x158): 103.306
Elapsed time for attention_linear_projection (4x5056x20224, b=2048): 0.0068
Throughput (in TFLOP/s) for attention_linear_projection (4x5056x20224, b=2048): 244.630
Elapsed time for mlp_h_to_4h (4x20224x20224, b=2048): 0.0260
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20224x20224, b=2048): 257.357
Elapsed time for mlp_4h_to_h (4x20224x20224, b=2048): 0.0258
Throughput (in TFLOP/s) for mlp_4h_to_h (4x20224x20224, b=2048): 259.845

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 230.425
MLP duration (in seconds): 0.0518
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0824
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20352x15264, b=2048): 0.0199
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20352x15264, b=2048): 256.114
Elapsed time for attention_key_query_prob (128x2048x159x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x159x2048): 60.179
Elapsed time for attention_prob_times_values (128x2048x2048x159): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x159): 62.687
Elapsed time for attention_linear_projection (4x5088x20352, b=2048): 0.0072
Throughput (in TFLOP/s) for attention_linear_projection (4x5088x20352, b=2048): 234.140
Elapsed time for mlp_h_to_4h (4x20352x20352, b=2048): 0.0265
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20352x20352, b=2048): 256.545
Elapsed time for mlp_4h_to_h (4x20352x20352, b=2048): 0.0261
Throughput (in TFLOP/s) for mlp_4h_to_h (4x20352x20352, b=2048): 259.742

Attention duration (in seconds): 0.0327
Attention throughput (in TFLOP/s): 218.112
MLP duration (in seconds): 0.0526
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0853
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20480x15360, b=2048): 0.0202
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20480x15360, b=2048): 255.005
Elapsed time for attention_key_query_prob (128x2048x160x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x160x2048): 123.492
Elapsed time for attention_prob_times_values (128x2048x2048x160): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x160): 148.972
Elapsed time for attention_linear_projection (4x5120x20480, b=2048): 0.0071
Throughput (in TFLOP/s) for attention_linear_projection (4x5120x20480, b=2048): 242.921
Elapsed time for mlp_h_to_4h (4x20480x20480, b=2048): 0.0270
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20480x20480, b=2048): 254.663
Elapsed time for mlp_4h_to_h (4x20480x20480, b=2048): 0.0270
Throughput (in TFLOP/s) for mlp_4h_to_h (4x20480x20480, b=2048): 254.647

Attention duration (in seconds): 0.0298
Attention throughput (in TFLOP/s): 241.906
MLP duration (in seconds): 0.0540
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0838
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20608x15456, b=2048): 0.0204
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20608x15456, b=2048): 256.322
Elapsed time for attention_key_query_prob (128x2048x161x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x161x2048): 47.141
Elapsed time for attention_prob_times_values (128x2048x2048x161): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x161): 62.143
Elapsed time for attention_linear_projection (4x5152x20608, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_linear_projection (4x5152x20608, b=2048): 236.021
Elapsed time for mlp_h_to_4h (4x20608x20608, b=2048): 0.0269
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20608x20608, b=2048): 259.141
Elapsed time for mlp_4h_to_h (4x20608x20608, b=2048): 0.0268
Throughput (in TFLOP/s) for mlp_4h_to_h (4x20608x20608, b=2048): 259.325

Attention duration (in seconds): 0.0342
Attention throughput (in TFLOP/s): 213.696
MLP duration (in seconds): 0.0537
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0879
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20736x15552, b=2048): 0.0208
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20736x15552, b=2048): 254.249
Elapsed time for attention_key_query_prob (128x2048x162x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x162x2048): 78.968
Elapsed time for attention_prob_times_values (128x2048x2048x162): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x162): 102.874
Elapsed time for attention_linear_projection (4x5184x20736, b=2048): 0.0077
Throughput (in TFLOP/s) for attention_linear_projection (4x5184x20736, b=2048): 229.091
Elapsed time for mlp_h_to_4h (4x20736x20736, b=2048): 0.0270
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20736x20736, b=2048): 260.724
Elapsed time for mlp_4h_to_h (4x20736x20736, b=2048): 0.0271
Throughput (in TFLOP/s) for mlp_4h_to_h (4x20736x20736, b=2048): 260.207

Attention duration (in seconds): 0.0324
Attention throughput (in TFLOP/s): 228.433
MLP duration (in seconds): 0.0541
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0865
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20864x15648, b=2048): 0.0238
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20864x15648, b=2048): 224.315
Elapsed time for attention_key_query_prob (128x2048x163x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x163x2048): 57.485
Elapsed time for attention_prob_times_values (128x2048x2048x163): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x163): 64.689
Elapsed time for attention_linear_projection (4x5216x20864, b=2048): 0.0077
Throughput (in TFLOP/s) for attention_linear_projection (4x5216x20864, b=2048): 232.346
Elapsed time for mlp_h_to_4h (4x20864x20864, b=2048): 0.0280
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20864x20864, b=2048): 254.750
Elapsed time for mlp_4h_to_h (4x20864x20864, b=2048): 0.0277
Throughput (in TFLOP/s) for mlp_4h_to_h (4x20864x20864, b=2048): 257.629

Attention duration (in seconds): 0.0373
Attention throughput (in TFLOP/s): 200.752
MLP duration (in seconds): 0.0557
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0930
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20992x15744, b=2048): 0.0211
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20992x15744, b=2048): 257.033
Elapsed time for attention_key_query_prob (128x2048x164x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x164x2048): 79.813
Elapsed time for attention_prob_times_values (128x2048x2048x164): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x164): 80.282
Elapsed time for attention_linear_projection (4x5248x20992, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_linear_projection (4x5248x20992, b=2048): 243.967
Elapsed time for mlp_h_to_4h (4x20992x20992, b=2048): 0.0282
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20992x20992, b=2048): 255.740
Elapsed time for mlp_4h_to_h (4x20992x20992, b=2048): 0.0279
Throughput (in TFLOP/s) for mlp_4h_to_h (4x20992x20992, b=2048): 258.966

Attention duration (in seconds): 0.0329
Attention throughput (in TFLOP/s): 230.398
MLP duration (in seconds): 0.0561
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0890
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21120x15840, b=2048): 0.0214
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21120x15840, b=2048): 255.600
Elapsed time for attention_key_query_prob (128x2048x165x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x165x2048): 58.122
Elapsed time for attention_prob_times_values (128x2048x2048x165): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x165): 65.540
Elapsed time for attention_linear_projection (4x5280x21120, b=2048): 0.0077
Throughput (in TFLOP/s) for attention_linear_projection (4x5280x21120, b=2048): 236.386
Elapsed time for mlp_h_to_4h (4x21120x21120, b=2048): 0.0281
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21120x21120, b=2048): 259.927
Elapsed time for mlp_4h_to_h (4x21120x21120, b=2048): 0.0281
Throughput (in TFLOP/s) for mlp_4h_to_h (4x21120x21120, b=2048): 260.171

Attention duration (in seconds): 0.0349
Attention throughput (in TFLOP/s): 219.402
MLP duration (in seconds): 0.0562
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0911
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21248x15936, b=2048): 0.0219
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21248x15936, b=2048): 253.770
Elapsed time for attention_key_query_prob (128x2048x166x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x166x2048): 80.743
Elapsed time for attention_prob_times_values (128x2048x2048x166): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x166): 80.499
Elapsed time for attention_linear_projection (4x5312x21248, b=2048): 0.0075
Throughput (in TFLOP/s) for attention_linear_projection (4x5312x21248, b=2048): 245.531
Elapsed time for mlp_h_to_4h (4x21248x21248, b=2048): 0.0291
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21248x21248, b=2048): 254.377
Elapsed time for mlp_4h_to_h (4x21248x21248, b=2048): 0.0289
Throughput (in TFLOP/s) for mlp_4h_to_h (4x21248x21248, b=2048): 256.050

Attention duration (in seconds): 0.0338
Attention throughput (in TFLOP/s): 229.294
MLP duration (in seconds): 0.0580
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0918
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21376x16032, b=2048): 0.0220
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21376x16032, b=2048): 255.242
Elapsed time for attention_key_query_prob (128x2048x167x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x167x2048): 58.100
Elapsed time for attention_prob_times_values (128x2048x2048x167): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x167): 64.354
Elapsed time for attention_linear_projection (4x5344x21376, b=2048): 0.0082
Throughput (in TFLOP/s) for attention_linear_projection (4x5344x21376, b=2048): 227.650
Elapsed time for mlp_h_to_4h (4x21376x21376, b=2048): 0.0293
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21376x21376, b=2048): 255.384
Elapsed time for mlp_4h_to_h (4x21376x21376, b=2048): 0.0290
Throughput (in TFLOP/s) for mlp_4h_to_h (4x21376x21376, b=2048): 258.489

Attention duration (in seconds): 0.0361
Attention throughput (in TFLOP/s): 217.362
MLP duration (in seconds): 0.0583
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0944
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21504x16128, b=2048): 0.0224
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21504x16128, b=2048): 253.519
Elapsed time for attention_key_query_prob (128x2048x168x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x168x2048): 111.380
Elapsed time for attention_prob_times_values (128x2048x2048x168): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x168): 148.616
Elapsed time for attention_linear_projection (4x5376x21504, b=2048): 0.0079
Throughput (in TFLOP/s) for attention_linear_projection (4x5376x21504, b=2048): 238.555
Elapsed time for mlp_h_to_4h (4x21504x21504, b=2048): 0.0295
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21504x21504, b=2048): 256.822
Elapsed time for mlp_4h_to_h (4x21504x21504, b=2048): 0.0292
Throughput (in TFLOP/s) for mlp_4h_to_h (4x21504x21504, b=2048): 259.897

Attention duration (in seconds): 0.0332
Attention throughput (in TFLOP/s): 239.165
MLP duration (in seconds): 0.0587
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0918
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21632x16224, b=2048): 0.0223
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21632x16224, b=2048): 257.929
Elapsed time for attention_key_query_prob (128x2048x169x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x169x2048): 59.060
Elapsed time for attention_prob_times_values (128x2048x2048x169): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x169): 64.770
Elapsed time for attention_linear_projection (4x5408x21632, b=2048): 0.0082
Throughput (in TFLOP/s) for attention_linear_projection (4x5408x21632, b=2048): 235.140
Elapsed time for mlp_h_to_4h (4x21632x21632, b=2048): 0.0298
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21632x21632, b=2048): 256.846
Elapsed time for mlp_4h_to_h (4x21632x21632, b=2048): 0.0302
Throughput (in TFLOP/s) for mlp_4h_to_h (4x21632x21632, b=2048): 253.681

Attention duration (in seconds): 0.0363
Attention throughput (in TFLOP/s): 221.090
MLP duration (in seconds): 0.0601
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0964
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21760x16320, b=2048): 0.0227
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21760x16320, b=2048): 255.916
Elapsed time for attention_key_query_prob (128x2048x170x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x170x2048): 82.359
Elapsed time for attention_prob_times_values (128x2048x2048x170): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x170): 110.926
Elapsed time for attention_linear_projection (4x5440x21760, b=2048): 0.0081
Throughput (in TFLOP/s) for attention_linear_projection (4x5440x21760, b=2048): 240.648
Elapsed time for mlp_h_to_4h (4x21760x21760, b=2048): 0.0303
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21760x21760, b=2048): 256.053
Elapsed time for mlp_4h_to_h (4x21760x21760, b=2048): 0.0304
Throughput (in TFLOP/s) for mlp_4h_to_h (4x21760x21760, b=2048): 255.464

Attention duration (in seconds): 0.0347
Attention throughput (in TFLOP/s): 234.382
MLP duration (in seconds): 0.0607
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0953
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21888x16416, b=2048): 0.0231
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21888x16416, b=2048): 254.607
Elapsed time for attention_key_query_prob (128x2048x171x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x171x2048): 59.685
Elapsed time for attention_prob_times_values (128x2048x2048x171): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x171): 68.031
Elapsed time for attention_linear_projection (4x5472x21888, b=2048): 0.0085
Throughput (in TFLOP/s) for attention_linear_projection (4x5472x21888, b=2048): 231.223
Elapsed time for mlp_h_to_4h (4x21888x21888, b=2048): 0.0310
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21888x21888, b=2048): 253.357
Elapsed time for mlp_4h_to_h (4x21888x21888, b=2048): 0.0306
Throughput (in TFLOP/s) for mlp_4h_to_h (4x21888x21888, b=2048): 256.278

Attention duration (in seconds): 0.0374
Attention throughput (in TFLOP/s): 219.789
MLP duration (in seconds): 0.0616
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0990
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22016x16512, b=2048): 0.0235
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22016x16512, b=2048): 253.156
Elapsed time for attention_key_query_prob (128x2048x172x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x172x2048): 83.337
Elapsed time for attention_prob_times_values (128x2048x2048x172): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x172): 111.601
Elapsed time for attention_linear_projection (4x5504x22016, b=2048): 0.0082
Throughput (in TFLOP/s) for attention_linear_projection (4x5504x22016, b=2048): 241.647
Elapsed time for mlp_h_to_4h (4x22016x22016, b=2048): 0.0305
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22016x22016, b=2048): 260.655
Elapsed time for mlp_4h_to_h (4x22016x22016, b=2048): 0.0304
Throughput (in TFLOP/s) for mlp_4h_to_h (4x22016x22016, b=2048): 261.365

Attention duration (in seconds): 0.0356
Attention throughput (in TFLOP/s): 233.357
MLP duration (in seconds): 0.0609
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0965
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22144x16608, b=2048): 0.0239
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22144x16608, b=2048): 251.661
Elapsed time for attention_key_query_prob (128x2048x173x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x173x2048): 59.808
Elapsed time for attention_prob_times_values (128x2048x2048x173): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x173): 68.111
Elapsed time for attention_linear_projection (4x5536x22144, b=2048): 0.0085
Throughput (in TFLOP/s) for attention_linear_projection (4x5536x22144, b=2048): 236.000
Elapsed time for mlp_h_to_4h (4x22144x22144, b=2048): 0.0315
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22144x22144, b=2048): 254.683
Elapsed time for mlp_4h_to_h (4x22144x22144, b=2048): 0.0311
Throughput (in TFLOP/s) for mlp_4h_to_h (4x22144x22144, b=2048): 257.915

Attention duration (in seconds): 0.0383
Attention throughput (in TFLOP/s): 219.542
MLP duration (in seconds): 0.0627
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22272x16704, b=2048): 0.0239
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22272x16704, b=2048): 255.541
Elapsed time for attention_key_query_prob (128x2048x174x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x174x2048): 82.801
Elapsed time for attention_prob_times_values (128x2048x2048x174): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x174): 110.964
Elapsed time for attention_linear_projection (4x5568x22272, b=2048): 0.0084
Throughput (in TFLOP/s) for attention_linear_projection (4x5568x22272, b=2048): 242.197
Elapsed time for mlp_h_to_4h (4x22272x22272, b=2048): 0.0324
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22272x22272, b=2048): 250.474
Elapsed time for mlp_4h_to_h (4x22272x22272, b=2048): 0.0322
Throughput (in TFLOP/s) for mlp_4h_to_h (4x22272x22272, b=2048): 252.786

Attention duration (in seconds): 0.0362
Attention throughput (in TFLOP/s): 234.946
MLP duration (in seconds): 0.0646
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22400x16800, b=2048): 0.0243
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22400x16800, b=2048): 254.235
Elapsed time for attention_key_query_prob (128x2048x175x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x175x2048): 60.245
Elapsed time for attention_prob_times_values (128x2048x2048x175): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x175): 66.853
Elapsed time for attention_linear_projection (4x5600x22400, b=2048): 0.0088
Throughput (in TFLOP/s) for attention_linear_projection (4x5600x22400, b=2048): 234.582
Elapsed time for mlp_h_to_4h (4x22400x22400, b=2048): 0.0320
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22400x22400, b=2048): 257.163
Elapsed time for mlp_4h_to_h (4x22400x22400, b=2048): 0.0319
Throughput (in TFLOP/s) for mlp_4h_to_h (4x22400x22400, b=2048): 257.836

Attention duration (in seconds): 0.0389
Attention throughput (in TFLOP/s): 220.752
MLP duration (in seconds): 0.0639
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22528x16896, b=2048): 0.0244
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22528x16896, b=2048): 255.753
Elapsed time for attention_key_query_prob (128x2048x176x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x176x2048): 122.908
Elapsed time for attention_prob_times_values (128x2048x2048x176): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x176): 148.935
Elapsed time for attention_linear_projection (4x5632x22528, b=2048): 0.0085
Throughput (in TFLOP/s) for attention_linear_projection (4x5632x22528, b=2048): 243.159
Elapsed time for mlp_h_to_4h (4x22528x22528, b=2048): 0.0324
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22528x22528, b=2048): 256.837
Elapsed time for mlp_4h_to_h (4x22528x22528, b=2048): 0.0323
Throughput (in TFLOP/s) for mlp_4h_to_h (4x22528x22528, b=2048): 257.459

Attention duration (in seconds): 0.0357
Attention throughput (in TFLOP/s): 243.233
MLP duration (in seconds): 0.0647
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1004
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22656x16992, b=2048): 0.0246
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22656x16992, b=2048): 256.815
Elapsed time for attention_key_query_prob (128x2048x177x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x177x2048): 60.785
Elapsed time for attention_prob_times_values (128x2048x2048x177): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x177): 67.167
Elapsed time for attention_linear_projection (4x5664x22656, b=2048): 0.0091
Throughput (in TFLOP/s) for attention_linear_projection (4x5664x22656, b=2048): 231.004
Elapsed time for mlp_h_to_4h (4x22656x22656, b=2048): 0.0330
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22656x22656, b=2048): 255.177
Elapsed time for mlp_4h_to_h (4x22656x22656, b=2048): 0.0325
Throughput (in TFLOP/s) for mlp_4h_to_h (4x22656x22656, b=2048): 258.716

Attention duration (in seconds): 0.0396
Attention throughput (in TFLOP/s): 221.869
MLP duration (in seconds): 0.0655
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22784x17088, b=2048): 0.0250
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22784x17088, b=2048): 254.985
Elapsed time for attention_key_query_prob (128x2048x178x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x178x2048): 84.357
Elapsed time for attention_prob_times_values (128x2048x2048x178): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x178): 113.210
Elapsed time for attention_linear_projection (4x5696x22784, b=2048): 0.0089
Throughput (in TFLOP/s) for attention_linear_projection (4x5696x22784, b=2048): 239.757
Elapsed time for mlp_h_to_4h (4x22784x22784, b=2048): 0.0327
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22784x22784, b=2048): 259.954
Elapsed time for mlp_4h_to_h (4x22784x22784, b=2048): 0.0330
Throughput (in TFLOP/s) for mlp_4h_to_h (4x22784x22784, b=2048): 257.416

Attention duration (in seconds): 0.0378
Attention throughput (in TFLOP/s): 234.873
MLP duration (in seconds): 0.0658
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22912x17184, b=2048): 0.0249
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22912x17184, b=2048): 259.263
Elapsed time for attention_key_query_prob (128x2048x179x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x179x2048): 60.905
Elapsed time for attention_prob_times_values (128x2048x2048x179): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x179): 70.063
Elapsed time for attention_linear_projection (4x5728x22912, b=2048): 0.0091
Throughput (in TFLOP/s) for attention_linear_projection (4x5728x22912, b=2048): 235.489
Elapsed time for mlp_h_to_4h (4x22912x22912, b=2048): 0.0331
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22912x22912, b=2048): 260.240
Elapsed time for mlp_4h_to_h (4x22912x22912, b=2048): 0.0334
Throughput (in TFLOP/s) for mlp_4h_to_h (4x22912x22912, b=2048): 257.188

Attention duration (in seconds): 0.0399
Attention throughput (in TFLOP/s): 225.136
MLP duration (in seconds): 0.0665
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23040x17280, b=2048): 0.0266
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23040x17280, b=2048): 245.459
Elapsed time for attention_key_query_prob (128x2048x180x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x180x2048): 86.386
Elapsed time for attention_prob_times_values (128x2048x2048x180): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x180): 114.047
Elapsed time for attention_linear_projection (4x5760x23040, b=2048): 0.0089
Throughput (in TFLOP/s) for attention_linear_projection (4x5760x23040, b=2048): 244.171
Elapsed time for mlp_h_to_4h (4x23040x23040, b=2048): 0.0353
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23040x23040, b=2048): 246.196
Elapsed time for mlp_4h_to_h (4x23040x23040, b=2048): 0.0354
Throughput (in TFLOP/s) for mlp_4h_to_h (4x23040x23040, b=2048): 245.823

Attention duration (in seconds): 0.0394
Attention throughput (in TFLOP/s): 230.487
MLP duration (in seconds): 0.0707
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23168x17376, b=2048): 0.0267
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23168x17376, b=2048): 247.088
Elapsed time for attention_key_query_prob (128x2048x181x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x181x2048): 60.846
Elapsed time for attention_prob_times_values (128x2048x2048x181): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x181): 70.833
Elapsed time for attention_linear_projection (4x5792x23168, b=2048): 0.0091
Throughput (in TFLOP/s) for attention_linear_projection (4x5792x23168, b=2048): 241.708
Elapsed time for mlp_h_to_4h (4x23168x23168, b=2048): 0.0357
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23168x23168, b=2048): 246.259
Elapsed time for mlp_4h_to_h (4x23168x23168, b=2048): 0.0357
Throughput (in TFLOP/s) for mlp_4h_to_h (4x23168x23168, b=2048): 246.057

Attention duration (in seconds): 0.0417
Attention throughput (in TFLOP/s): 220.070
MLP duration (in seconds): 0.0715
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23296x17472, b=2048): 0.0267
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23296x17472, b=2048): 250.192
Elapsed time for attention_key_query_prob (128x2048x182x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x182x2048): 87.514
Elapsed time for attention_prob_times_values (128x2048x2048x182): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x182): 115.787
Elapsed time for attention_linear_projection (4x5824x23296, b=2048): 0.0092
Throughput (in TFLOP/s) for attention_linear_projection (4x5824x23296, b=2048): 240.602
Elapsed time for mlp_h_to_4h (4x23296x23296, b=2048): 0.0346
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23296x23296, b=2048): 257.167
Elapsed time for mlp_4h_to_h (4x23296x23296, b=2048): 0.0348
Throughput (in TFLOP/s) for mlp_4h_to_h (4x23296x23296, b=2048): 255.554

Attention duration (in seconds): 0.0398
Attention throughput (in TFLOP/s): 233.145
MLP duration (in seconds): 0.0694
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23424x17568, b=2048): 0.0274
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23424x17568, b=2048): 245.771
Elapsed time for attention_key_query_prob (128x2048x183x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x183x2048): 61.258
Elapsed time for attention_prob_times_values (128x2048x2048x183): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x183): 68.412
Elapsed time for attention_linear_projection (4x5856x23424, b=2048): 0.0095
Throughput (in TFLOP/s) for attention_linear_projection (4x5856x23424, b=2048): 236.795
Elapsed time for mlp_h_to_4h (4x23424x23424, b=2048): 0.0366
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23424x23424, b=2048): 245.477
Elapsed time for mlp_4h_to_h (4x23424x23424, b=2048): 0.0368
Throughput (in TFLOP/s) for mlp_4h_to_h (4x23424x23424, b=2048): 244.485

Attention duration (in seconds): 0.0430
Attention throughput (in TFLOP/s): 218.181
MLP duration (in seconds): 0.0734
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23552x17664, b=2048): 0.0278
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23552x17664, b=2048): 245.505
Elapsed time for attention_key_query_prob (128x2048x184x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x184x2048): 113.624
Elapsed time for attention_prob_times_values (128x2048x2048x184): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x184): 162.324
Elapsed time for attention_linear_projection (4x5888x23552, b=2048): 0.0093
Throughput (in TFLOP/s) for attention_linear_projection (4x5888x23552, b=2048): 245.122
Elapsed time for mlp_h_to_4h (4x23552x23552, b=2048): 0.0351
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23552x23552, b=2048): 259.122
Elapsed time for mlp_4h_to_h (4x23552x23552, b=2048): 0.0351
Throughput (in TFLOP/s) for mlp_4h_to_h (4x23552x23552, b=2048): 259.078

Attention duration (in seconds): 0.0400
Attention throughput (in TFLOP/s): 237.150
MLP duration (in seconds): 0.0702
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23680x17760, b=2048): 0.0287
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23680x17760, b=2048): 239.919
Elapsed time for attention_key_query_prob (128x2048x185x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x185x2048): 62.028
Elapsed time for attention_prob_times_values (128x2048x2048x185): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x185): 69.240
Elapsed time for attention_linear_projection (4x5920x23680, b=2048): 0.0098
Throughput (in TFLOP/s) for attention_linear_projection (4x5920x23680, b=2048): 235.451
Elapsed time for mlp_h_to_4h (4x23680x23680, b=2048): 0.0357
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23680x23680, b=2048): 257.032
Elapsed time for mlp_4h_to_h (4x23680x23680, b=2048): 0.0353
Throughput (in TFLOP/s) for mlp_4h_to_h (4x23680x23680, b=2048): 260.354

Attention duration (in seconds): 0.0445
Attention throughput (in TFLOP/s): 215.160
MLP duration (in seconds): 0.0710
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23808x17856, b=2048): 0.0286
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23808x17856, b=2048): 243.944
Elapsed time for attention_key_query_prob (128x2048x186x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x186x2048): 87.723
Elapsed time for attention_prob_times_values (128x2048x2048x186): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x186): 118.165
Elapsed time for attention_linear_projection (4x5952x23808, b=2048): 0.0095
Throughput (in TFLOP/s) for attention_linear_projection (4x5952x23808, b=2048): 244.015
Elapsed time for mlp_h_to_4h (4x23808x23808, b=2048): 0.0378
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23808x23808, b=2048): 245.482
Elapsed time for mlp_4h_to_h (4x23808x23808, b=2048): 0.0378
Throughput (in TFLOP/s) for mlp_4h_to_h (4x23808x23808, b=2048): 245.811

Attention duration (in seconds): 0.0420
Attention throughput (in TFLOP/s): 230.441
MLP duration (in seconds): 0.0756
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23936x17952, b=2048): 0.0285
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23936x17952, b=2048): 247.234
Elapsed time for attention_key_query_prob (128x2048x187x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x187x2048): 62.480
Elapsed time for attention_prob_times_values (128x2048x2048x187): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x187): 73.443
Elapsed time for attention_linear_projection (4x5984x23936, b=2048): 0.0101
Throughput (in TFLOP/s) for attention_linear_projection (4x5984x23936, b=2048): 232.270
Elapsed time for mlp_h_to_4h (4x23936x23936, b=2048): 0.0358
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23936x23936, b=2048): 262.137
Elapsed time for mlp_4h_to_h (4x23936x23936, b=2048): 0.0362
Throughput (in TFLOP/s) for mlp_4h_to_h (4x23936x23936, b=2048): 258.954

Attention duration (in seconds): 0.0445
Attention throughput (in TFLOP/s): 219.834
MLP duration (in seconds): 0.0721
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24064x18048, b=2048): 0.0283
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24064x18048, b=2048): 251.586
Elapsed time for attention_key_query_prob (128x2048x188x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x188x2048): 89.843
Elapsed time for attention_prob_times_values (128x2048x2048x188): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x188): 119.977
Elapsed time for attention_linear_projection (4x6016x24064, b=2048): 0.0100
Throughput (in TFLOP/s) for attention_linear_projection (4x6016x24064, b=2048): 237.388
Elapsed time for mlp_h_to_4h (4x24064x24064, b=2048): 0.0365
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24064x24064, b=2048): 259.875
Elapsed time for mlp_4h_to_h (4x24064x24064, b=2048): 0.0369
Throughput (in TFLOP/s) for mlp_4h_to_h (4x24064x24064, b=2048): 256.885

Attention duration (in seconds): 0.0422
Attention throughput (in TFLOP/s): 234.367
MLP duration (in seconds): 0.0734
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24192x18144, b=2048): 0.0286
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24192x18144, b=2048): 251.234
Elapsed time for attention_key_query_prob (128x2048x189x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x189x2048): 62.990
Elapsed time for attention_prob_times_values (128x2048x2048x189): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x189): 73.945
Elapsed time for attention_linear_projection (4x6048x24192, b=2048): 0.0103
Throughput (in TFLOP/s) for attention_linear_projection (4x6048x24192, b=2048): 233.518
Elapsed time for mlp_h_to_4h (4x24192x24192, b=2048): 0.0367
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24192x24192, b=2048): 261.108
Elapsed time for mlp_4h_to_h (4x24192x24192, b=2048): 0.0378
Throughput (in TFLOP/s) for mlp_4h_to_h (4x24192x24192, b=2048): 253.706

Attention duration (in seconds): 0.0449
Attention throughput (in TFLOP/s): 222.813
MLP duration (in seconds): 0.0745
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24320x18240, b=2048): 0.0291
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24320x18240, b=2048): 250.086
Elapsed time for attention_key_query_prob (128x2048x190x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x190x2048): 91.253
Elapsed time for attention_prob_times_values (128x2048x2048x190): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x190): 121.425
Elapsed time for attention_linear_projection (4x6080x24320, b=2048): 0.0099
Throughput (in TFLOP/s) for attention_linear_projection (4x6080x24320, b=2048): 244.773
Elapsed time for mlp_h_to_4h (4x24320x24320, b=2048): 0.0394
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24320x24320, b=2048): 246.251
Elapsed time for mlp_4h_to_h (4x24320x24320, b=2048): 0.0395
Throughput (in TFLOP/s) for mlp_4h_to_h (4x24320x24320, b=2048): 245.521

Attention duration (in seconds): 0.0429
Attention throughput (in TFLOP/s): 235.536
MLP duration (in seconds): 0.0788
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24448x18336, b=2048): 0.0297
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24448x18336, b=2048): 247.608
Elapsed time for attention_key_query_prob (128x2048x191x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x191x2048): 63.310
Elapsed time for attention_prob_times_values (128x2048x2048x191): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x191): 61.411
Elapsed time for attention_linear_projection (4x6112x24448, b=2048): 0.0107
Throughput (in TFLOP/s) for attention_linear_projection (4x6112x24448, b=2048): 229.422
Elapsed time for mlp_h_to_4h (4x24448x24448, b=2048): 0.0383
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24448x24448, b=2048): 255.959
Elapsed time for mlp_4h_to_h (4x24448x24448, b=2048): 0.0380
Throughput (in TFLOP/s) for mlp_4h_to_h (4x24448x24448, b=2048): 257.662

Attention duration (in seconds): 0.0469
Attention throughput (in TFLOP/s): 217.491
MLP duration (in seconds): 0.0763
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24576x18432, b=2048): 0.0302
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24576x18432, b=2048): 245.697
Elapsed time for attention_key_query_prob (128x2048x192x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x192x2048): 137.471
Elapsed time for attention_prob_times_values (128x2048x2048x192): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x192): 173.842
Elapsed time for attention_linear_projection (4x6144x24576, b=2048): 0.0104
Throughput (in TFLOP/s) for attention_linear_projection (4x6144x24576, b=2048): 238.459
Elapsed time for mlp_h_to_4h (4x24576x24576, b=2048): 0.0380
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24576x24576, b=2048): 260.554
Elapsed time for mlp_4h_to_h (4x24576x24576, b=2048): 0.0385
Throughput (in TFLOP/s) for mlp_4h_to_h (4x24576x24576, b=2048): 257.246

Attention duration (in seconds): 0.0433
Attention throughput (in TFLOP/s): 238.241
MLP duration (in seconds): 0.0764
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24704x18528, b=2048): 0.0298
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24704x18528, b=2048): 251.725
Elapsed time for attention_key_query_prob (128x2048x193x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x193x2048): 60.868
Elapsed time for attention_prob_times_values (128x2048x2048x193): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x193): 72.025
Elapsed time for attention_linear_projection (4x6176x24704, b=2048): 0.0107
Throughput (in TFLOP/s) for attention_linear_projection (4x6176x24704, b=2048): 233.548
Elapsed time for mlp_h_to_4h (4x24704x24704, b=2048): 0.0406
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24704x24704, b=2048): 246.232
Elapsed time for mlp_4h_to_h (4x24704x24704, b=2048): 0.0402
Throughput (in TFLOP/s) for mlp_4h_to_h (4x24704x24704, b=2048): 248.547

Attention duration (in seconds): 0.0468
Attention throughput (in TFLOP/s): 222.621
MLP duration (in seconds): 0.0808
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24832x18624, b=2048): 0.0305
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24832x18624, b=2048): 248.202
Elapsed time for attention_key_query_prob (128x2048x194x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x194x2048): 87.056
Elapsed time for attention_prob_times_values (128x2048x2048x194): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x194): 121.128
Elapsed time for attention_linear_projection (4x6208x24832, b=2048): 0.0105
Throughput (in TFLOP/s) for attention_linear_projection (4x6208x24832, b=2048): 239.956
Elapsed time for mlp_h_to_4h (4x24832x24832, b=2048): 0.0398
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24832x24832, b=2048): 253.912
Elapsed time for mlp_4h_to_h (4x24832x24832, b=2048): 0.0395
Throughput (in TFLOP/s) for mlp_4h_to_h (4x24832x24832, b=2048): 255.992

Attention duration (in seconds): 0.0452
Attention throughput (in TFLOP/s): 232.905
MLP duration (in seconds): 0.0793
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24960x18720, b=2048): 0.0313
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24960x18720, b=2048): 244.724
Elapsed time for attention_key_query_prob (128x2048x195x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x195x2048): 61.207
Elapsed time for attention_prob_times_values (128x2048x2048x195): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x195): 75.305
Elapsed time for attention_linear_projection (4x6240x24960, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x6240x24960, b=2048): 233.295
Elapsed time for mlp_h_to_4h (4x24960x24960, b=2048): 0.0391
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24960x24960, b=2048): 261.126
Elapsed time for mlp_4h_to_h (4x24960x24960, b=2048): 0.0392
Throughput (in TFLOP/s) for mlp_4h_to_h (4x24960x24960, b=2048): 260.628

Attention duration (in seconds): 0.0484
Attention throughput (in TFLOP/s): 219.449
MLP duration (in seconds): 0.0783
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25088x18816, b=2048): 0.0311
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25088x18816, b=2048): 248.369
Elapsed time for attention_key_query_prob (128x2048x196x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x196x2048): 87.518
Elapsed time for attention_prob_times_values (128x2048x2048x196): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x196): 119.803
Elapsed time for attention_linear_projection (4x6272x25088, b=2048): 0.0106
Throughput (in TFLOP/s) for attention_linear_projection (4x6272x25088, b=2048): 244.161
Elapsed time for mlp_h_to_4h (4x25088x25088, b=2048): 0.0421
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25088x25088, b=2048): 245.007
Elapsed time for mlp_4h_to_h (4x25088x25088, b=2048): 0.0420
Throughput (in TFLOP/s) for mlp_4h_to_h (4x25088x25088, b=2048): 245.572

Attention duration (in seconds): 0.0459
Attention throughput (in TFLOP/s): 234.041
MLP duration (in seconds): 0.0841
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1299
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25216x18912, b=2048): 0.0313
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25216x18912, b=2048): 249.365
Elapsed time for attention_key_query_prob (128x2048x197x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x197x2048): 52.327
Elapsed time for attention_prob_times_values (128x2048x2048x197): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x197): 75.162
Elapsed time for attention_linear_projection (4x6304x25216, b=2048): 0.0111
Throughput (in TFLOP/s) for attention_linear_projection (4x6304x25216, b=2048): 235.375
Elapsed time for mlp_h_to_4h (4x25216x25216, b=2048): 0.0402
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25216x25216, b=2048): 259.186
Elapsed time for mlp_4h_to_h (4x25216x25216, b=2048): 0.0402
Throughput (in TFLOP/s) for mlp_4h_to_h (4x25216x25216, b=2048): 259.184

Attention duration (in seconds): 0.0493
Attention throughput (in TFLOP/s): 220.097
MLP duration (in seconds): 0.0804
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1296
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25344x19008, b=2048): 0.0317
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25344x19008, b=2048): 248.864
Elapsed time for attention_key_query_prob (128x2048x198x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x198x2048): 87.286
Elapsed time for attention_prob_times_values (128x2048x2048x198): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x198): 118.974
Elapsed time for attention_linear_projection (4x6336x25344, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x6336x25344, b=2048): 243.796
Elapsed time for mlp_h_to_4h (4x25344x25344, b=2048): 0.0405
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25344x25344, b=2048): 260.088
Elapsed time for mlp_4h_to_h (4x25344x25344, b=2048): 0.0405
Throughput (in TFLOP/s) for mlp_4h_to_h (4x25344x25344, b=2048): 260.161

Attention duration (in seconds): 0.0467
Attention throughput (in TFLOP/s): 234.304
MLP duration (in seconds): 0.0809
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25472x19104, b=2048): 0.0322
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25472x19104, b=2048): 247.411
Elapsed time for attention_key_query_prob (128x2048x199x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x199x2048): 61.482
Elapsed time for attention_prob_times_values (128x2048x2048x199): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x199): 67.430
Elapsed time for attention_linear_projection (4x6368x25472, b=2048): 0.0114
Throughput (in TFLOP/s) for attention_linear_projection (4x6368x25472, b=2048): 234.066
Elapsed time for mlp_h_to_4h (4x25472x25472, b=2048): 0.0407
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25472x25472, b=2048): 261.151
Elapsed time for mlp_4h_to_h (4x25472x25472, b=2048): 0.0407
Throughput (in TFLOP/s) for mlp_4h_to_h (4x25472x25472, b=2048): 261.196

Attention duration (in seconds): 0.0502
Attention throughput (in TFLOP/s): 220.172
MLP duration (in seconds): 0.0814
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1316
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25600x19200, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25600x19200, b=2048): 248.990
Elapsed time for attention_key_query_prob (128x2048x200x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x200x2048): 120.530
Elapsed time for attention_prob_times_values (128x2048x2048x200): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x200): 176.370
Elapsed time for attention_linear_projection (4x6400x25600, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x6400x25600, b=2048): 245.471
Elapsed time for mlp_h_to_4h (4x25600x25600, b=2048): 0.0415
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25600x25600, b=2048): 258.753
Elapsed time for mlp_4h_to_h (4x25600x25600, b=2048): 0.0410
Throughput (in TFLOP/s) for mlp_4h_to_h (4x25600x25600, b=2048): 261.880

Attention duration (in seconds): 0.0463
Attention throughput (in TFLOP/s): 241.302
MLP duration (in seconds): 0.0825
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1288
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25728x19296, b=2048): 0.0332
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25728x19296, b=2048): 244.950
Elapsed time for attention_key_query_prob (128x2048x201x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x201x2048): 62.593
Elapsed time for attention_prob_times_values (128x2048x2048x201): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x201): 73.143
Elapsed time for attention_linear_projection (4x6432x25728, b=2048): 0.0115
Throughput (in TFLOP/s) for attention_linear_projection (4x6432x25728, b=2048): 236.480
Elapsed time for mlp_h_to_4h (4x25728x25728, b=2048): 0.0417
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25728x25728, b=2048): 259.925
Elapsed time for mlp_4h_to_h (4x25728x25728, b=2048): 0.0557
Throughput (in TFLOP/s) for mlp_4h_to_h (4x25728x25728, b=2048): 194.869

Attention duration (in seconds): 0.0511
Attention throughput (in TFLOP/s): 220.810
MLP duration (in seconds): 0.0974
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1484
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25856x19392, b=2048): 0.0334
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25856x19392, b=2048): 246.267
Elapsed time for attention_key_query_prob (128x2048x202x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x202x2048): 89.338
Elapsed time for attention_prob_times_values (128x2048x2048x202): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x202): 117.657
Elapsed time for attention_linear_projection (4x6464x25856, b=2048): 0.0113
Throughput (in TFLOP/s) for attention_linear_projection (4x6464x25856, b=2048): 241.653
Elapsed time for mlp_h_to_4h (4x25856x25856, b=2048): 0.0420
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25856x25856, b=2048): 260.830
Elapsed time for mlp_4h_to_h (4x25856x25856, b=2048): 0.0421
Throughput (in TFLOP/s) for mlp_4h_to_h (4x25856x25856, b=2048): 260.464

Attention duration (in seconds): 0.0490
Attention throughput (in TFLOP/s): 232.575
MLP duration (in seconds): 0.0840
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1330
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25984x19488, b=2048): 0.0336
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25984x19488, b=2048): 247.258
Elapsed time for attention_key_query_prob (128x2048x203x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x203x2048): 63.212
Elapsed time for attention_prob_times_values (128x2048x2048x203): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x203): 75.017
Elapsed time for attention_linear_projection (4x6496x25984, b=2048): 0.0117
Throughput (in TFLOP/s) for attention_linear_projection (4x6496x25984, b=2048): 235.519
Elapsed time for mlp_h_to_4h (4x25984x25984, b=2048): 0.0425
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25984x25984, b=2048): 260.116
Elapsed time for mlp_4h_to_h (4x25984x25984, b=2048): 0.0428
Throughput (in TFLOP/s) for mlp_4h_to_h (4x25984x25984, b=2048): 258.309

Attention duration (in seconds): 0.0516
Attention throughput (in TFLOP/s): 222.613
MLP duration (in seconds): 0.0854
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1370
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26112x19584, b=2048): 0.0337
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26112x19584, b=2048): 248.498
Elapsed time for attention_key_query_prob (128x2048x204x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x204x2048): 90.355
Elapsed time for attention_prob_times_values (128x2048x2048x204): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x204): 121.574
Elapsed time for attention_linear_projection (4x6528x26112, b=2048): 0.0113
Throughput (in TFLOP/s) for attention_linear_projection (4x6528x26112, b=2048): 246.188
Elapsed time for mlp_h_to_4h (4x26112x26112, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26112x26112, b=2048): 259.407
Elapsed time for mlp_4h_to_h (4x26112x26112, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_4h_to_h (4x26112x26112, b=2048): 259.462

Attention duration (in seconds): 0.0493
Attention throughput (in TFLOP/s): 235.548
MLP duration (in seconds): 0.0861
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1354
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26240x19680, b=2048): 0.0345
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26240x19680, b=2048): 245.140
Elapsed time for attention_key_query_prob (128x2048x205x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x205x2048): 63.028
Elapsed time for attention_prob_times_values (128x2048x2048x205): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x205): 74.926
Elapsed time for attention_linear_projection (4x6560x26240, b=2048): 0.0119
Throughput (in TFLOP/s) for attention_linear_projection (4x6560x26240, b=2048): 236.712
Elapsed time for mlp_h_to_4h (4x26240x26240, b=2048): 0.0437
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26240x26240, b=2048): 258.041
Elapsed time for mlp_4h_to_h (4x26240x26240, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x26240x26240, b=2048): 260.486

Attention duration (in seconds): 0.0529
Attention throughput (in TFLOP/s): 221.748
MLP duration (in seconds): 0.0870
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1399
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26368x19776, b=2048): 0.0347
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26368x19776, b=2048): 246.023
Elapsed time for attention_key_query_prob (128x2048x206x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x206x2048): 91.152
Elapsed time for attention_prob_times_values (128x2048x2048x206): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x206): 122.329
Elapsed time for attention_linear_projection (4x6592x26368, b=2048): 0.0118
Throughput (in TFLOP/s) for attention_linear_projection (4x6592x26368, b=2048): 242.236
Elapsed time for mlp_h_to_4h (4x26368x26368, b=2048): 0.0441
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26368x26368, b=2048): 258.177
Elapsed time for mlp_4h_to_h (4x26368x26368, b=2048): 0.0441
Throughput (in TFLOP/s) for mlp_4h_to_h (4x26368x26368, b=2048): 258.181

Attention duration (in seconds): 0.0507
Attention throughput (in TFLOP/s): 233.326
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1390
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26496x19872, b=2048): 0.0349
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26496x19872, b=2048): 247.444
Elapsed time for attention_key_query_prob (128x2048x207x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x207x2048): 64.050
Elapsed time for attention_prob_times_values (128x2048x2048x207): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x207): 75.333
Elapsed time for attention_linear_projection (4x6624x26496, b=2048): 0.0122
Throughput (in TFLOP/s) for attention_linear_projection (4x6624x26496, b=2048): 236.026
Elapsed time for mlp_h_to_4h (4x26496x26496, b=2048): 0.0461
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26496x26496, b=2048): 249.557
Elapsed time for mlp_4h_to_h (4x26496x26496, b=2048): 0.0444
Throughput (in TFLOP/s) for mlp_4h_to_h (4x26496x26496, b=2048): 259.245

Attention duration (in seconds): 0.0535
Attention throughput (in TFLOP/s): 223.442
MLP duration (in seconds): 0.0905
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1439
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26624x19968, b=2048): 0.0350
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26624x19968, b=2048): 248.581
Elapsed time for attention_key_query_prob (128x2048x208x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x208x2048): 134.939
Elapsed time for attention_prob_times_values (128x2048x2048x208): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x208): 184.073
Elapsed time for attention_linear_projection (4x6656x26624, b=2048): 0.0120
Throughput (in TFLOP/s) for attention_linear_projection (4x6656x26624, b=2048): 241.315
Elapsed time for mlp_h_to_4h (4x26624x26624, b=2048): 0.0446
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26624x26624, b=2048): 260.236
Elapsed time for mlp_4h_to_h (4x26624x26624, b=2048): 0.0446
Throughput (in TFLOP/s) for mlp_4h_to_h (4x26624x26624, b=2048): 260.411

Attention duration (in seconds): 0.0499
Attention throughput (in TFLOP/s): 241.497
MLP duration (in seconds): 0.0892
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1392
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26752x20064, b=2048): 0.0361
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26752x20064, b=2048): 243.570
Elapsed time for attention_key_query_prob (128x2048x209x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x209x2048): 64.527
Elapsed time for attention_prob_times_values (128x2048x2048x209): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x209): 75.192
Elapsed time for attention_linear_projection (4x6688x26752, b=2048): 0.0126
Throughput (in TFLOP/s) for attention_linear_projection (4x6688x26752, b=2048): 231.795
Elapsed time for mlp_h_to_4h (4x26752x26752, b=2048): 0.0449
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26752x26752, b=2048): 261.334
Elapsed time for mlp_4h_to_h (4x26752x26752, b=2048): 0.0596
Throughput (in TFLOP/s) for mlp_4h_to_h (4x26752x26752, b=2048): 196.704

Attention duration (in seconds): 0.0552
Attention throughput (in TFLOP/s): 220.494
MLP duration (in seconds): 0.1045
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1597
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26880x20160, b=2048): 0.0366
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26880x20160, b=2048): 242.820
Elapsed time for attention_key_query_prob (128x2048x210x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x210x2048): 92.242
Elapsed time for attention_prob_times_values (128x2048x2048x210): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x210): 123.580
Elapsed time for attention_linear_projection (4x6720x26880, b=2048): 0.0123
Throughput (in TFLOP/s) for attention_linear_projection (4x6720x26880, b=2048): 240.028
Elapsed time for mlp_h_to_4h (4x26880x26880, b=2048): 0.0457
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26880x26880, b=2048): 259.164
Elapsed time for mlp_4h_to_h (4x26880x26880, b=2048): 0.0457
Throughput (in TFLOP/s) for mlp_4h_to_h (4x26880x26880, b=2048): 258.877

Attention duration (in seconds): 0.0532
Attention throughput (in TFLOP/s): 231.156
MLP duration (in seconds): 0.0914
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1446
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27008x20256, b=2048): 0.0371
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27008x20256, b=2048): 241.510
Elapsed time for attention_key_query_prob (128x2048x211x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x211x2048): 64.578
Elapsed time for attention_prob_times_values (128x2048x2048x211): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x211): 77.244
Elapsed time for attention_linear_projection (4x6752x27008, b=2048): 0.0125
Throughput (in TFLOP/s) for attention_linear_projection (4x6752x27008, b=2048): 239.921
Elapsed time for mlp_h_to_4h (4x27008x27008, b=2048): 0.0460
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27008x27008, b=2048): 259.872
Elapsed time for mlp_4h_to_h (4x27008x27008, b=2048): 0.0460
Throughput (in TFLOP/s) for mlp_4h_to_h (4x27008x27008, b=2048): 259.828

Attention duration (in seconds): 0.0560
Attention throughput (in TFLOP/s): 221.471
MLP duration (in seconds): 0.0920
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1480
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27136x20352, b=2048): 0.0362
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27136x20352, b=2048): 249.632
Elapsed time for attention_key_query_prob (128x2048x212x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x212x2048): 93.248
Elapsed time for attention_prob_times_values (128x2048x2048x212): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x212): 123.100
Elapsed time for attention_linear_projection (4x6784x27136, b=2048): 0.0128
Throughput (in TFLOP/s) for attention_linear_projection (4x6784x27136, b=2048): 236.063
Elapsed time for mlp_h_to_4h (4x27136x27136, b=2048): 0.0462
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27136x27136, b=2048): 260.888
Elapsed time for mlp_4h_to_h (4x27136x27136, b=2048): 0.0468
Throughput (in TFLOP/s) for mlp_4h_to_h (4x27136x27136, b=2048): 257.587

Attention duration (in seconds): 0.0533
Attention throughput (in TFLOP/s): 234.831
MLP duration (in seconds): 0.0931
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1464
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27264x20448, b=2048): 0.0371
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27264x20448, b=2048): 245.969
Elapsed time for attention_key_query_prob (128x2048x213x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x213x2048): 64.789
Elapsed time for attention_prob_times_values (128x2048x2048x213): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x213): 77.673
Elapsed time for attention_linear_projection (4x6816x27264, b=2048): 0.0129
Throughput (in TFLOP/s) for attention_linear_projection (4x6816x27264, b=2048): 235.657
Elapsed time for mlp_h_to_4h (4x27264x27264, b=2048): 0.0471
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27264x27264, b=2048): 258.712
Elapsed time for mlp_4h_to_h (4x27264x27264, b=2048): 0.0471
Throughput (in TFLOP/s) for mlp_4h_to_h (4x27264x27264, b=2048): 258.691

Attention duration (in seconds): 0.0565
Attention throughput (in TFLOP/s): 223.532
MLP duration (in seconds): 0.0942
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1507
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27392x20544, b=2048): 0.0371
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27392x20544, b=2048): 248.447
Elapsed time for attention_key_query_prob (128x2048x214x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x214x2048): 94.608
Elapsed time for attention_prob_times_values (128x2048x2048x214): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x214): 123.942
Elapsed time for attention_linear_projection (4x6848x27392, b=2048): 0.0129
Throughput (in TFLOP/s) for attention_linear_projection (4x6848x27392, b=2048): 238.108
Elapsed time for mlp_h_to_4h (4x27392x27392, b=2048): 0.0473
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27392x27392, b=2048): 259.725
Elapsed time for mlp_4h_to_h (4x27392x27392, b=2048): 0.0473
Throughput (in TFLOP/s) for mlp_4h_to_h (4x27392x27392, b=2048): 259.810

Attention duration (in seconds): 0.0543
Attention throughput (in TFLOP/s): 234.857
MLP duration (in seconds): 0.0946
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1489
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27520x20640, b=2048): 0.0379
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27520x20640, b=2048): 245.364
Elapsed time for attention_key_query_prob (128x2048x215x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x215x2048): 64.915
Elapsed time for attention_prob_times_values (128x2048x2048x215): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x215): 76.933
Elapsed time for attention_linear_projection (4x6880x27520, b=2048): 0.0129
Throughput (in TFLOP/s) for attention_linear_projection (4x6880x27520, b=2048): 240.462
Elapsed time for mlp_h_to_4h (4x27520x27520, b=2048): 0.0478
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27520x27520, b=2048): 259.392
Elapsed time for mlp_4h_to_h (4x27520x27520, b=2048): 0.0476
Throughput (in TFLOP/s) for mlp_4h_to_h (4x27520x27520, b=2048): 260.536

Attention duration (in seconds): 0.0574
Attention throughput (in TFLOP/s): 224.273
MLP duration (in seconds): 0.0955
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1528
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27648x20736, b=2048): 0.0379
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27648x20736, b=2048): 247.719
Elapsed time for attention_key_query_prob (128x2048x216x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x216x2048): 124.827
Elapsed time for attention_prob_times_values (128x2048x2048x216): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x216): 186.893
Elapsed time for attention_linear_projection (4x6912x27648, b=2048): 0.0133
Throughput (in TFLOP/s) for attention_linear_projection (4x6912x27648, b=2048): 236.145
Elapsed time for mlp_h_to_4h (4x27648x27648, b=2048): 0.0488
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27648x27648, b=2048): 256.882
Elapsed time for mlp_4h_to_h (4x27648x27648, b=2048): 0.0485
Throughput (in TFLOP/s) for mlp_4h_to_h (4x27648x27648, b=2048): 258.375

Attention duration (in seconds): 0.0543
Attention throughput (in TFLOP/s): 239.294
MLP duration (in seconds): 0.0972
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1515
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27776x20832, b=2048): 0.0383
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27776x20832, b=2048): 247.798
Elapsed time for attention_key_query_prob (128x2048x217x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x217x2048): 65.664
Elapsed time for attention_prob_times_values (128x2048x2048x217): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x217): 76.951
Elapsed time for attention_linear_projection (4x6944x27776, b=2048): 0.0133
Throughput (in TFLOP/s) for attention_linear_projection (4x6944x27776, b=2048): 236.833
Elapsed time for mlp_h_to_4h (4x27776x27776, b=2048): 0.0487
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27776x27776, b=2048): 259.368
Elapsed time for mlp_4h_to_h (4x27776x27776, b=2048): 0.0488
Throughput (in TFLOP/s) for mlp_4h_to_h (4x27776x27776, b=2048): 259.226

Attention duration (in seconds): 0.0582
Attention throughput (in TFLOP/s): 225.282
MLP duration (in seconds): 0.0975
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1557
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27904x20928, b=2048): 0.0388
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27904x20928, b=2048): 246.600
Elapsed time for attention_key_query_prob (128x2048x218x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x218x2048): 95.542
Elapsed time for attention_prob_times_values (128x2048x2048x218): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x218): 126.307
Elapsed time for attention_linear_projection (4x6976x27904, b=2048): 0.0132
Throughput (in TFLOP/s) for attention_linear_projection (4x6976x27904, b=2048): 241.790
Elapsed time for mlp_h_to_4h (4x27904x27904, b=2048): 0.0490
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27904x27904, b=2048): 260.270
Elapsed time for mlp_4h_to_h (4x27904x27904, b=2048): 0.0490
Throughput (in TFLOP/s) for mlp_4h_to_h (4x27904x27904, b=2048): 260.358

Attention duration (in seconds): 0.0563
Attention throughput (in TFLOP/s): 234.938
MLP duration (in seconds): 0.0980
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1543
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28032x21024, b=2048): 0.0389
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28032x21024, b=2048): 248.356
Elapsed time for attention_key_query_prob (128x2048x219x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x219x2048): 65.530
Elapsed time for attention_prob_times_values (128x2048x2048x219): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x219): 80.441
Elapsed time for attention_linear_projection (4x7008x28032, b=2048): 0.0131
Throughput (in TFLOP/s) for attention_linear_projection (4x7008x28032, b=2048): 245.095
Elapsed time for mlp_h_to_4h (4x28032x28032, b=2048): 0.0505
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28032x28032, b=2048): 254.716
Elapsed time for mlp_4h_to_h (4x28032x28032, b=2048): 0.0664
Throughput (in TFLOP/s) for mlp_4h_to_h (4x28032x28032, b=2048): 193.955

Attention duration (in seconds): 0.0585
Attention throughput (in TFLOP/s): 228.027
MLP duration (in seconds): 0.1169
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1754
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28160x21120, b=2048): 0.0391
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28160x21120, b=2048): 248.935
Elapsed time for attention_key_query_prob (128x2048x220x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x220x2048): 96.503
Elapsed time for attention_prob_times_values (128x2048x2048x220): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x220): 127.139
Elapsed time for attention_linear_projection (4x7040x28160, b=2048): 0.0133
Throughput (in TFLOP/s) for attention_linear_projection (4x7040x28160, b=2048): 243.497
Elapsed time for mlp_h_to_4h (4x28160x28160, b=2048): 0.0497
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28160x28160, b=2048): 261.369
Elapsed time for mlp_4h_to_h (4x28160x28160, b=2048): 0.0501
Throughput (in TFLOP/s) for mlp_4h_to_h (4x28160x28160, b=2048): 259.104

Attention duration (in seconds): 0.0568
Attention throughput (in TFLOP/s): 237.102
MLP duration (in seconds): 0.0999
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1566
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28288x21216, b=2048): 0.0397
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28288x21216, b=2048): 247.375
Elapsed time for attention_key_query_prob (128x2048x221x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x221x2048): 67.236
Elapsed time for attention_prob_times_values (128x2048x2048x221): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x221): 81.215
Elapsed time for attention_linear_projection (4x7072x28288, b=2048): 0.0136
Throughput (in TFLOP/s) for attention_linear_projection (4x7072x28288, b=2048): 240.438
Elapsed time for mlp_h_to_4h (4x28288x28288, b=2048): 0.0504
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28288x28288, b=2048): 259.994
Elapsed time for mlp_4h_to_h (4x28288x28288, b=2048): 0.0507
Throughput (in TFLOP/s) for mlp_4h_to_h (4x28288x28288, b=2048): 258.634

Attention duration (in seconds): 0.0598
Attention throughput (in TFLOP/s): 227.054
MLP duration (in seconds): 0.1011
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1610
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28416x21312, b=2048): 0.0402
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28416x21312, b=2048): 247.015
Elapsed time for attention_key_query_prob (128x2048x222x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x222x2048): 97.962
Elapsed time for attention_prob_times_values (128x2048x2048x222): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x222): 129.257
Elapsed time for attention_linear_projection (4x7104x28416, b=2048): 0.0136
Throughput (in TFLOP/s) for attention_linear_projection (4x7104x28416, b=2048): 242.487
Elapsed time for mlp_h_to_4h (4x28416x28416, b=2048): 0.0507
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28416x28416, b=2048): 261.026
Elapsed time for mlp_4h_to_h (4x28416x28416, b=2048): 0.0507
Throughput (in TFLOP/s) for mlp_4h_to_h (4x28416x28416, b=2048): 261.109

Attention duration (in seconds): 0.0581
Attention throughput (in TFLOP/s): 235.969
MLP duration (in seconds): 0.1013
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1594
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28544x21408, b=2048): 0.0405
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28544x21408, b=2048): 247.390
Elapsed time for attention_key_query_prob (128x2048x223x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x223x2048): 68.287
Elapsed time for attention_prob_times_values (128x2048x2048x223): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x223): 80.454
Elapsed time for attention_linear_projection (4x7136x28544, b=2048): 0.0138
Throughput (in TFLOP/s) for attention_linear_projection (4x7136x28544, b=2048): 242.683
Elapsed time for mlp_h_to_4h (4x28544x28544, b=2048): 0.0519
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28544x28544, b=2048): 257.318
Elapsed time for mlp_4h_to_h (4x28544x28544, b=2048): 0.0515
Throughput (in TFLOP/s) for mlp_4h_to_h (4x28544x28544, b=2048): 259.150

Attention duration (in seconds): 0.0607
Attention throughput (in TFLOP/s): 227.793
MLP duration (in seconds): 0.1034
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1641
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28672x21504, b=2048): 0.0410
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28672x21504, b=2048): 246.574
Elapsed time for attention_key_query_prob (128x2048x224x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x224x2048): 148.310
Elapsed time for attention_prob_times_values (128x2048x2048x224): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x224): 196.610
Elapsed time for attention_linear_projection (4x7168x28672, b=2048): 0.0138
Throughput (in TFLOP/s) for attention_linear_projection (4x7168x28672, b=2048): 244.525
Elapsed time for mlp_h_to_4h (4x28672x28672, b=2048): 0.0524
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28672x28672, b=2048): 257.016
Elapsed time for mlp_4h_to_h (4x28672x28672, b=2048): 0.0525
Throughput (in TFLOP/s) for mlp_4h_to_h (4x28672x28672, b=2048): 256.588

Attention duration (in seconds): 0.0576
Attention throughput (in TFLOP/s): 242.256
MLP duration (in seconds): 0.1049
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1625
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28800x21600, b=2048): 0.0411
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28800x21600, b=2048): 248.169
Elapsed time for attention_key_query_prob (128x2048x225x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x225x2048): 66.238
Elapsed time for attention_prob_times_values (128x2048x2048x225): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x225): 80.903
Elapsed time for attention_linear_projection (4x7200x28800, b=2048): 0.0143
Throughput (in TFLOP/s) for attention_linear_projection (4x7200x28800, b=2048): 238.106
Elapsed time for mlp_h_to_4h (4x28800x28800, b=2048): 0.0521
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28800x28800, b=2048): 261.003
Elapsed time for mlp_4h_to_h (4x28800x28800, b=2048): 0.0522
Throughput (in TFLOP/s) for mlp_4h_to_h (4x28800x28800, b=2048): 260.517

Attention duration (in seconds): 0.0620
Attention throughput (in TFLOP/s): 227.084
MLP duration (in seconds): 0.1042
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1662
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28928x21696, b=2048): 0.0419
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28928x21696, b=2048): 245.561
Elapsed time for attention_key_query_prob (128x2048x226x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x226x2048): 93.583
Elapsed time for attention_prob_times_values (128x2048x2048x226): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x226): 131.568
Elapsed time for attention_linear_projection (4x7232x28928, b=2048): 0.0142
Throughput (in TFLOP/s) for attention_linear_projection (4x7232x28928, b=2048): 240.964
Elapsed time for mlp_h_to_4h (4x28928x28928, b=2048): 0.0528
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28928x28928, b=2048): 259.758
Elapsed time for mlp_4h_to_h (4x28928x28928, b=2048): 0.0531
Throughput (in TFLOP/s) for mlp_4h_to_h (4x28928x28928, b=2048): 258.083

Attention duration (in seconds): 0.0605
Attention throughput (in TFLOP/s): 234.498
MLP duration (in seconds): 0.1059
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1664
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29056x21792, b=2048): 0.0416
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29056x21792, b=2048): 249.380
Elapsed time for attention_key_query_prob (128x2048x227x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x227x2048): 66.161
Elapsed time for attention_prob_times_values (128x2048x2048x227): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x227): 83.190
Elapsed time for attention_linear_projection (4x7264x29056, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_linear_projection (4x7264x29056, b=2048): 240.087
Elapsed time for mlp_h_to_4h (4x29056x29056, b=2048): 0.0533
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29056x29056, b=2048): 259.541
Elapsed time for mlp_4h_to_h (4x29056x29056, b=2048): 0.0533
Throughput (in TFLOP/s) for mlp_4h_to_h (4x29056x29056, b=2048): 259.726

Attention duration (in seconds): 0.0626
Attention throughput (in TFLOP/s): 228.687
MLP duration (in seconds): 0.1066
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1692
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29184x21888, b=2048): 0.0425
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29184x21888, b=2048): 246.319
Elapsed time for attention_key_query_prob (128x2048x228x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x228x2048): 94.195
Elapsed time for attention_prob_times_values (128x2048x2048x228): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x228): 133.008
Elapsed time for attention_linear_projection (4x7296x29184, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_linear_projection (4x7296x29184, b=2048): 242.440
Elapsed time for mlp_h_to_4h (4x29184x29184, b=2048): 0.0536
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29184x29184, b=2048): 260.338
Elapsed time for mlp_4h_to_h (4x29184x29184, b=2048): 0.0539
Throughput (in TFLOP/s) for mlp_4h_to_h (4x29184x29184, b=2048): 259.119

Attention duration (in seconds): 0.0613
Attention throughput (in TFLOP/s): 235.560
MLP duration (in seconds): 0.1075
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1688
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29312x21984, b=2048): 0.0427
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29312x21984, b=2048): 247.477
Elapsed time for attention_key_query_prob (128x2048x229x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x229x2048): 66.268
Elapsed time for attention_prob_times_values (128x2048x2048x229): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x229): 83.501
Elapsed time for attention_linear_projection (4x7328x29312, b=2048): 0.0146
Throughput (in TFLOP/s) for attention_linear_projection (4x7328x29312, b=2048): 240.565
Elapsed time for mlp_h_to_4h (4x29312x29312, b=2048): 0.0539
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29312x29312, b=2048): 261.148
Elapsed time for mlp_4h_to_h (4x29312x29312, b=2048): 0.0545
Throughput (in TFLOP/s) for mlp_4h_to_h (4x29312x29312, b=2048): 258.177

Attention duration (in seconds): 0.0639
Attention throughput (in TFLOP/s): 227.830
MLP duration (in seconds): 0.1084
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1724
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29440x22080, b=2048): 0.0440
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29440x22080, b=2048): 242.227
Elapsed time for attention_key_query_prob (128x2048x230x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x230x2048): 94.709
Elapsed time for attention_prob_times_values (128x2048x2048x230): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x230): 134.088
Elapsed time for attention_linear_projection (4x7360x29440, b=2048): 0.0147
Throughput (in TFLOP/s) for attention_linear_projection (4x7360x29440, b=2048): 241.345
Elapsed time for mlp_h_to_4h (4x29440x29440, b=2048): 0.0548
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29440x29440, b=2048): 259.273
Elapsed time for mlp_4h_to_h (4x29440x29440, b=2048): 0.0553
Throughput (in TFLOP/s) for mlp_4h_to_h (4x29440x29440, b=2048): 256.970

Attention duration (in seconds): 0.0631
Attention throughput (in TFLOP/s): 232.773
MLP duration (in seconds): 0.1100
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1732
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29568x22176, b=2048): 0.0436
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29568x22176, b=2048): 246.397
Elapsed time for attention_key_query_prob (128x2048x231x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x231x2048): 66.679
Elapsed time for attention_prob_times_values (128x2048x2048x231): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x231): 82.429
Elapsed time for attention_linear_projection (4x7392x29568, b=2048): 0.0150
Throughput (in TFLOP/s) for attention_linear_projection (4x7392x29568, b=2048): 239.051
Elapsed time for mlp_h_to_4h (4x29568x29568, b=2048): 0.0551
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29568x29568, b=2048): 260.100
Elapsed time for mlp_4h_to_h (4x29568x29568, b=2048): 0.0553
Throughput (in TFLOP/s) for mlp_4h_to_h (4x29568x29568, b=2048): 258.802

Attention duration (in seconds): 0.0653
Attention throughput (in TFLOP/s): 226.921
MLP duration (in seconds): 0.1104
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1757
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29696x22272, b=2048): 0.0446
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29696x22272, b=2048): 243.157
Elapsed time for attention_key_query_prob (128x2048x232x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x232x2048): 129.938
Elapsed time for attention_prob_times_values (128x2048x2048x232): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x232): 198.789
Elapsed time for attention_linear_projection (4x7424x29696, b=2048): 0.0154
Throughput (in TFLOP/s) for attention_linear_projection (4x7424x29696, b=2048): 234.380
Elapsed time for mlp_h_to_4h (4x29696x29696, b=2048): 0.0571
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29696x29696, b=2048): 252.879
Elapsed time for mlp_4h_to_h (4x29696x29696, b=2048): 0.0555
Throughput (in TFLOP/s) for mlp_4h_to_h (4x29696x29696, b=2048): 260.452

Attention duration (in seconds): 0.0631
Attention throughput (in TFLOP/s): 236.697
MLP duration (in seconds): 0.1126
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1758
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29824x22368, b=2048): 0.0451
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29824x22368, b=2048): 242.446
Elapsed time for attention_key_query_prob (128x2048x233x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x233x2048): 67.085
Elapsed time for attention_prob_times_values (128x2048x2048x233): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x233): 82.580
Elapsed time for attention_linear_projection (4x7456x29824, b=2048): 0.0151
Throughput (in TFLOP/s) for attention_linear_projection (4x7456x29824, b=2048): 241.398
Elapsed time for mlp_h_to_4h (4x29824x29824, b=2048): 0.0575
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29824x29824, b=2048): 253.280
Elapsed time for mlp_4h_to_h (4x29824x29824, b=2048): 0.0563
Throughput (in TFLOP/s) for mlp_4h_to_h (4x29824x29824, b=2048): 258.987

Attention duration (in seconds): 0.0669
Attention throughput (in TFLOP/s): 225.203
MLP duration (in seconds): 0.1138
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1807
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29952x22464, b=2048): 0.0446
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29952x22464, b=2048): 247.075
Elapsed time for attention_key_query_prob (128x2048x234x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x234x2048): 96.843
Elapsed time for attention_prob_times_values (128x2048x2048x234): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x234): 135.212
Elapsed time for attention_linear_projection (4x7488x29952, b=2048): 0.0154
Throughput (in TFLOP/s) for attention_linear_projection (4x7488x29952, b=2048): 238.916
Elapsed time for mlp_h_to_4h (4x29952x29952, b=2048): 0.0569
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29952x29952, b=2048): 258.329
Elapsed time for mlp_4h_to_h (4x29952x29952, b=2048): 0.0566
Throughput (in TFLOP/s) for mlp_4h_to_h (4x29952x29952, b=2048): 259.842

Attention duration (in seconds): 0.0645
Attention throughput (in TFLOP/s): 235.855
MLP duration (in seconds): 0.1135
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1779
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30080x22560, b=2048): 0.0449
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30080x22560, b=2048): 247.387
Elapsed time for attention_key_query_prob (128x2048x235x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x235x2048): 67.817
Elapsed time for attention_prob_times_values (128x2048x2048x235): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x235): 85.849
Elapsed time for attention_linear_projection (4x7520x30080, b=2048): 0.0155
Throughput (in TFLOP/s) for attention_linear_projection (4x7520x30080, b=2048): 239.610
Elapsed time for mlp_h_to_4h (4x30080x30080, b=2048): 0.0571
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30080x30080, b=2048): 259.456
Elapsed time for mlp_4h_to_h (4x30080x30080, b=2048): 0.0612
Throughput (in TFLOP/s) for mlp_4h_to_h (4x30080x30080, b=2048): 242.359

Attention duration (in seconds): 0.0671
Attention throughput (in TFLOP/s): 228.552
MLP duration (in seconds): 0.1183
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1854
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30208x22656, b=2048): 0.0453
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30208x22656, b=2048): 247.274
Elapsed time for attention_key_query_prob (128x2048x236x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x236x2048): 97.011
Elapsed time for attention_prob_times_values (128x2048x2048x236): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x236): 136.123
Elapsed time for attention_linear_projection (4x7552x30208, b=2048): 0.0157
Throughput (in TFLOP/s) for attention_linear_projection (4x7552x30208, b=2048): 238.119
Elapsed time for mlp_h_to_4h (4x30208x30208, b=2048): 0.0575
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30208x30208, b=2048): 259.918
Elapsed time for mlp_4h_to_h (4x30208x30208, b=2048): 0.0571
Throughput (in TFLOP/s) for mlp_4h_to_h (4x30208x30208, b=2048): 261.616

Attention duration (in seconds): 0.0655
Attention throughput (in TFLOP/s): 235.932
MLP duration (in seconds): 0.1147
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1802
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30336x22752, b=2048): 0.0458
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30336x22752, b=2048): 247.175
Elapsed time for attention_key_query_prob (128x2048x237x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x237x2048): 68.698
Elapsed time for attention_prob_times_values (128x2048x2048x237): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x237): 86.636
Elapsed time for attention_linear_projection (4x7584x30336, b=2048): 0.0155
Throughput (in TFLOP/s) for attention_linear_projection (4x7584x30336, b=2048): 243.432
Elapsed time for mlp_h_to_4h (4x30336x30336, b=2048): 0.0581
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30336x30336, b=2048): 259.584
Elapsed time for mlp_4h_to_h (4x30336x30336, b=2048): 0.0588
Throughput (in TFLOP/s) for mlp_4h_to_h (4x30336x30336, b=2048): 256.459

Attention duration (in seconds): 0.0679
Attention throughput (in TFLOP/s): 229.634
MLP duration (in seconds): 0.1169
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1848
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30464x22848, b=2048): 0.0460
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30464x22848, b=2048): 248.082
Elapsed time for attention_key_query_prob (128x2048x238x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x238x2048): 98.075
Elapsed time for attention_prob_times_values (128x2048x2048x238): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x238): 138.161
Elapsed time for attention_linear_projection (4x7616x30464, b=2048): 0.0159
Throughput (in TFLOP/s) for attention_linear_projection (4x7616x30464, b=2048): 239.275
Elapsed time for mlp_h_to_4h (4x30464x30464, b=2048): 0.0584
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30464x30464, b=2048): 260.352
Elapsed time for mlp_4h_to_h (4x30464x30464, b=2048): 0.0591
Throughput (in TFLOP/s) for mlp_4h_to_h (4x30464x30464, b=2048): 257.135

Attention duration (in seconds): 0.0663
Attention throughput (in TFLOP/s): 237.011
MLP duration (in seconds): 0.1175
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1838
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30592x22944, b=2048): 0.0467
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30592x22944, b=2048): 246.009
Elapsed time for attention_key_query_prob (128x2048x239x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x239x2048): 68.501
Elapsed time for attention_prob_times_values (128x2048x2048x239): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x239): 85.588
Elapsed time for attention_linear_projection (4x7648x30592, b=2048): 0.0158
Throughput (in TFLOP/s) for attention_linear_projection (4x7648x30592, b=2048): 242.283
Elapsed time for mlp_h_to_4h (4x30592x30592, b=2048): 0.0607
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30592x30592, b=2048): 252.569
Elapsed time for mlp_4h_to_h (4x30592x30592, b=2048): 0.0713
Throughput (in TFLOP/s) for mlp_4h_to_h (4x30592x30592, b=2048): 215.028

Attention duration (in seconds): 0.0693
Attention throughput (in TFLOP/s): 228.624
MLP duration (in seconds): 0.1320
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2013
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30720x23040, b=2048): 0.0467
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30720x23040, b=2048): 248.443
Elapsed time for attention_key_query_prob (128x2048x240x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x240x2048): 146.122
Elapsed time for attention_prob_times_values (128x2048x2048x240): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x240): 205.409
Elapsed time for attention_linear_projection (4x7680x30720, b=2048): 0.0160
Throughput (in TFLOP/s) for attention_linear_projection (4x7680x30720, b=2048): 242.292
Elapsed time for mlp_h_to_4h (4x30720x30720, b=2048): 0.0599
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30720x30720, b=2048): 258.239
Elapsed time for mlp_4h_to_h (4x30720x30720, b=2048): 0.0606
Throughput (in TFLOP/s) for mlp_4h_to_h (4x30720x30720, b=2048): 254.990

Attention duration (in seconds): 0.0656
Attention throughput (in TFLOP/s): 243.377
MLP duration (in seconds): 0.1205
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1862
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30848x23136, b=2048): 0.0471
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30848x23136, b=2048): 248.463
Elapsed time for attention_key_query_prob (128x2048x241x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x241x2048): 68.982
Elapsed time for attention_prob_times_values (128x2048x2048x241): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x241): 86.086
Elapsed time for attention_linear_projection (4x7712x30848, b=2048): 0.0163
Throughput (in TFLOP/s) for attention_linear_projection (4x7712x30848, b=2048): 238.526
Elapsed time for mlp_h_to_4h (4x30848x30848, b=2048): 0.0601
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30848x30848, b=2048): 259.339
Elapsed time for mlp_4h_to_h (4x30848x30848, b=2048): 0.0608
Throughput (in TFLOP/s) for mlp_4h_to_h (4x30848x30848, b=2048): 256.375

Attention duration (in seconds): 0.0702
Attention throughput (in TFLOP/s): 229.595
MLP duration (in seconds): 0.1209
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1911
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30976x23232, b=2048): 0.0482
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30976x23232, b=2048): 244.843
Elapsed time for attention_key_query_prob (128x2048x242x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x242x2048): 99.650
Elapsed time for attention_prob_times_values (128x2048x2048x242): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x242): 140.919
Elapsed time for attention_linear_projection (4x7744x30976, b=2048): 0.0238
Throughput (in TFLOP/s) for attention_linear_projection (4x7744x30976, b=2048): 165.299
Elapsed time for mlp_h_to_4h (4x30976x30976, b=2048): 0.0605
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30976x30976, b=2048): 259.897
Elapsed time for mlp_4h_to_h (4x30976x30976, b=2048): 0.0606
Throughput (in TFLOP/s) for mlp_4h_to_h (4x30976x30976, b=2048): 259.216

Attention duration (in seconds): 0.0764
Attention throughput (in TFLOP/s): 212.618
MLP duration (in seconds): 0.1211
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1975
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31104x23328, b=2048): 0.0483
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31104x23328, b=2048): 246.318
Elapsed time for attention_key_query_prob (128x2048x243x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x243x2048): 69.799
Elapsed time for attention_prob_times_values (128x2048x2048x243): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x243): 88.786
Elapsed time for attention_linear_projection (4x7776x31104, b=2048): 0.0165
Throughput (in TFLOP/s) for attention_linear_projection (4x7776x31104, b=2048): 239.614
Elapsed time for mlp_h_to_4h (4x31104x31104, b=2048): 0.0613
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31104x31104, b=2048): 258.723
Elapsed time for mlp_4h_to_h (4x31104x31104, b=2048): 0.0613
Throughput (in TFLOP/s) for mlp_4h_to_h (4x31104x31104, b=2048): 258.663

Attention duration (in seconds): 0.0715
Attention throughput (in TFLOP/s): 229.059
MLP duration (in seconds): 0.1225
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1940
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31232x23424, b=2048): 0.0486
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31232x23424, b=2048): 246.562
Elapsed time for attention_key_query_prob (128x2048x244x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x244x2048): 100.759
Elapsed time for attention_prob_times_values (128x2048x2048x244): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x244): 142.526
Elapsed time for attention_linear_projection (4x7808x31232, b=2048): 0.0163
Throughput (in TFLOP/s) for attention_linear_projection (4x7808x31232, b=2048): 245.293
Elapsed time for mlp_h_to_4h (4x31232x31232, b=2048): 0.0629
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31232x31232, b=2048): 254.148
Elapsed time for mlp_4h_to_h (4x31232x31232, b=2048): 0.0615
Throughput (in TFLOP/s) for mlp_4h_to_h (4x31232x31232, b=2048): 259.889

Attention duration (in seconds): 0.0693
Attention throughput (in TFLOP/s): 238.038
MLP duration (in seconds): 0.1244
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1937
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31360x23520, b=2048): 0.0490
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31360x23520, b=2048): 246.409
Elapsed time for attention_key_query_prob (128x2048x245x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x245x2048): 70.630
Elapsed time for attention_prob_times_values (128x2048x2048x245): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x245): 89.742
Elapsed time for attention_linear_projection (4x7840x31360, b=2048): 0.0164
Throughput (in TFLOP/s) for attention_linear_projection (4x7840x31360, b=2048): 244.895
Elapsed time for mlp_h_to_4h (4x31360x31360, b=2048): 0.0620
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31360x31360, b=2048): 259.944
Elapsed time for mlp_4h_to_h (4x31360x31360, b=2048): 0.0620
Throughput (in TFLOP/s) for mlp_4h_to_h (4x31360x31360, b=2048): 259.937

Attention duration (in seconds): 0.0721
Attention throughput (in TFLOP/s): 230.624
MLP duration (in seconds): 0.1240
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1961
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31488x23616, b=2048): 0.0497
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31488x23616, b=2048): 245.368
Elapsed time for attention_key_query_prob (128x2048x246x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x246x2048): 101.557
Elapsed time for attention_prob_times_values (128x2048x2048x246): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x246): 143.583
Elapsed time for attention_linear_projection (4x7872x31488, b=2048): 0.0167
Throughput (in TFLOP/s) for attention_linear_projection (4x7872x31488, b=2048): 243.374
Elapsed time for mlp_h_to_4h (4x31488x31488, b=2048): 0.0629
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31488x31488, b=2048): 258.072
Elapsed time for mlp_4h_to_h (4x31488x31488, b=2048): 0.0622
Throughput (in TFLOP/s) for mlp_4h_to_h (4x31488x31488, b=2048): 261.326

Attention duration (in seconds): 0.0708
Attention throughput (in TFLOP/s): 236.968
MLP duration (in seconds): 0.1251
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1959
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31616x23712, b=2048): 0.0498
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31616x23712, b=2048): 246.476
Elapsed time for attention_key_query_prob (128x2048x247x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x247x2048): 70.165
Elapsed time for attention_prob_times_values (128x2048x2048x247): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x247): 89.521
Elapsed time for attention_linear_projection (4x7904x31616, b=2048): 0.0180
Throughput (in TFLOP/s) for attention_linear_projection (4x7904x31616, b=2048): 227.165
Elapsed time for mlp_h_to_4h (4x31616x31616, b=2048): 0.0634
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31616x31616, b=2048): 258.125
Elapsed time for mlp_4h_to_h (4x31616x31616, b=2048): 0.0633
Throughput (in TFLOP/s) for mlp_4h_to_h (4x31616x31616, b=2048): 258.826

Attention duration (in seconds): 0.0746
Attention throughput (in TFLOP/s): 226.644
MLP duration (in seconds): 0.1267
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2013
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31744x23808, b=2048): 0.0503
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31744x23808, b=2048): 246.365
Elapsed time for attention_key_query_prob (128x2048x248x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x248x2048): 129.076
Elapsed time for attention_prob_times_values (128x2048x2048x248): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x248): 209.982
Elapsed time for attention_linear_projection (4x7936x31744, b=2048): 0.0166
Throughput (in TFLOP/s) for attention_linear_projection (4x7936x31744, b=2048): 248.419
Elapsed time for mlp_h_to_4h (4x31744x31744, b=2048): 0.0636
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31744x31744, b=2048): 259.439
Elapsed time for mlp_4h_to_h (4x31744x31744, b=2048): 0.0640
Throughput (in TFLOP/s) for mlp_4h_to_h (4x31744x31744, b=2048): 257.985

Attention duration (in seconds): 0.0702
Attention throughput (in TFLOP/s): 242.748
MLP duration (in seconds): 0.1276
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1978
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31872x23904, b=2048): 0.0510
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31872x23904, b=2048): 244.662
Elapsed time for attention_key_query_prob (128x2048x249x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x249x2048): 70.764
Elapsed time for attention_prob_times_values (128x2048x2048x249): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x249): 90.304
Elapsed time for attention_linear_projection (4x7968x31872, b=2048): 0.0171
Throughput (in TFLOP/s) for attention_linear_projection (4x7968x31872, b=2048): 243.003
Elapsed time for mlp_h_to_4h (4x31872x31872, b=2048): 0.0637
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31872x31872, b=2048): 261.275
Elapsed time for mlp_4h_to_h (4x31872x31872, b=2048): 0.0661
Throughput (in TFLOP/s) for mlp_4h_to_h (4x31872x31872, b=2048): 251.939

Attention duration (in seconds): 0.0749
Attention throughput (in TFLOP/s): 229.405
MLP duration (in seconds): 0.1298
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32000x24000, b=2048): 0.0588
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32000x24000, b=2048): 213.904
Elapsed time for attention_key_query_prob (128x2048x250x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x250x2048): 103.598
Elapsed time for attention_prob_times_values (128x2048x2048x250): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x250): 148.145
Elapsed time for attention_linear_projection (4x8000x32000, b=2048): 0.0173
Throughput (in TFLOP/s) for attention_linear_projection (4x8000x32000, b=2048): 242.260
Elapsed time for mlp_h_to_4h (4x32000x32000, b=2048): 0.0648
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32000x32000, b=2048): 258.842
Elapsed time for mlp_4h_to_h (4x32000x32000, b=2048): 0.0649
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32000x32000, b=2048): 258.562

Attention duration (in seconds): 0.0805
Attention throughput (in TFLOP/s): 214.971
MLP duration (in seconds): 0.1297
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32128x24096, b=2048): 0.0518
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32128x24096, b=2048): 244.719
Elapsed time for attention_key_query_prob (128x2048x251x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x251x2048): 71.790
Elapsed time for attention_prob_times_values (128x2048x2048x251): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x251): 93.468
Elapsed time for attention_linear_projection (4x8032x32128, b=2048): 0.0175
Throughput (in TFLOP/s) for attention_linear_projection (4x8032x32128, b=2048): 241.594
Elapsed time for mlp_h_to_4h (4x32128x32128, b=2048): 0.0658
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32128x32128, b=2048): 257.089
Elapsed time for mlp_4h_to_h (4x32128x32128, b=2048): 0.0657
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32128x32128, b=2048): 257.264

Attention duration (in seconds): 0.0760
Attention throughput (in TFLOP/s): 229.712
MLP duration (in seconds): 0.1315
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32256x24192, b=2048): 0.0516
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32256x24192, b=2048): 247.826
Elapsed time for attention_key_query_prob (128x2048x252x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x252x2048): 104.658
Elapsed time for attention_prob_times_values (128x2048x2048x252): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x252): 146.971
Elapsed time for attention_linear_projection (4x8064x32256, b=2048): 0.0175
Throughput (in TFLOP/s) for attention_linear_projection (4x8064x32256, b=2048): 243.479
Elapsed time for mlp_h_to_4h (4x32256x32256, b=2048): 0.0660
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32256x32256, b=2048): 258.271
Elapsed time for mlp_4h_to_h (4x32256x32256, b=2048): 0.0653
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32256x32256, b=2048): 261.056

Attention duration (in seconds): 0.0735
Attention throughput (in TFLOP/s): 239.231
MLP duration (in seconds): 0.1313
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32384x24288, b=2048): 0.0522
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32384x24288, b=2048): 246.688
Elapsed time for attention_key_query_prob (128x2048x253x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x253x2048): 72.574
Elapsed time for attention_prob_times_values (128x2048x2048x253): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x253): 94.174
Elapsed time for attention_linear_projection (4x8096x32384, b=2048): 0.0178
Throughput (in TFLOP/s) for attention_linear_projection (4x8096x32384, b=2048): 241.071
Elapsed time for mlp_h_to_4h (4x32384x32384, b=2048): 0.0664
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32384x32384, b=2048): 258.702
Elapsed time for mlp_4h_to_h (4x32384x32384, b=2048): 0.0663
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32384x32384, b=2048): 258.975

Attention duration (in seconds): 0.0767
Attention throughput (in TFLOP/s): 231.147
MLP duration (in seconds): 0.1328
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32512x24384, b=2048): 0.0528
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32512x24384, b=2048): 245.943
Elapsed time for attention_key_query_prob (128x2048x254x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x254x2048): 106.154
Elapsed time for attention_prob_times_values (128x2048x2048x254): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x254): 149.629
Elapsed time for attention_linear_projection (4x8128x32512, b=2048): 0.0176
Throughput (in TFLOP/s) for attention_linear_projection (4x8128x32512, b=2048): 245.480
Elapsed time for mlp_h_to_4h (4x32512x32512, b=2048): 0.0670
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32512x32512, b=2048): 258.457
Elapsed time for mlp_4h_to_h (4x32512x32512, b=2048): 0.0694
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32512x32512, b=2048): 249.588

Attention duration (in seconds): 0.0748
Attention throughput (in TFLOP/s): 238.690
MLP duration (in seconds): 0.1364
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32640x24480, b=2048): 0.0537
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32640x24480, b=2048): 243.688
Elapsed time for attention_key_query_prob (128x2048x255x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x255x2048): 72.209
Elapsed time for attention_prob_times_values (128x2048x2048x255): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x255): 93.871
Elapsed time for attention_linear_projection (4x8160x32640, b=2048): 0.0182
Throughput (in TFLOP/s) for attention_linear_projection (4x8160x32640, b=2048): 240.104
Elapsed time for mlp_h_to_4h (4x32640x32640, b=2048): 0.0678
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32640x32640, b=2048): 257.492
Elapsed time for mlp_4h_to_h (4x32640x32640, b=2048): 0.0678
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32640x32640, b=2048): 257.518

Attention duration (in seconds): 0.0786
Attention throughput (in TFLOP/s): 229.028
MLP duration (in seconds): 0.1356
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8192x3072, b=2048): 0.0018
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8192x3072, b=2048): 232.475
Elapsed time for attention_key_query_prob (64x2048x64x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x64x2048): 57.646
Elapsed time for attention_prob_times_values (64x2048x2048x64): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x64): 77.149
Elapsed time for attention_linear_projection (4x1024x8192, b=2048): 0.0006
Throughput (in TFLOP/s) for attention_linear_projection (4x1024x8192, b=2048): 216.715
Elapsed time for mlp_h_to_4h (4x8192x4096, b=2048): 0.0023
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8192x4096, b=2048): 240.167
Elapsed time for mlp_4h_to_h (4x4096x8192, b=2048): 0.0024
Throughput (in TFLOP/s) for mlp_4h_to_h (4x4096x8192, b=2048): 233.267

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 179.310
MLP duration (in seconds): 0.0046
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8320x3120, b=2048): 0.0018
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8320x3120, b=2048): 232.000
Elapsed time for attention_key_query_prob (64x2048x65x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x65x2048): 26.695
Elapsed time for attention_prob_times_values (64x2048x2048x65): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x65): 37.958
Elapsed time for attention_linear_projection (4x1040x8320, b=2048): 0.0140
Throughput (in TFLOP/s) for attention_linear_projection (4x1040x8320, b=2048): 10.137
Elapsed time for mlp_h_to_4h (4x8320x4160, b=2048): 0.0023
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8320x4160, b=2048): 251.211
Elapsed time for mlp_4h_to_h (4x4160x8320, b=2048): 0.0023
Throughput (in TFLOP/s) for mlp_4h_to_h (4x4160x8320, b=2048): 246.498

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 35.293
MLP duration (in seconds): 0.0046
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8448x3168, b=2048): 0.0018
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8448x3168, b=2048): 241.772
Elapsed time for attention_key_query_prob (64x2048x66x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x66x2048): 19.711
Elapsed time for attention_prob_times_values (64x2048x2048x66): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x66): 65.848
Elapsed time for attention_linear_projection (4x1056x8448, b=2048): 0.0007
Throughput (in TFLOP/s) for attention_linear_projection (4x1056x8448, b=2048): 220.048
Elapsed time for mlp_h_to_4h (4x8448x4224, b=2048): 0.0025
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8448x4224, b=2048): 236.517
Elapsed time for mlp_4h_to_h (4x4224x8448, b=2048): 0.0023
Throughput (in TFLOP/s) for mlp_4h_to_h (4x4224x8448, b=2048): 250.021

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 136.179
MLP duration (in seconds): 0.0048
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8576x3216, b=2048): 0.0018
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8576x3216, b=2048): 244.936
Elapsed time for attention_key_query_prob (64x2048x67x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x67x2048): 27.411
Elapsed time for attention_prob_times_values (64x2048x2048x67): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x67): 39.269
Elapsed time for attention_linear_projection (4x1072x8576, b=2048): 0.0007
Throughput (in TFLOP/s) for attention_linear_projection (4x1072x8576, b=2048): 206.259
Elapsed time for mlp_h_to_4h (4x8576x4288, b=2048): 0.0027
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8576x4288, b=2048): 226.259
Elapsed time for mlp_4h_to_h (4x4288x8576, b=2048): 0.0024
Throughput (in TFLOP/s) for mlp_4h_to_h (4x4288x8576, b=2048): 253.063

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 140.409
MLP duration (in seconds): 0.0050
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8704x3264, b=2048): 0.0019
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8704x3264, b=2048): 238.727
Elapsed time for attention_key_query_prob (64x2048x68x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x68x2048): 18.944
Elapsed time for attention_prob_times_values (64x2048x2048x68): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x68): 65.916
Elapsed time for attention_linear_projection (4x1088x8704, b=2048): 0.0007
Throughput (in TFLOP/s) for attention_linear_projection (4x1088x8704, b=2048): 215.060
Elapsed time for mlp_h_to_4h (4x8704x4352, b=2048): 0.0028
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8704x4352, b=2048): 223.805
Elapsed time for mlp_4h_to_h (4x4352x8704, b=2048): 0.0027
Throughput (in TFLOP/s) for mlp_4h_to_h (4x4352x8704, b=2048): 233.397

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 134.629
MLP duration (in seconds): 0.0054
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8832x3312, b=2048): 0.0020
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8832x3312, b=2048): 238.821
Elapsed time for attention_key_query_prob (64x2048x69x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x69x2048): 27.549
Elapsed time for attention_prob_times_values (64x2048x2048x69): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x69): 39.667
Elapsed time for attention_linear_projection (4x1104x8832, b=2048): 0.0008
Throughput (in TFLOP/s) for attention_linear_projection (4x1104x8832, b=2048): 211.306
Elapsed time for mlp_h_to_4h (4x8832x4416, b=2048): 0.0026
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8832x4416, b=2048): 245.778
Elapsed time for mlp_4h_to_h (4x4416x8832, b=2048): 0.0026
Throughput (in TFLOP/s) for mlp_4h_to_h (4x4416x8832, b=2048): 247.160

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 141.450
MLP duration (in seconds): 0.0052
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8960x3360, b=2048): 0.0020
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8960x3360, b=2048): 242.537
Elapsed time for attention_key_query_prob (64x2048x70x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x70x2048): 19.875
Elapsed time for attention_prob_times_values (64x2048x2048x70): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x70): 68.652
Elapsed time for attention_linear_projection (4x1120x8960, b=2048): 0.0008
Throughput (in TFLOP/s) for attention_linear_projection (4x1120x8960, b=2048): 217.270
Elapsed time for mlp_h_to_4h (4x8960x4480, b=2048): 0.0027
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8960x4480, b=2048): 247.240
Elapsed time for mlp_4h_to_h (4x4480x8960, b=2048): 0.0027
Throughput (in TFLOP/s) for mlp_4h_to_h (4x4480x8960, b=2048): 248.040

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 140.154
MLP duration (in seconds): 0.0053
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9088x3408, b=2048): 0.0021
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9088x3408, b=2048): 244.024
Elapsed time for attention_key_query_prob (64x2048x71x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x71x2048): 27.302
Elapsed time for attention_prob_times_values (64x2048x2048x71): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x71): 40.942
Elapsed time for attention_linear_projection (4x1136x9088, b=2048): 0.0009
Throughput (in TFLOP/s) for attention_linear_projection (4x1136x9088, b=2048): 185.334
Elapsed time for mlp_h_to_4h (4x9088x4544, b=2048): 0.0027
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9088x4544, b=2048): 251.202
Elapsed time for mlp_4h_to_h (4x4544x9088, b=2048): 0.0028
Throughput (in TFLOP/s) for mlp_4h_to_h (4x4544x9088, b=2048): 241.046

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 141.526
MLP duration (in seconds): 0.0055
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9216x3456, b=2048): 0.0021
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9216x3456, b=2048): 247.009
Elapsed time for attention_key_query_prob (64x2048x72x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x72x2048): 65.693
Elapsed time for attention_prob_times_values (64x2048x2048x72): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x72): 78.742
Elapsed time for attention_linear_projection (4x1152x9216, b=2048): 0.0008
Throughput (in TFLOP/s) for attention_linear_projection (4x1152x9216, b=2048): 215.153
Elapsed time for mlp_h_to_4h (4x9216x4608, b=2048): 0.0034
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9216x4608, b=2048): 203.766
Elapsed time for mlp_4h_to_h (4x4608x9216, b=2048): 0.0029
Throughput (in TFLOP/s) for mlp_4h_to_h (4x4608x9216, b=2048): 240.648

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 193.253
MLP duration (in seconds): 0.0063
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9344x3504, b=2048): 0.0023
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9344x3504, b=2048): 237.464
Elapsed time for attention_key_query_prob (64x2048x73x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x73x2048): 27.139
Elapsed time for attention_prob_times_values (64x2048x2048x73): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x73): 41.647
Elapsed time for attention_linear_projection (4x1168x9344, b=2048): 0.0009
Throughput (in TFLOP/s) for attention_linear_projection (4x1168x9344, b=2048): 210.199
Elapsed time for mlp_h_to_4h (4x9344x4672, b=2048): 0.0028
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9344x4672, b=2048): 251.442
Elapsed time for mlp_4h_to_h (4x4672x9344, b=2048): 0.0029
Throughput (in TFLOP/s) for mlp_4h_to_h (4x4672x9344, b=2048): 244.356

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 144.432
MLP duration (in seconds): 0.0058
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9472x3552, b=2048): 0.0023
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9472x3552, b=2048): 243.398
Elapsed time for attention_key_query_prob (64x2048x74x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x74x2048): 44.036
Elapsed time for attention_prob_times_values (64x2048x2048x74): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x74): 71.150
Elapsed time for attention_linear_projection (4x1184x9472, b=2048): 0.0008
Throughput (in TFLOP/s) for attention_linear_projection (4x1184x9472, b=2048): 221.970
Elapsed time for mlp_h_to_4h (4x9472x4736, b=2048): 0.0030
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9472x4736, b=2048): 245.674
Elapsed time for mlp_4h_to_h (4x4736x9472, b=2048): 0.0030
Throughput (in TFLOP/s) for mlp_4h_to_h (4x4736x9472, b=2048): 247.906

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 178.875
MLP duration (in seconds): 0.0060
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9600x3600, b=2048): 0.0024
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9600x3600, b=2048): 235.283
Elapsed time for attention_key_query_prob (64x2048x75x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x75x2048): 23.587
Elapsed time for attention_prob_times_values (64x2048x2048x75): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x75): 39.285
Elapsed time for attention_linear_projection (4x1200x9600, b=2048): 0.0009
Throughput (in TFLOP/s) for attention_linear_projection (4x1200x9600, b=2048): 210.713
Elapsed time for mlp_h_to_4h (4x9600x4800, b=2048): 0.0031
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9600x4800, b=2048): 242.967
Elapsed time for mlp_4h_to_h (4x4800x9600, b=2048): 0.0031
Throughput (in TFLOP/s) for mlp_4h_to_h (4x4800x9600, b=2048): 244.600

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 138.458
MLP duration (in seconds): 0.0062
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9728x3648, b=2048): 0.0025
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9728x3648, b=2048): 236.629
Elapsed time for attention_key_query_prob (64x2048x76x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x76x2048): 50.572
Elapsed time for attention_prob_times_values (64x2048x2048x76): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x76): 73.011
Elapsed time for attention_linear_projection (4x1216x9728, b=2048): 0.0009
Throughput (in TFLOP/s) for attention_linear_projection (4x1216x9728, b=2048): 225.617
Elapsed time for mlp_h_to_4h (4x9728x4864, b=2048): 0.0031
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9728x4864, b=2048): 247.590
Elapsed time for mlp_4h_to_h (4x4864x9728, b=2048): 0.0031
Throughput (in TFLOP/s) for mlp_4h_to_h (4x4864x9728, b=2048): 248.517

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 183.015
MLP duration (in seconds): 0.0063
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9856x3696, b=2048): 0.0025
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9856x3696, b=2048): 241.584
Elapsed time for attention_key_query_prob (64x2048x77x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x77x2048): 29.358
Elapsed time for attention_prob_times_values (64x2048x2048x77): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x77): 44.018
Elapsed time for attention_linear_projection (4x1232x9856, b=2048): 0.0009
Throughput (in TFLOP/s) for attention_linear_projection (4x1232x9856, b=2048): 217.754
Elapsed time for mlp_h_to_4h (4x9856x4928, b=2048): 0.0033
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9856x4928, b=2048): 240.384
Elapsed time for mlp_4h_to_h (4x4928x9856, b=2048): 0.0032
Throughput (in TFLOP/s) for mlp_4h_to_h (4x4928x9856, b=2048): 247.368

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 153.272
MLP duration (in seconds): 0.0065
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9984x3744, b=2048): 0.0025
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9984x3744, b=2048): 243.760
Elapsed time for attention_key_query_prob (64x2048x78x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x78x2048): 51.965
Elapsed time for attention_prob_times_values (64x2048x2048x78): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x78): 74.519
Elapsed time for attention_linear_projection (4x1248x9984, b=2048): 0.0009
Throughput (in TFLOP/s) for attention_linear_projection (4x1248x9984, b=2048): 225.744
Elapsed time for mlp_h_to_4h (4x9984x4992, b=2048): 0.0033
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9984x4992, b=2048): 243.875
Elapsed time for mlp_4h_to_h (4x4992x9984, b=2048): 0.0033
Throughput (in TFLOP/s) for mlp_4h_to_h (4x4992x9984, b=2048): 244.502

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 188.174
MLP duration (in seconds): 0.0067
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10112x3792, b=2048): 0.0026
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10112x3792, b=2048): 243.017
Elapsed time for attention_key_query_prob (64x2048x79x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x79x2048): 29.144
Elapsed time for attention_prob_times_values (64x2048x2048x79): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x79): 44.798
Elapsed time for attention_linear_projection (4x1264x10112, b=2048): 0.0010
Throughput (in TFLOP/s) for attention_linear_projection (4x1264x10112, b=2048): 220.191
Elapsed time for mlp_h_to_4h (4x10112x5056, b=2048): 0.0034
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10112x5056, b=2048): 246.604
Elapsed time for mlp_4h_to_h (4x5056x10112, b=2048): 0.0038
Throughput (in TFLOP/s) for mlp_4h_to_h (4x5056x10112, b=2048): 220.939

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 155.344
MLP duration (in seconds): 0.0072
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10240x3840, b=2048): 0.0026
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10240x3840, b=2048): 247.927
Elapsed time for attention_key_query_prob (64x2048x80x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x80x2048): 75.978
Elapsed time for attention_prob_times_values (64x2048x2048x80): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x80): 87.110
Elapsed time for attention_linear_projection (4x1280x10240, b=2048): 0.0010
Throughput (in TFLOP/s) for attention_linear_projection (4x1280x10240, b=2048): 225.236
Elapsed time for mlp_h_to_4h (4x10240x5120, b=2048): 0.0035
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10240x5120, b=2048): 246.233
Elapsed time for mlp_4h_to_h (4x5120x10240, b=2048): 0.0034
Throughput (in TFLOP/s) for mlp_4h_to_h (4x5120x10240, b=2048): 250.200

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 204.953
MLP duration (in seconds): 0.0069
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10368x3888, b=2048): 0.0028
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10368x3888, b=2048): 238.066
Elapsed time for attention_key_query_prob (64x2048x81x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x81x2048): 29.896
Elapsed time for attention_prob_times_values (64x2048x2048x81): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x81): 45.633
Elapsed time for attention_linear_projection (4x1296x10368, b=2048): 0.0010
Throughput (in TFLOP/s) for attention_linear_projection (4x1296x10368, b=2048): 217.112
Elapsed time for mlp_h_to_4h (4x10368x5184, b=2048): 0.0036
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10368x5184, b=2048): 241.580
Elapsed time for mlp_4h_to_h (4x5184x10368, b=2048): 0.0035
Throughput (in TFLOP/s) for mlp_4h_to_h (4x5184x10368, b=2048): 250.119

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 156.167
MLP duration (in seconds): 0.0072
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10496x3936, b=2048): 0.0028
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10496x3936, b=2048): 243.124
Elapsed time for attention_key_query_prob (64x2048x82x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x82x2048): 54.101
Elapsed time for attention_prob_times_values (64x2048x2048x82): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x82): 77.388
Elapsed time for attention_linear_projection (4x1312x10496, b=2048): 0.0012
Throughput (in TFLOP/s) for attention_linear_projection (4x1312x10496, b=2048): 187.167
Elapsed time for mlp_h_to_4h (4x10496x5248, b=2048): 0.0037
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10496x5248, b=2048): 246.662
Elapsed time for mlp_4h_to_h (4x5248x10496, b=2048): 0.0037
Throughput (in TFLOP/s) for mlp_4h_to_h (4x5248x10496, b=2048): 245.590

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 184.385
MLP duration (in seconds): 0.0073
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10624x3984, b=2048): 0.0028
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10624x3984, b=2048): 243.910
Elapsed time for attention_key_query_prob (64x2048x83x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x83x2048): 30.730
Elapsed time for attention_prob_times_values (64x2048x2048x83): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x83): 47.340
Elapsed time for attention_linear_projection (4x1328x10624, b=2048): 0.0022
Throughput (in TFLOP/s) for attention_linear_projection (4x1328x10624, b=2048): 105.949
Elapsed time for mlp_h_to_4h (4x10624x5312, b=2048): 0.0037
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10624x5312, b=2048): 246.765
Elapsed time for mlp_4h_to_h (4x5312x10624, b=2048): 0.0037
Throughput (in TFLOP/s) for mlp_4h_to_h (4x5312x10624, b=2048): 247.474

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 136.693
MLP duration (in seconds): 0.0075
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10752x4032, b=2048): 0.0029
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10752x4032, b=2048): 245.479
Elapsed time for attention_key_query_prob (64x2048x84x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x84x2048): 54.731
Elapsed time for attention_prob_times_values (64x2048x2048x84): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x84): 79.143
Elapsed time for attention_linear_projection (4x1344x10752, b=2048): 0.0011
Throughput (in TFLOP/s) for attention_linear_projection (4x1344x10752, b=2048): 216.774
Elapsed time for mlp_h_to_4h (4x10752x5376, b=2048): 0.0038
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10752x5376, b=2048): 251.531
Elapsed time for mlp_4h_to_h (4x5376x10752, b=2048): 0.0038
Throughput (in TFLOP/s) for mlp_4h_to_h (4x5376x10752, b=2048): 247.843

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 192.815
MLP duration (in seconds): 0.0076
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10880x4080, b=2048): 0.0029
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10880x4080, b=2048): 248.007
Elapsed time for attention_key_query_prob (64x2048x85x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x85x2048): 31.032
Elapsed time for attention_prob_times_values (64x2048x2048x85): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x85): 48.555
Elapsed time for attention_linear_projection (4x1360x10880, b=2048): 0.0016
Throughput (in TFLOP/s) for attention_linear_projection (4x1360x10880, b=2048): 149.314
Elapsed time for mlp_h_to_4h (4x10880x5440, b=2048): 0.0040
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10880x5440, b=2048): 242.622
Elapsed time for mlp_4h_to_h (4x5440x10880, b=2048): 0.0040
Throughput (in TFLOP/s) for mlp_4h_to_h (4x5440x10880, b=2048): 244.621

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 152.297
MLP duration (in seconds): 0.0080
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11008x4128, b=2048): 0.0035
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11008x4128, b=2048): 215.759
Elapsed time for attention_key_query_prob (64x2048x86x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x86x2048): 56.542
Elapsed time for attention_prob_times_values (64x2048x2048x86): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x86): 81.061
Elapsed time for attention_linear_projection (4x1376x11008, b=2048): 0.0013
Throughput (in TFLOP/s) for attention_linear_projection (4x1376x11008, b=2048): 188.807
Elapsed time for mlp_h_to_4h (4x11008x5504, b=2048): 0.0040
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11008x5504, b=2048): 247.567
Elapsed time for mlp_4h_to_h (4x5504x11008, b=2048): 0.0040
Throughput (in TFLOP/s) for mlp_4h_to_h (4x5504x11008, b=2048): 250.005

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 176.391
MLP duration (in seconds): 0.0080
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11136x4176, b=2048): 0.0032
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11136x4176, b=2048): 241.862
Elapsed time for attention_key_query_prob (64x2048x87x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x87x2048): 31.156
Elapsed time for attention_prob_times_values (64x2048x2048x87): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x87): 49.001
Elapsed time for attention_linear_projection (4x1392x11136, b=2048): 0.0017
Throughput (in TFLOP/s) for attention_linear_projection (4x1392x11136, b=2048): 149.466
Elapsed time for mlp_h_to_4h (4x11136x5568, b=2048): 0.0042
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11136x5568, b=2048): 244.154
Elapsed time for mlp_4h_to_h (4x5568x11136, b=2048): 0.0041
Throughput (in TFLOP/s) for mlp_4h_to_h (4x5568x11136, b=2048): 250.307

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 151.923
MLP duration (in seconds): 0.0082
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11264x4224, b=2048): 0.0032
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11264x4224, b=2048): 245.984
Elapsed time for attention_key_query_prob (64x2048x88x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x88x2048): 74.440
Elapsed time for attention_prob_times_values (64x2048x2048x88): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x88): 93.339
Elapsed time for attention_linear_projection (4x1408x11264, b=2048): 0.0013
Throughput (in TFLOP/s) for attention_linear_projection (4x1408x11264, b=2048): 196.055
Elapsed time for mlp_h_to_4h (4x11264x5632, b=2048): 0.0042
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11264x5632, b=2048): 245.854
Elapsed time for mlp_4h_to_h (4x5632x11264, b=2048): 0.0042
Throughput (in TFLOP/s) for mlp_4h_to_h (4x5632x11264, b=2048): 245.771

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 201.210
MLP duration (in seconds): 0.0085
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11392x4272, b=2048): 0.0033
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11392x4272, b=2048): 239.053
Elapsed time for attention_key_query_prob (64x2048x89x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x89x2048): 31.826
Elapsed time for attention_prob_times_values (64x2048x2048x89): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x89): 49.878
Elapsed time for attention_linear_projection (4x1424x11392, b=2048): 0.0017
Throughput (in TFLOP/s) for attention_linear_projection (4x1424x11392, b=2048): 154.466
Elapsed time for mlp_h_to_4h (4x11392x5696, b=2048): 0.0043
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11392x5696, b=2048): 249.587
Elapsed time for mlp_4h_to_h (4x5696x11392, b=2048): 0.0043
Throughput (in TFLOP/s) for mlp_4h_to_h (4x5696x11392, b=2048): 247.153

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 154.176
MLP duration (in seconds): 0.0086
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11520x4320, b=2048): 0.0034
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11520x4320, b=2048): 241.827
Elapsed time for attention_key_query_prob (64x2048x90x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x90x2048): 59.467
Elapsed time for attention_prob_times_values (64x2048x2048x90): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x90): 71.942
Elapsed time for attention_linear_projection (4x1440x11520, b=2048): 0.0014
Throughput (in TFLOP/s) for attention_linear_projection (4x1440x11520, b=2048): 191.335
Elapsed time for mlp_h_to_4h (4x11520x5760, b=2048): 0.0044
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11520x5760, b=2048): 249.597
Elapsed time for mlp_4h_to_h (4x5760x11520, b=2048): 0.0043
Throughput (in TFLOP/s) for mlp_4h_to_h (4x5760x11520, b=2048): 251.886

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 188.612
MLP duration (in seconds): 0.0087
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11648x4368, b=2048): 0.0034
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11648x4368, b=2048): 245.185
Elapsed time for attention_key_query_prob (64x2048x91x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x91x2048): 32.490
Elapsed time for attention_prob_times_values (64x2048x2048x91): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x91): 51.707
Elapsed time for attention_linear_projection (4x1456x11648, b=2048): 0.0024
Throughput (in TFLOP/s) for attention_linear_projection (4x1456x11648, b=2048): 116.370
Elapsed time for mlp_h_to_4h (4x11648x5824, b=2048): 0.0045
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11648x5824, b=2048): 244.739
Elapsed time for mlp_4h_to_h (4x5824x11648, b=2048): 0.0045
Throughput (in TFLOP/s) for mlp_4h_to_h (4x5824x11648, b=2048): 249.627

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 146.812
MLP duration (in seconds): 0.0090
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11776x4416, b=2048): 0.0034
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11776x4416, b=2048): 248.409
Elapsed time for attention_key_query_prob (64x2048x92x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x92x2048): 37.253
Elapsed time for attention_prob_times_values (64x2048x2048x92): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x92): 87.008
Elapsed time for attention_linear_projection (4x1472x11776, b=2048): 0.0014
Throughput (in TFLOP/s) for attention_linear_projection (4x1472x11776, b=2048): 203.624
Elapsed time for mlp_h_to_4h (4x11776x5888, b=2048): 0.0045
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11776x5888, b=2048): 251.733
Elapsed time for mlp_4h_to_h (4x5888x11776, b=2048): 0.0046
Throughput (in TFLOP/s) for mlp_4h_to_h (4x5888x11776, b=2048): 247.214

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 183.801
MLP duration (in seconds): 0.0091
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11904x4464, b=2048): 0.0035
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11904x4464, b=2048): 247.624
Elapsed time for attention_key_query_prob (64x2048x93x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x93x2048): 33.021
Elapsed time for attention_prob_times_values (64x2048x2048x93): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x93): 52.763
Elapsed time for attention_linear_projection (4x1488x11904, b=2048): 0.0019
Throughput (in TFLOP/s) for attention_linear_projection (4x1488x11904, b=2048): 156.639
Elapsed time for mlp_h_to_4h (4x11904x5952, b=2048): 0.0046
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11904x5952, b=2048): 250.370
Elapsed time for mlp_4h_to_h (4x5952x11904, b=2048): 0.0047
Throughput (in TFLOP/s) for mlp_4h_to_h (4x5952x11904, b=2048): 249.613

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 161.071
MLP duration (in seconds): 0.0093
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12032x4512, b=2048): 0.0036
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12032x4512, b=2048): 247.458
Elapsed time for attention_key_query_prob (64x2048x94x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x94x2048): 35.266
Elapsed time for attention_prob_times_values (64x2048x2048x94): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x94): 88.713
Elapsed time for attention_linear_projection (4x1504x12032, b=2048): 0.0015
Throughput (in TFLOP/s) for attention_linear_projection (4x1504x12032, b=2048): 194.003
Elapsed time for mlp_h_to_4h (4x12032x6016, b=2048): 0.0047
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12032x6016, b=2048): 253.891
Elapsed time for mlp_4h_to_h (4x6016x12032, b=2048): 0.0047
Throughput (in TFLOP/s) for mlp_4h_to_h (4x6016x12032, b=2048): 250.389

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 180.678
MLP duration (in seconds): 0.0094
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12160x4560, b=2048): 0.0037
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12160x4560, b=2048): 248.530
Elapsed time for attention_key_query_prob (64x2048x95x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x95x2048): 24.800
Elapsed time for attention_prob_times_values (64x2048x2048x95): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x95): 53.122
Elapsed time for attention_linear_projection (4x1520x12160, b=2048): 0.0022
Throughput (in TFLOP/s) for attention_linear_projection (4x1520x12160, b=2048): 139.424
Elapsed time for mlp_h_to_4h (4x12160x6080, b=2048): 0.0050
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12160x6080, b=2048): 242.454
Elapsed time for mlp_4h_to_h (4x6080x12160, b=2048): 0.0053
Throughput (in TFLOP/s) for mlp_4h_to_h (4x6080x12160, b=2048): 229.074

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 148.496
MLP duration (in seconds): 0.0103
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12288x4608, b=2048): 0.0037
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12288x4608, b=2048): 251.039
Elapsed time for attention_key_query_prob (64x2048x96x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x96x2048): 76.413
Elapsed time for attention_prob_times_values (64x2048x2048x96): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x96): 102.161
Elapsed time for attention_linear_projection (4x1536x12288, b=2048): 0.0015
Throughput (in TFLOP/s) for attention_linear_projection (4x1536x12288, b=2048): 201.905
Elapsed time for mlp_h_to_4h (4x12288x6144, b=2048): 0.0051
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12288x6144, b=2048): 243.289
Elapsed time for mlp_4h_to_h (4x6144x12288, b=2048): 0.0050
Throughput (in TFLOP/s) for mlp_4h_to_h (4x6144x12288, b=2048): 246.784

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 209.181
MLP duration (in seconds): 0.0101
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12416x4656, b=2048): 0.0038
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12416x4656, b=2048): 249.645
Elapsed time for attention_key_query_prob (64x2048x97x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x97x2048): 31.414
Elapsed time for attention_prob_times_values (64x2048x2048x97): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x97): 54.416
Elapsed time for attention_linear_projection (4x1552x12416, b=2048): 0.0020
Throughput (in TFLOP/s) for attention_linear_projection (4x1552x12416, b=2048): 157.156
Elapsed time for mlp_h_to_4h (4x12416x6208, b=2048): 0.0050
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12416x6208, b=2048): 251.952
Elapsed time for mlp_4h_to_h (4x6208x12416, b=2048): 0.0050
Throughput (in TFLOP/s) for mlp_4h_to_h (4x6208x12416, b=2048): 251.367

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 162.399
MLP duration (in seconds): 0.0100
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12544x4704, b=2048): 0.0038
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12544x4704, b=2048): 253.734
Elapsed time for attention_key_query_prob (64x2048x98x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x98x2048): 58.365
Elapsed time for attention_prob_times_values (64x2048x2048x98): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x98): 92.488
Elapsed time for attention_linear_projection (4x1568x12544, b=2048): 0.0016
Throughput (in TFLOP/s) for attention_linear_projection (4x1568x12544, b=2048): 196.545
Elapsed time for mlp_h_to_4h (4x12544x6272, b=2048): 0.0051
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12544x6272, b=2048): 255.111
Elapsed time for mlp_4h_to_h (4x6272x12544, b=2048): 0.0052
Throughput (in TFLOP/s) for mlp_4h_to_h (4x6272x12544, b=2048): 245.563

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 201.479
MLP duration (in seconds): 0.0103
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12672x4752, b=2048): 0.0041
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12672x4752, b=2048): 240.420
Elapsed time for attention_key_query_prob (64x2048x99x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x99x2048): 30.902
Elapsed time for attention_prob_times_values (64x2048x2048x99): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x99): 55.676
Elapsed time for attention_linear_projection (4x1584x12672, b=2048): 0.0021
Throughput (in TFLOP/s) for attention_linear_projection (4x1584x12672, b=2048): 156.729
Elapsed time for mlp_h_to_4h (4x12672x6336, b=2048): 0.0051
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12672x6336, b=2048): 257.393
Elapsed time for mlp_4h_to_h (4x6336x12672, b=2048): 0.0053
Throughput (in TFLOP/s) for mlp_4h_to_h (4x6336x12672, b=2048): 249.999

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 160.171
MLP duration (in seconds): 0.0104
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12800x4800, b=2048): 0.0041
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12800x4800, b=2048): 243.758
Elapsed time for attention_key_query_prob (64x2048x100x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x100x2048): 59.430
Elapsed time for attention_prob_times_values (64x2048x2048x100): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x100): 93.088
Elapsed time for attention_linear_projection (4x1600x12800, b=2048): 0.0016
Throughput (in TFLOP/s) for attention_linear_projection (4x1600x12800, b=2048): 208.655
Elapsed time for mlp_h_to_4h (4x12800x6400, b=2048): 0.0053
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12800x6400, b=2048): 253.353
Elapsed time for mlp_4h_to_h (4x6400x12800, b=2048): 0.0053
Throughput (in TFLOP/s) for mlp_4h_to_h (4x6400x12800, b=2048): 252.523

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 200.828
MLP duration (in seconds): 0.0106
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12928x4848, b=2048): 0.0042
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12928x4848, b=2048): 245.259
Elapsed time for attention_key_query_prob (64x2048x101x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x101x2048): 30.847
Elapsed time for attention_prob_times_values (64x2048x2048x101): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x101): 56.943
Elapsed time for attention_linear_projection (4x1616x12928, b=2048): 0.0022
Throughput (in TFLOP/s) for attention_linear_projection (4x1616x12928, b=2048): 158.549
Elapsed time for mlp_h_to_4h (4x12928x6464, b=2048): 0.0057
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12928x6464, b=2048): 241.908
Elapsed time for mlp_4h_to_h (4x6464x12928, b=2048): 0.0054
Throughput (in TFLOP/s) for mlp_4h_to_h (4x6464x12928, b=2048): 253.360

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 163.166
MLP duration (in seconds): 0.0111
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x4896, b=2048): 0.0043
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x4896, b=2048): 245.745
Elapsed time for attention_key_query_prob (64x2048x102x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x102x2048): 60.205
Elapsed time for attention_prob_times_values (64x2048x2048x102): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x102): 96.750
Elapsed time for attention_linear_projection (4x1632x13056, b=2048): 0.0018
Throughput (in TFLOP/s) for attention_linear_projection (4x1632x13056, b=2048): 197.869
Elapsed time for mlp_h_to_4h (4x13056x6528, b=2048): 0.0060
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x6528, b=2048): 233.810
Elapsed time for mlp_4h_to_h (4x6528x13056, b=2048): 0.0056
Throughput (in TFLOP/s) for mlp_4h_to_h (4x6528x13056, b=2048): 249.062

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 200.747
MLP duration (in seconds): 0.0116
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13184x4944, b=2048): 0.0043
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13184x4944, b=2048): 249.221
Elapsed time for attention_key_query_prob (64x2048x103x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x103x2048): 30.498
Elapsed time for attention_prob_times_values (64x2048x2048x103): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x103): 55.341
Elapsed time for attention_linear_projection (4x1648x13184, b=2048): 0.0023
Throughput (in TFLOP/s) for attention_linear_projection (4x1648x13184, b=2048): 157.432
Elapsed time for mlp_h_to_4h (4x13184x6592, b=2048): 0.0056
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13184x6592, b=2048): 253.183
Elapsed time for mlp_4h_to_h (4x6592x13184, b=2048): 0.0061
Throughput (in TFLOP/s) for mlp_4h_to_h (4x6592x13184, b=2048): 233.852

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 163.967
MLP duration (in seconds): 0.0117
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13312x4992, b=2048): 0.0044
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13312x4992, b=2048): 250.227
Elapsed time for attention_key_query_prob (64x2048x104x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x104x2048): 75.691
Elapsed time for attention_prob_times_values (64x2048x2048x104): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x104): 109.331
Elapsed time for attention_linear_projection (4x1664x13312, b=2048): 0.0015
Throughput (in TFLOP/s) for attention_linear_projection (4x1664x13312, b=2048): 238.330
Elapsed time for mlp_h_to_4h (4x13312x6656, b=2048): 0.0057
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13312x6656, b=2048): 255.888
Elapsed time for mlp_4h_to_h (4x6656x13312, b=2048): 0.0057
Throughput (in TFLOP/s) for mlp_4h_to_h (4x6656x13312, b=2048): 252.996

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 219.504
MLP duration (in seconds): 0.0114
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13440x5040, b=2048): 0.0044
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13440x5040, b=2048): 251.507
Elapsed time for attention_key_query_prob (64x2048x105x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x105x2048): 30.379
Elapsed time for attention_prob_times_values (64x2048x2048x105): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x105): 58.467
Elapsed time for attention_linear_projection (4x1680x13440, b=2048): 0.0029
Throughput (in TFLOP/s) for attention_linear_projection (4x1680x13440, b=2048): 129.324
Elapsed time for mlp_h_to_4h (4x13440x6720, b=2048): 0.0057
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13440x6720, b=2048): 258.994
Elapsed time for mlp_4h_to_h (4x6720x13440, b=2048): 0.0059
Throughput (in TFLOP/s) for mlp_4h_to_h (4x6720x13440, b=2048): 251.124

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 157.782
MLP duration (in seconds): 0.0116
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13568x5088, b=2048): 0.0044
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13568x5088, b=2048): 254.301
Elapsed time for attention_key_query_prob (64x2048x106x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x106x2048): 62.094
Elapsed time for attention_prob_times_values (64x2048x2048x106): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x106): 98.470
Elapsed time for attention_linear_projection (4x1696x13568, b=2048): 0.0019
Throughput (in TFLOP/s) for attention_linear_projection (4x1696x13568, b=2048): 199.360
Elapsed time for mlp_h_to_4h (4x13568x6784, b=2048): 0.0058
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13568x6784, b=2048): 260.183
Elapsed time for mlp_4h_to_h (4x6784x13568, b=2048): 0.0067
Throughput (in TFLOP/s) for mlp_4h_to_h (4x6784x13568, b=2048): 223.525

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 207.052
MLP duration (in seconds): 0.0125
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13696x5136, b=2048): 0.0048
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13696x5136, b=2048): 240.901
Elapsed time for attention_key_query_prob (64x2048x107x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x107x2048): 30.969
Elapsed time for attention_prob_times_values (64x2048x2048x107): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x107): 60.100
Elapsed time for attention_linear_projection (4x1712x13696, b=2048): 0.0017
Throughput (in TFLOP/s) for attention_linear_projection (4x1712x13696, b=2048): 222.556
Elapsed time for mlp_h_to_4h (4x13696x6848, b=2048): 0.0061
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13696x6848, b=2048): 250.475
Elapsed time for mlp_4h_to_h (4x6848x13696, b=2048): 0.0073
Throughput (in TFLOP/s) for mlp_4h_to_h (4x6848x13696, b=2048): 211.014

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 177.186
MLP duration (in seconds): 0.0134
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13824x5184, b=2048): 0.0048
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13824x5184, b=2048): 242.619
Elapsed time for attention_key_query_prob (64x2048x108x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x108x2048): 62.792
Elapsed time for attention_prob_times_values (64x2048x2048x108): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x108): 102.355
Elapsed time for attention_linear_projection (4x1728x13824, b=2048): 0.0016
Throughput (in TFLOP/s) for attention_linear_projection (4x1728x13824, b=2048): 238.530
Elapsed time for mlp_h_to_4h (4x13824x6912, b=2048): 0.0061
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13824x6912, b=2048): 255.635
Elapsed time for mlp_4h_to_h (4x6912x13824, b=2048): 0.0062
Throughput (in TFLOP/s) for mlp_4h_to_h (4x6912x13824, b=2048): 253.817

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 210.974
MLP duration (in seconds): 0.0123
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13952x5232, b=2048): 0.0049
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13952x5232, b=2048): 243.238
Elapsed time for attention_key_query_prob (64x2048x109x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x109x2048): 32.156
Elapsed time for attention_prob_times_values (64x2048x2048x109): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x109): 60.996
Elapsed time for attention_linear_projection (4x1744x13952, b=2048): 0.0024
Throughput (in TFLOP/s) for attention_linear_projection (4x1744x13952, b=2048): 162.973
Elapsed time for mlp_h_to_4h (4x13952x6976, b=2048): 0.0063
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13952x6976, b=2048): 251.472
Elapsed time for mlp_4h_to_h (4x6976x13952, b=2048): 0.0064
Throughput (in TFLOP/s) for mlp_4h_to_h (4x6976x13952, b=2048): 248.520

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 168.766
MLP duration (in seconds): 0.0128
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14080x5280, b=2048): 0.0049
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14080x5280, b=2048): 247.051
Elapsed time for attention_key_query_prob (64x2048x110x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x110x2048): 64.220
Elapsed time for attention_prob_times_values (64x2048x2048x110): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x110): 104.470
Elapsed time for attention_linear_projection (4x1760x14080, b=2048): 0.0017
Throughput (in TFLOP/s) for attention_linear_projection (4x1760x14080, b=2048): 233.661
Elapsed time for mlp_h_to_4h (4x14080x7040, b=2048): 0.0066
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14080x7040, b=2048): 245.502
Elapsed time for mlp_4h_to_h (4x7040x14080, b=2048): 0.0067
Throughput (in TFLOP/s) for mlp_4h_to_h (4x7040x14080, b=2048): 242.383

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 213.689
MLP duration (in seconds): 0.0133
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14208x5328, b=2048): 0.0051
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14208x5328, b=2048): 245.011
Elapsed time for attention_key_query_prob (64x2048x111x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x111x2048): 32.584
Elapsed time for attention_prob_times_values (64x2048x2048x111): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x111): 61.640
Elapsed time for attention_linear_projection (4x1776x14208, b=2048): 0.0017
Throughput (in TFLOP/s) for attention_linear_projection (4x1776x14208, b=2048): 238.354
Elapsed time for mlp_h_to_4h (4x14208x7104, b=2048): 0.0067
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14208x7104, b=2048): 246.995
Elapsed time for mlp_4h_to_h (4x7104x14208, b=2048): 0.0067
Throughput (in TFLOP/s) for mlp_4h_to_h (4x7104x14208, b=2048): 247.250

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 184.824
MLP duration (in seconds): 0.0134
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14336x5376, b=2048): 0.0051
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14336x5376, b=2048): 249.811
Elapsed time for attention_key_query_prob (64x2048x112x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x112x2048): 81.671
Elapsed time for attention_prob_times_values (64x2048x2048x112): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x112): 119.982
Elapsed time for attention_linear_projection (4x1792x14336, b=2048): 0.0019
Throughput (in TFLOP/s) for attention_linear_projection (4x1792x14336, b=2048): 216.137
Elapsed time for mlp_h_to_4h (4x14336x7168, b=2048): 0.0068
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14336x7168, b=2048): 246.170
Elapsed time for mlp_4h_to_h (4x7168x14336, b=2048): 0.0068
Throughput (in TFLOP/s) for mlp_4h_to_h (4x7168x14336, b=2048): 248.956

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 218.931
MLP duration (in seconds): 0.0136
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14464x5424, b=2048): 0.0051
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14464x5424, b=2048): 251.012
Elapsed time for attention_key_query_prob (64x2048x113x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x113x2048): 33.063
Elapsed time for attention_prob_times_values (64x2048x2048x113): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x113): 62.259
Elapsed time for attention_linear_projection (4x1808x14464, b=2048): 0.0018
Throughput (in TFLOP/s) for attention_linear_projection (4x1808x14464, b=2048): 232.481
Elapsed time for mlp_h_to_4h (4x14464x7232, b=2048): 0.0071
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14464x7232, b=2048): 242.300
Elapsed time for mlp_4h_to_h (4x7232x14464, b=2048): 0.0574
Throughput (in TFLOP/s) for mlp_4h_to_h (4x7232x14464, b=2048): 29.842

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 187.778
MLP duration (in seconds): 0.0645
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0743
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14592x5472, b=2048): 0.0052
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14592x5472, b=2048): 250.414
Elapsed time for attention_key_query_prob (64x2048x114x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x114x2048): 65.805
Elapsed time for attention_prob_times_values (64x2048x2048x114): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x114): 105.814
Elapsed time for attention_linear_projection (4x1824x14592, b=2048): 0.0018
Throughput (in TFLOP/s) for attention_linear_projection (4x1824x14592, b=2048): 236.675
Elapsed time for mlp_h_to_4h (4x14592x7296, b=2048): 0.0072
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14592x7296, b=2048): 241.799
Elapsed time for mlp_4h_to_h (4x7296x14592, b=2048): 0.0072
Throughput (in TFLOP/s) for mlp_4h_to_h (4x7296x14592, b=2048): 241.010

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 217.686
MLP duration (in seconds): 0.0145
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14720x5520, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14720x5520, b=2048): 241.439
Elapsed time for attention_key_query_prob (64x2048x115x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x115x2048): 34.002
Elapsed time for attention_prob_times_values (64x2048x2048x115): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x115): 63.908
Elapsed time for attention_linear_projection (4x1840x14720, b=2048): 0.0019
Throughput (in TFLOP/s) for attention_linear_projection (4x1840x14720, b=2048): 234.179
Elapsed time for mlp_h_to_4h (4x14720x7360, b=2048): 0.0077
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14720x7360, b=2048): 230.667
Elapsed time for mlp_4h_to_h (4x7360x14720, b=2048): 0.0072
Throughput (in TFLOP/s) for mlp_4h_to_h (4x7360x14720, b=2048): 245.572

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 186.298
MLP duration (in seconds): 0.0149
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14848x5568, b=2048): 0.0056
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14848x5568, b=2048): 243.300
Elapsed time for attention_key_query_prob (64x2048x116x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x116x2048): 68.165
Elapsed time for attention_prob_times_values (64x2048x2048x116): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x116): 108.430
Elapsed time for attention_linear_projection (4x1856x14848, b=2048): 0.0019
Throughput (in TFLOP/s) for attention_linear_projection (4x1856x14848, b=2048): 240.417
Elapsed time for mlp_h_to_4h (4x14848x7424, b=2048): 0.0077
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14848x7424, b=2048): 235.821
Elapsed time for mlp_4h_to_h (4x7424x14848, b=2048): 0.0075
Throughput (in TFLOP/s) for mlp_4h_to_h (4x7424x14848, b=2048): 241.698

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 216.111
MLP duration (in seconds): 0.0151
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14976x5616, b=2048): 0.0056
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14976x5616, b=2048): 245.306
Elapsed time for attention_key_query_prob (64x2048x117x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x117x2048): 34.702
Elapsed time for attention_prob_times_values (64x2048x2048x117): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x117): 65.586
Elapsed time for attention_linear_projection (4x1872x14976, b=2048): 0.0020
Throughput (in TFLOP/s) for attention_linear_projection (4x1872x14976, b=2048): 234.631
Elapsed time for mlp_h_to_4h (4x14976x7488, b=2048): 0.0076
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14976x7488, b=2048): 242.999
Elapsed time for mlp_4h_to_h (4x7488x14976, b=2048): 0.0076
Throughput (in TFLOP/s) for mlp_4h_to_h (4x7488x14976, b=2048): 240.669

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 189.787
MLP duration (in seconds): 0.0152
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15104x5664, b=2048): 0.0056
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15104x5664, b=2048): 250.528
Elapsed time for attention_key_query_prob (64x2048x118x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x118x2048): 69.196
Elapsed time for attention_prob_times_values (64x2048x2048x118): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x118): 111.363
Elapsed time for attention_linear_projection (4x1888x15104, b=2048): 0.0020
Throughput (in TFLOP/s) for attention_linear_projection (4x1888x15104, b=2048): 232.184
Elapsed time for mlp_h_to_4h (4x15104x7552, b=2048): 0.0077
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15104x7552, b=2048): 242.154
Elapsed time for mlp_4h_to_h (4x7552x15104, b=2048): 0.0078
Throughput (in TFLOP/s) for mlp_4h_to_h (4x7552x15104, b=2048): 239.410

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 219.499
MLP duration (in seconds): 0.0155
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15232x5712, b=2048): 0.0058
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15232x5712, b=2048): 246.088
Elapsed time for attention_key_query_prob (64x2048x119x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x119x2048): 33.689
Elapsed time for attention_prob_times_values (64x2048x2048x119): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x119): 66.377
Elapsed time for attention_linear_projection (4x1904x15232, b=2048): 0.0020
Throughput (in TFLOP/s) for attention_linear_projection (4x1904x15232, b=2048): 237.713
Elapsed time for mlp_h_to_4h (4x15232x7616, b=2048): 0.0078
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15232x7616, b=2048): 244.658
Elapsed time for mlp_4h_to_h (4x7616x15232, b=2048): 0.0078
Throughput (in TFLOP/s) for mlp_4h_to_h (4x7616x15232, b=2048): 244.298

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 190.456
MLP duration (in seconds): 0.0155
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15360x5760, b=2048): 0.0058
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15360x5760, b=2048): 251.619
Elapsed time for attention_key_query_prob (64x2048x120x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x120x2048): 86.886
Elapsed time for attention_prob_times_values (64x2048x2048x120): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x120): 127.340
Elapsed time for attention_linear_projection (4x1920x15360, b=2048): 0.0020
Throughput (in TFLOP/s) for attention_linear_projection (4x1920x15360, b=2048): 241.149
Elapsed time for mlp_h_to_4h (4x15360x7680, b=2048): 0.0081
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15360x7680, b=2048): 237.365
Elapsed time for mlp_4h_to_h (4x7680x15360, b=2048): 0.0080
Throughput (in TFLOP/s) for mlp_4h_to_h (4x7680x15360, b=2048): 240.242

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 228.760
MLP duration (in seconds): 0.0162
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15488x5808, b=2048): 0.0059
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15488x5808, b=2048): 250.552
Elapsed time for attention_key_query_prob (64x2048x121x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x121x2048): 34.342
Elapsed time for attention_prob_times_values (64x2048x2048x121): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x121): 66.830
Elapsed time for attention_linear_projection (4x1936x15488, b=2048): 0.0021
Throughput (in TFLOP/s) for attention_linear_projection (4x1936x15488, b=2048): 235.706
Elapsed time for mlp_h_to_4h (4x15488x7744, b=2048): 0.0086
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15488x7744, b=2048): 229.663
Elapsed time for mlp_4h_to_h (4x7744x15488, b=2048): 0.0082
Throughput (in TFLOP/s) for mlp_4h_to_h (4x7744x15488, b=2048): 238.958

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 193.442
MLP duration (in seconds): 0.0168
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15616x5856, b=2048): 0.0063
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15616x5856, b=2048): 238.291
Elapsed time for attention_key_query_prob (64x2048x122x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x122x2048): 70.405
Elapsed time for attention_prob_times_values (64x2048x2048x122): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x122): 114.371
Elapsed time for attention_linear_projection (4x1952x15616, b=2048): 0.0021
Throughput (in TFLOP/s) for attention_linear_projection (4x1952x15616, b=2048): 235.258
Elapsed time for mlp_h_to_4h (4x15616x7808, b=2048): 0.0083
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15616x7808, b=2048): 240.767
Elapsed time for mlp_4h_to_h (4x7808x15616, b=2048): 0.0082
Throughput (in TFLOP/s) for mlp_4h_to_h (4x7808x15616, b=2048): 244.078

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 214.728
MLP duration (in seconds): 0.0165
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15744x5904, b=2048): 0.0062
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15744x5904, b=2048): 247.478
Elapsed time for attention_key_query_prob (64x2048x123x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x123x2048): 36.248
Elapsed time for attention_prob_times_values (64x2048x2048x123): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x123): 68.744
Elapsed time for attention_linear_projection (4x1968x15744, b=2048): 0.0021
Throughput (in TFLOP/s) for attention_linear_projection (4x1968x15744, b=2048): 237.901
Elapsed time for mlp_h_to_4h (4x15744x7872, b=2048): 0.0082
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15744x7872, b=2048): 248.334
Elapsed time for mlp_4h_to_h (4x7872x15744, b=2048): 0.0085
Throughput (in TFLOP/s) for mlp_4h_to_h (4x7872x15744, b=2048): 240.304

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 195.361
MLP duration (in seconds): 0.0166
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15872x5952, b=2048): 0.0062
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15872x5952, b=2048): 248.476
Elapsed time for attention_key_query_prob (64x2048x124x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x124x2048): 72.132
Elapsed time for attention_prob_times_values (64x2048x2048x124): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x124): 115.716
Elapsed time for attention_linear_projection (4x1984x15872, b=2048): 0.0021
Throughput (in TFLOP/s) for attention_linear_projection (4x1984x15872, b=2048): 243.308
Elapsed time for mlp_h_to_4h (4x15872x7936, b=2048): 0.0083
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15872x7936, b=2048): 248.305
Elapsed time for mlp_4h_to_h (4x7936x15872, b=2048): 0.0085
Throughput (in TFLOP/s) for mlp_4h_to_h (4x7936x15872, b=2048): 242.212

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 223.081
MLP duration (in seconds): 0.0168
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16000x6000, b=2048): 0.0062
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16000x6000, b=2048): 253.880
Elapsed time for attention_key_query_prob (64x2048x125x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x125x2048): 36.920
Elapsed time for attention_prob_times_values (64x2048x2048x125): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x125): 69.897
Elapsed time for attention_linear_projection (4x2000x16000, b=2048): 0.0022
Throughput (in TFLOP/s) for attention_linear_projection (4x2000x16000, b=2048): 234.437
Elapsed time for mlp_h_to_4h (4x16000x8000, b=2048): 0.0087
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16000x8000, b=2048): 240.949
Elapsed time for mlp_4h_to_h (4x8000x16000, b=2048): 0.0084
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8000x16000, b=2048): 249.811

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 199.061
MLP duration (in seconds): 0.0171
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16128x6048, b=2048): 0.0069
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16128x6048, b=2048): 232.632
Elapsed time for attention_key_query_prob (64x2048x126x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x126x2048): 72.994
Elapsed time for attention_prob_times_values (64x2048x2048x126): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x126): 116.329
Elapsed time for attention_linear_projection (4x2016x16128, b=2048): 0.0022
Throughput (in TFLOP/s) for attention_linear_projection (4x2016x16128, b=2048): 238.763
Elapsed time for mlp_h_to_4h (4x16128x8064, b=2048): 0.0090
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16128x8064, b=2048): 236.233
Elapsed time for mlp_4h_to_h (4x8064x16128, b=2048): 0.0087
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8064x16128, b=2048): 245.425

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 213.602
MLP duration (in seconds): 0.0177
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16256x6096, b=2048): 0.0070
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16256x6096, b=2048): 230.976
Elapsed time for attention_key_query_prob (64x2048x127x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x127x2048): 36.491
Elapsed time for attention_prob_times_values (64x2048x2048x127): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x127): 70.351
Elapsed time for attention_linear_projection (4x2032x16256, b=2048): 0.0023
Throughput (in TFLOP/s) for attention_linear_projection (4x2032x16256, b=2048): 239.220
Elapsed time for mlp_h_to_4h (4x16256x8128, b=2048): 0.0090
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16256x8128, b=2048): 241.132
Elapsed time for mlp_4h_to_h (4x8128x16256, b=2048): 0.0090
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8128x16256, b=2048): 241.369

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 189.719
MLP duration (in seconds): 0.0179
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16384x6144, b=2048): 0.0083
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16384x6144, b=2048): 198.153
Elapsed time for attention_key_query_prob (64x2048x128x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x128x2048): 92.649
Elapsed time for attention_prob_times_values (64x2048x2048x128): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x128): 134.373
Elapsed time for attention_linear_projection (4x2048x16384, b=2048): 0.0022
Throughput (in TFLOP/s) for attention_linear_projection (4x2048x16384, b=2048): 246.403
Elapsed time for mlp_h_to_4h (4x16384x8192, b=2048): 0.0089
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16384x8192, b=2048): 245.806
Elapsed time for mlp_4h_to_h (4x8192x16384, b=2048): 0.0092
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8192x16384, b=2048): 240.198

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 197.880
MLP duration (in seconds): 0.0181
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16512x6192, b=2048): 0.0070
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16512x6192, b=2048): 240.956
Elapsed time for attention_key_query_prob (64x2048x129x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x129x2048): 33.923
Elapsed time for attention_prob_times_values (64x2048x2048x129): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x129): 52.058
Elapsed time for attention_linear_projection (4x2064x16512, b=2048): 0.0024
Throughput (in TFLOP/s) for attention_linear_projection (4x2064x16512, b=2048): 235.402
Elapsed time for mlp_h_to_4h (4x16512x8256, b=2048): 0.0092
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16512x8256, b=2048): 242.043
Elapsed time for mlp_4h_to_h (4x8256x16512, b=2048): 0.0091
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8256x16512, b=2048): 245.275

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 186.832
MLP duration (in seconds): 0.0183
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0310
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16640x6240, b=2048): 0.0071
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16640x6240, b=2048): 239.942
Elapsed time for attention_key_query_prob (64x2048x130x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x130x2048): 69.815
Elapsed time for attention_prob_times_values (64x2048x2048x130): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x130): 89.823
Elapsed time for attention_linear_projection (4x2080x16640, b=2048): 0.0024
Throughput (in TFLOP/s) for attention_linear_projection (4x2080x16640, b=2048): 238.850
Elapsed time for mlp_h_to_4h (4x16640x8320, b=2048): 0.0093
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16640x8320, b=2048): 244.001
Elapsed time for mlp_4h_to_h (4x8320x16640, b=2048): 0.0092
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8320x16640, b=2048): 246.882

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 214.205
MLP duration (in seconds): 0.0185
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16768x6288, b=2048): 0.0071
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16768x6288, b=2048): 242.238
Elapsed time for attention_key_query_prob (64x2048x131x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x131x2048): 34.497
Elapsed time for attention_prob_times_values (64x2048x2048x131): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x131): 54.597
Elapsed time for attention_linear_projection (4x2096x16768, b=2048): 0.0024
Throughput (in TFLOP/s) for attention_linear_projection (4x2096x16768, b=2048): 240.342
Elapsed time for mlp_h_to_4h (4x16768x8384, b=2048): 0.0094
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16768x8384, b=2048): 245.903
Elapsed time for mlp_4h_to_h (4x8384x16768, b=2048): 0.0095
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8384x16768, b=2048): 242.709

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 190.132
MLP duration (in seconds): 0.0189
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0317
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16896x6336, b=2048): 0.0073
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16896x6336, b=2048): 241.233
Elapsed time for attention_key_query_prob (64x2048x132x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x132x2048): 70.653
Elapsed time for attention_prob_times_values (64x2048x2048x132): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x132): 89.963
Elapsed time for attention_linear_projection (4x2112x16896, b=2048): 0.0024
Throughput (in TFLOP/s) for attention_linear_projection (4x2112x16896, b=2048): 242.865
Elapsed time for mlp_h_to_4h (4x16896x8448, b=2048): 0.0096
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16896x8448, b=2048): 244.628
Elapsed time for mlp_4h_to_h (4x8448x16896, b=2048): 0.0095
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8448x16896, b=2048): 244.970

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 216.267
MLP duration (in seconds): 0.0191
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17024x6384, b=2048): 0.0073
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17024x6384, b=2048): 242.374
Elapsed time for attention_key_query_prob (64x2048x133x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x133x2048): 34.216
Elapsed time for attention_prob_times_values (64x2048x2048x133): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x133): 55.144
Elapsed time for attention_linear_projection (4x2128x17024, b=2048): 0.0025
Throughput (in TFLOP/s) for attention_linear_projection (4x2128x17024, b=2048): 237.367
Elapsed time for mlp_h_to_4h (4x17024x8512, b=2048): 0.0099
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17024x8512, b=2048): 240.375
Elapsed time for mlp_4h_to_h (4x8512x17024, b=2048): 0.0097
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8512x17024, b=2048): 243.896

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 190.264
MLP duration (in seconds): 0.0196
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0328
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17152x6432, b=2048): 0.0073
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17152x6432, b=2048): 247.301
Elapsed time for attention_key_query_prob (64x2048x134x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x134x2048): 71.283
Elapsed time for attention_prob_times_values (64x2048x2048x134): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x134): 89.220
Elapsed time for attention_linear_projection (4x2144x17152, b=2048): 0.0025
Throughput (in TFLOP/s) for attention_linear_projection (4x2144x17152, b=2048): 238.607
Elapsed time for mlp_h_to_4h (4x17152x8576, b=2048): 0.0102
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17152x8576, b=2048): 235.977
Elapsed time for mlp_4h_to_h (4x8576x17152, b=2048): 0.0103
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8576x17152, b=2048): 233.174

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 219.226
MLP duration (in seconds): 0.0205
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0322
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17280x6480, b=2048): 0.0080
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17280x6480, b=2048): 230.343
Elapsed time for attention_key_query_prob (64x2048x135x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x135x2048): 33.136
Elapsed time for attention_prob_times_values (64x2048x2048x135): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x135): 54.411
Elapsed time for attention_linear_projection (4x2160x17280, b=2048): 0.0025
Throughput (in TFLOP/s) for attention_linear_projection (4x2160x17280, b=2048): 241.338
Elapsed time for mlp_h_to_4h (4x17280x8640, b=2048): 0.0107
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17280x8640, b=2048): 228.921
Elapsed time for mlp_4h_to_h (4x8640x17280, b=2048): 0.0102
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8640x17280, b=2048): 238.855

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 184.841
MLP duration (in seconds): 0.0209
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0349
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17408x6528, b=2048): 0.0081
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17408x6528, b=2048): 229.171
Elapsed time for attention_key_query_prob (64x2048x136x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x136x2048): 89.624
Elapsed time for attention_prob_times_values (64x2048x2048x136): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x136): 117.606
Elapsed time for attention_linear_projection (4x2176x17408, b=2048): 0.0025
Throughput (in TFLOP/s) for attention_linear_projection (4x2176x17408, b=2048): 244.031
Elapsed time for mlp_h_to_4h (4x17408x8704, b=2048): 0.0106
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17408x8704, b=2048): 233.701
Elapsed time for mlp_4h_to_h (4x8704x17408, b=2048): 0.0105
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8704x17408, b=2048): 237.437

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 217.178
MLP duration (in seconds): 0.0211
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0332
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17536x6576, b=2048): 0.0076
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17536x6576, b=2048): 247.193
Elapsed time for attention_key_query_prob (64x2048x137x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x137x2048): 33.355
Elapsed time for attention_prob_times_values (64x2048x2048x137): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x137): 54.893
Elapsed time for attention_linear_projection (4x2192x17536, b=2048): 0.0026
Throughput (in TFLOP/s) for attention_linear_projection (4x2192x17536, b=2048): 237.823
Elapsed time for mlp_h_to_4h (4x17536x8768, b=2048): 0.0104
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17536x8768, b=2048): 242.179
Elapsed time for mlp_4h_to_h (4x8768x17536, b=2048): 0.0107
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8768x17536, b=2048): 236.212

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 192.697
MLP duration (in seconds): 0.0211
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0349
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17664x6624, b=2048): 0.0078
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17664x6624, b=2048): 245.710
Elapsed time for attention_key_query_prob (64x2048x138x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x138x2048): 72.486
Elapsed time for attention_prob_times_values (64x2048x2048x138): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x138): 89.812
Elapsed time for attention_linear_projection (4x2208x17664, b=2048): 0.0027
Throughput (in TFLOP/s) for attention_linear_projection (4x2208x17664, b=2048): 240.939
Elapsed time for mlp_h_to_4h (4x17664x8832, b=2048): 0.0105
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17664x8832, b=2048): 243.871
Elapsed time for mlp_4h_to_h (4x8832x17664, b=2048): 0.0105
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8832x17664, b=2048): 244.199

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 219.834
MLP duration (in seconds): 0.0209
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0332
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17792x6672, b=2048): 0.0082
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17792x6672, b=2048): 238.073
Elapsed time for attention_key_query_prob (64x2048x139x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x139x2048): 35.180
Elapsed time for attention_prob_times_values (64x2048x2048x139): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x139): 56.508
Elapsed time for attention_linear_projection (4x2224x17792, b=2048): 0.0027
Throughput (in TFLOP/s) for attention_linear_projection (4x2224x17792, b=2048): 240.743
Elapsed time for mlp_h_to_4h (4x17792x8896, b=2048): 0.0107
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17792x8896, b=2048): 242.644
Elapsed time for mlp_4h_to_h (4x8896x17792, b=2048): 0.0108
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8896x17792, b=2048): 239.983

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 191.725
MLP duration (in seconds): 0.0215
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0358
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17920x6720, b=2048): 0.0083
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17920x6720, b=2048): 237.634
Elapsed time for attention_key_query_prob (64x2048x140x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x140x2048): 73.760
Elapsed time for attention_prob_times_values (64x2048x2048x140): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x140): 90.227
Elapsed time for attention_linear_projection (4x2240x17920, b=2048): 0.0027
Throughput (in TFLOP/s) for attention_linear_projection (4x2240x17920, b=2048): 246.225
Elapsed time for mlp_h_to_4h (4x17920x8960, b=2048): 0.0109
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17920x8960, b=2048): 241.715
Elapsed time for mlp_4h_to_h (4x8960x17920, b=2048): 0.0111
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8960x17920, b=2048): 235.967

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 216.829
MLP duration (in seconds): 0.0220
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0349
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18048x6768, b=2048): 0.0084
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18048x6768, b=2048): 238.554
Elapsed time for attention_key_query_prob (64x2048x141x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x141x2048): 34.799
Elapsed time for attention_prob_times_values (64x2048x2048x141): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x141): 55.849
Elapsed time for attention_linear_projection (4x2256x18048, b=2048): 0.0028
Throughput (in TFLOP/s) for attention_linear_projection (4x2256x18048, b=2048): 239.658
Elapsed time for mlp_h_to_4h (4x18048x9024, b=2048): 0.0116
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18048x9024, b=2048): 230.849
Elapsed time for mlp_4h_to_h (4x9024x18048, b=2048): 0.0127
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9024x18048, b=2048): 210.499

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 191.776
MLP duration (in seconds): 0.0242
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0389
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18176x6816, b=2048): 0.0084
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18176x6816, b=2048): 240.440
Elapsed time for attention_key_query_prob (64x2048x142x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x142x2048): 74.779
Elapsed time for attention_prob_times_values (64x2048x2048x142): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x142): 90.275
Elapsed time for attention_linear_projection (4x2272x18176, b=2048): 0.0028
Throughput (in TFLOP/s) for attention_linear_projection (4x2272x18176, b=2048): 237.854
Elapsed time for mlp_h_to_4h (4x18176x9088, b=2048): 0.0116
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18176x9088, b=2048): 232.681
Elapsed time for mlp_4h_to_h (4x9088x18176, b=2048): 0.0116
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9088x18176, b=2048): 233.932

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 217.395
MLP duration (in seconds): 0.0232
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0364
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18304x6864, b=2048): 0.0086
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18304x6864, b=2048): 238.827
Elapsed time for attention_key_query_prob (64x2048x143x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x143x2048): 34.714
Elapsed time for attention_prob_times_values (64x2048x2048x143): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x143): 55.509
Elapsed time for attention_linear_projection (4x2288x18304, b=2048): 0.0028
Throughput (in TFLOP/s) for attention_linear_projection (4x2288x18304, b=2048): 242.455
Elapsed time for mlp_h_to_4h (4x18304x9152, b=2048): 0.0110
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18304x9152, b=2048): 250.610
Elapsed time for mlp_4h_to_h (4x9152x18304, b=2048): 0.0114
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9152x18304, b=2048): 241.641

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 192.649
MLP duration (in seconds): 0.0223
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0374
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18432x6912, b=2048): 0.0087
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18432x6912, b=2048): 240.568
Elapsed time for attention_key_query_prob (64x2048x144x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x144x2048): 90.803
Elapsed time for attention_prob_times_values (64x2048x2048x144): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x144): 129.860
Elapsed time for attention_linear_projection (4x2304x18432, b=2048): 0.0028
Throughput (in TFLOP/s) for attention_linear_projection (4x2304x18432, b=2048): 249.281
Elapsed time for mlp_h_to_4h (4x18432x9216, b=2048): 0.0110
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18432x9216, b=2048): 252.615
Elapsed time for mlp_4h_to_h (4x9216x18432, b=2048): 0.0115
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9216x18432, b=2048): 242.972

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 227.475
MLP duration (in seconds): 0.0225
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0354
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18560x6960, b=2048): 0.0085
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18560x6960, b=2048): 248.963
Elapsed time for attention_key_query_prob (64x2048x145x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x145x2048): 55.557
Elapsed time for attention_prob_times_values (64x2048x2048x145): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x145): 55.472
Elapsed time for attention_linear_projection (4x2320x18560, b=2048): 0.0029
Throughput (in TFLOP/s) for attention_linear_projection (4x2320x18560, b=2048): 241.020
Elapsed time for mlp_h_to_4h (4x18560x9280, b=2048): 0.0115
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18560x9280, b=2048): 244.844
Elapsed time for mlp_4h_to_h (4x9280x18560, b=2048): 0.0116
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9280x18560, b=2048): 242.447

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 209.211
MLP duration (in seconds): 0.0232
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0374
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18688x7008, b=2048): 0.0087
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18688x7008, b=2048): 245.630
Elapsed time for attention_key_query_prob (64x2048x146x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x146x2048): 76.032
Elapsed time for attention_prob_times_values (64x2048x2048x146): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x146): 89.581
Elapsed time for attention_linear_projection (4x2336x18688, b=2048): 0.0030
Throughput (in TFLOP/s) for attention_linear_projection (4x2336x18688, b=2048): 240.806
Elapsed time for mlp_h_to_4h (4x18688x9344, b=2048): 0.0117
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18688x9344, b=2048): 243.538
Elapsed time for mlp_4h_to_h (4x9344x18688, b=2048): 0.0116
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9344x18688, b=2048): 247.455

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 221.701
MLP duration (in seconds): 0.0233
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0369
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18816x7056, b=2048): 0.0089
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18816x7056, b=2048): 244.240
Elapsed time for attention_key_query_prob (64x2048x147x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x147x2048): 56.171
Elapsed time for attention_prob_times_values (64x2048x2048x147): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x147): 55.971
Elapsed time for attention_linear_projection (4x2352x18816, b=2048): 0.0030
Throughput (in TFLOP/s) for attention_linear_projection (4x2352x18816, b=2048): 241.518
Elapsed time for mlp_h_to_4h (4x18816x9408, b=2048): 0.0120
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18816x9408, b=2048): 241.893
Elapsed time for mlp_4h_to_h (4x9408x18816, b=2048): 0.0121
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9408x18816, b=2048): 239.276

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 207.708
MLP duration (in seconds): 0.0241
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0388
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18944x7104, b=2048): 0.0088
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18944x7104, b=2048): 251.493
Elapsed time for attention_key_query_prob (64x2048x148x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x148x2048): 77.612
Elapsed time for attention_prob_times_values (64x2048x2048x148): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x148): 93.352
Elapsed time for attention_linear_projection (4x2368x18944, b=2048): 0.0030
Throughput (in TFLOP/s) for attention_linear_projection (4x2368x18944, b=2048): 244.195
Elapsed time for mlp_h_to_4h (4x18944x9472, b=2048): 0.0121
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18944x9472, b=2048): 243.375
Elapsed time for mlp_4h_to_h (4x9472x18944, b=2048): 0.0123
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9472x18944, b=2048): 239.467

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 226.985
MLP duration (in seconds): 0.0244
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0380
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19072x7152, b=2048): 0.0092
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19072x7152, b=2048): 243.912
Elapsed time for attention_key_query_prob (64x2048x149x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x149x2048): 56.925
Elapsed time for attention_prob_times_values (64x2048x2048x149): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x149): 56.154
Elapsed time for attention_linear_projection (4x2384x19072, b=2048): 0.0031
Throughput (in TFLOP/s) for attention_linear_projection (4x2384x19072, b=2048): 237.569
Elapsed time for mlp_h_to_4h (4x19072x9536, b=2048): 0.0119
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19072x9536, b=2048): 250.016
Elapsed time for mlp_4h_to_h (4x9536x19072, b=2048): 0.0123
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9536x19072, b=2048): 242.436

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 207.547
MLP duration (in seconds): 0.0242
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0393
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19200x7200, b=2048): 0.0093
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19200x7200, b=2048): 242.348
Elapsed time for attention_key_query_prob (64x2048x150x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x150x2048): 79.029
Elapsed time for attention_prob_times_values (64x2048x2048x150): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x150): 94.455
Elapsed time for attention_linear_projection (4x2400x19200, b=2048): 0.0031
Throughput (in TFLOP/s) for attention_linear_projection (4x2400x19200, b=2048): 241.026
Elapsed time for mlp_h_to_4h (4x19200x9600, b=2048): 0.0120
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19200x9600, b=2048): 251.962
Elapsed time for mlp_4h_to_h (4x9600x19200, b=2048): 0.0125
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9600x19200, b=2048): 240.897

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 221.674
MLP duration (in seconds): 0.0245
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0389
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19328x7248, b=2048): 0.0095
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19328x7248, b=2048): 240.870
Elapsed time for attention_key_query_prob (64x2048x151x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x151x2048): 56.946
Elapsed time for attention_prob_times_values (64x2048x2048x151): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x151): 56.035
Elapsed time for attention_linear_projection (4x2416x19328, b=2048): 0.0031
Throughput (in TFLOP/s) for attention_linear_projection (4x2416x19328, b=2048): 243.324
Elapsed time for mlp_h_to_4h (4x19328x9664, b=2048): 0.0125
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19328x9664, b=2048): 244.157
Elapsed time for mlp_4h_to_h (4x9664x19328, b=2048): 0.0127
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9664x19328, b=2048): 240.394

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 207.318
MLP duration (in seconds): 0.0253
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0408
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19456x7296, b=2048): 0.0097
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19456x7296, b=2048): 239.876
Elapsed time for attention_key_query_prob (64x2048x152x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x152x2048): 98.666
Elapsed time for attention_prob_times_values (64x2048x2048x152): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x152): 134.172
Elapsed time for attention_linear_projection (4x2432x19456, b=2048): 0.0032
Throughput (in TFLOP/s) for attention_linear_projection (4x2432x19456, b=2048): 243.931
Elapsed time for mlp_h_to_4h (4x19456x9728, b=2048): 0.0126
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19456x9728, b=2048): 245.793
Elapsed time for mlp_4h_to_h (4x9728x19456, b=2048): 0.0128
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9728x19456, b=2048): 242.385

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 228.122
MLP duration (in seconds): 0.0254
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0397
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19584x7344, b=2048): 0.0106
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19584x7344, b=2048): 222.028
Elapsed time for attention_key_query_prob (64x2048x153x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x153x2048): 57.306
Elapsed time for attention_prob_times_values (64x2048x2048x153): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x153): 56.157
Elapsed time for attention_linear_projection (4x2448x19584, b=2048): 0.0033
Throughput (in TFLOP/s) for attention_linear_projection (4x2448x19584, b=2048): 239.794
Elapsed time for mlp_h_to_4h (4x19584x9792, b=2048): 0.0130
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19584x9792, b=2048): 241.547
Elapsed time for mlp_4h_to_h (4x9792x19584, b=2048): 0.0132
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9792x19584, b=2048): 237.212

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 196.974
MLP duration (in seconds): 0.0263
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0430
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19712x7392, b=2048): 0.0105
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19712x7392, b=2048): 227.093
Elapsed time for attention_key_query_prob (64x2048x154x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x154x2048): 79.701
Elapsed time for attention_prob_times_values (64x2048x2048x154): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x154): 95.399
Elapsed time for attention_linear_projection (4x2464x19712, b=2048): 0.0033
Throughput (in TFLOP/s) for attention_linear_projection (4x2464x19712, b=2048): 243.062
Elapsed time for mlp_h_to_4h (4x19712x9856, b=2048): 0.0133
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19712x9856, b=2048): 239.594
Elapsed time for mlp_4h_to_h (4x9856x19712, b=2048): 0.0131
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9856x19712, b=2048): 242.801

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 213.406
MLP duration (in seconds): 0.0264
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0421
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19840x7440, b=2048): 0.0097
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19840x7440, b=2048): 248.900
Elapsed time for attention_key_query_prob (64x2048x155x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x155x2048): 57.653
Elapsed time for attention_prob_times_values (64x2048x2048x155): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x155): 58.958
Elapsed time for attention_linear_projection (4x2480x19840, b=2048): 0.0033
Throughput (in TFLOP/s) for attention_linear_projection (4x2480x19840, b=2048): 245.800
Elapsed time for mlp_h_to_4h (4x19840x9920, b=2048): 0.0128
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19840x9920, b=2048): 252.664
Elapsed time for mlp_4h_to_h (4x9920x19840, b=2048): 0.0136
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9920x19840, b=2048): 237.624

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 213.930
MLP duration (in seconds): 0.0263
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0422
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19968x7488, b=2048): 0.0101
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19968x7488, b=2048): 241.491
Elapsed time for attention_key_query_prob (64x2048x156x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x156x2048): 80.238
Elapsed time for attention_prob_times_values (64x2048x2048x156): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x156): 96.745
Elapsed time for attention_linear_projection (4x2496x19968, b=2048): 0.0033
Throughput (in TFLOP/s) for attention_linear_projection (4x2496x19968, b=2048): 247.113
Elapsed time for mlp_h_to_4h (4x19968x9984, b=2048): 0.0130
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19968x9984, b=2048): 251.366
Elapsed time for mlp_4h_to_h (4x9984x19968, b=2048): 0.0136
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9984x19968, b=2048): 240.194

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 223.582
MLP duration (in seconds): 0.0266
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0420
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20096x7536, b=2048): 0.0104
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20096x7536, b=2048): 237.492
Elapsed time for attention_key_query_prob (64x2048x157x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x157x2048): 58.195
Elapsed time for attention_prob_times_values (64x2048x2048x157): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x157): 59.467
Elapsed time for attention_linear_projection (4x2512x20096, b=2048): 0.0034
Throughput (in TFLOP/s) for attention_linear_projection (4x2512x20096, b=2048): 241.408
Elapsed time for mlp_h_to_4h (4x20096x10048, b=2048): 0.0134
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20096x10048, b=2048): 246.635
Elapsed time for mlp_4h_to_h (4x10048x20096, b=2048): 0.0138
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10048x20096, b=2048): 240.072

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 207.706
MLP duration (in seconds): 0.0272
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0439
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20224x7584, b=2048): 0.0104
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20224x7584, b=2048): 242.045
Elapsed time for attention_key_query_prob (64x2048x158x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x158x2048): 82.396
Elapsed time for attention_prob_times_values (64x2048x2048x158): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x158): 98.473
Elapsed time for attention_linear_projection (4x2528x20224, b=2048): 0.0034
Throughput (in TFLOP/s) for attention_linear_projection (4x2528x20224, b=2048): 244.664
Elapsed time for mlp_h_to_4h (4x20224x10112, b=2048): 0.0135
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20224x10112, b=2048): 248.010
Elapsed time for mlp_4h_to_h (4x10112x20224, b=2048): 0.0141
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10112x20224, b=2048): 237.836

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 224.267
MLP duration (in seconds): 0.0276
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0433
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20352x7632, b=2048): 0.0105
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20352x7632, b=2048): 243.059
Elapsed time for attention_key_query_prob (64x2048x159x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x159x2048): 59.140
Elapsed time for attention_prob_times_values (64x2048x2048x159): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x159): 58.917
Elapsed time for attention_linear_projection (4x2544x20352, b=2048): 0.0035
Throughput (in TFLOP/s) for attention_linear_projection (4x2544x20352, b=2048): 242.535
Elapsed time for mlp_h_to_4h (4x20352x10176, b=2048): 0.0139
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20352x10176, b=2048): 243.414
Elapsed time for mlp_4h_to_h (4x10176x20352, b=2048): 0.0141
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10176x20352, b=2048): 240.510

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 211.381
MLP duration (in seconds): 0.0280
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0449
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20480x7680, b=2048): 0.0105
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20480x7680, b=2048): 244.285
Elapsed time for attention_key_query_prob (64x2048x160x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x160x2048): 104.401
Elapsed time for attention_prob_times_values (64x2048x2048x160): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x160): 139.701
Elapsed time for attention_linear_projection (4x2560x20480, b=2048): 0.0035
Throughput (in TFLOP/s) for attention_linear_projection (4x2560x20480, b=2048): 245.545
Elapsed time for mlp_h_to_4h (4x20480x10240, b=2048): 0.0142
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20480x10240, b=2048): 242.606
Elapsed time for mlp_4h_to_h (4x10240x20480, b=2048): 0.0148
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10240x20480, b=2048): 232.883

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 232.984
MLP duration (in seconds): 0.0289
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0444
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20608x7728, b=2048): 0.0107
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20608x7728, b=2048): 242.799
Elapsed time for attention_key_query_prob (64x2048x161x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x161x2048): 57.030
Elapsed time for attention_prob_times_values (64x2048x2048x161): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x161): 58.277
Elapsed time for attention_linear_projection (4x2576x20608, b=2048): 0.0036
Throughput (in TFLOP/s) for attention_linear_projection (4x2576x20608, b=2048): 243.577
Elapsed time for mlp_h_to_4h (4x20608x10304, b=2048): 0.0145
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20608x10304, b=2048): 239.968
Elapsed time for mlp_4h_to_h (4x10304x20608, b=2048): 0.0146
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10304x20608, b=2048): 238.521

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 210.895
MLP duration (in seconds): 0.0291
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0464
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20736x7776, b=2048): 0.0107
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20736x7776, b=2048): 246.997
Elapsed time for attention_key_query_prob (64x2048x162x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x162x2048): 78.721
Elapsed time for attention_prob_times_values (64x2048x2048x162): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x162): 99.398
Elapsed time for attention_linear_projection (4x2592x20736, b=2048): 0.0036
Throughput (in TFLOP/s) for attention_linear_projection (4x2592x20736, b=2048): 242.978
Elapsed time for mlp_h_to_4h (4x20736x10368, b=2048): 0.0201
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20736x10368, b=2048): 174.872
Elapsed time for mlp_4h_to_h (4x10368x20736, b=2048): 0.0146
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10368x20736, b=2048): 241.091

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 226.774
MLP duration (in seconds): 0.0348
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0511
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20864x7824, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20864x7824, b=2048): 248.373
Elapsed time for attention_key_query_prob (64x2048x163x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x163x2048): 56.765
Elapsed time for attention_prob_times_values (64x2048x2048x163): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x163): 60.598
Elapsed time for attention_linear_projection (4x2608x20864, b=2048): 0.0037
Throughput (in TFLOP/s) for attention_linear_projection (4x2608x20864, b=2048): 241.227
Elapsed time for mlp_h_to_4h (4x20864x10432, b=2048): 0.0148
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20864x10432, b=2048): 240.877
Elapsed time for mlp_4h_to_h (4x10432x20864, b=2048): 0.0150
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10432x20864, b=2048): 238.154

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 214.392
MLP duration (in seconds): 0.0298
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0472
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20992x7872, b=2048): 0.0111
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20992x7872, b=2048): 243.986
Elapsed time for attention_key_query_prob (64x2048x164x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x164x2048): 78.557
Elapsed time for attention_prob_times_values (64x2048x2048x164): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x164): 102.582
Elapsed time for attention_linear_projection (4x2624x20992, b=2048): 0.0036
Throughput (in TFLOP/s) for attention_linear_projection (4x2624x20992, b=2048): 247.274
Elapsed time for mlp_h_to_4h (4x20992x10496, b=2048): 0.0144
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20992x10496, b=2048): 250.419
Elapsed time for mlp_4h_to_h (4x10496x20992, b=2048): 0.0150
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10496x20992, b=2048): 240.909

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 226.361
MLP duration (in seconds): 0.0294
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0461
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21120x7920, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21120x7920, b=2048): 248.460
Elapsed time for attention_key_query_prob (64x2048x165x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x165x2048): 57.488
Elapsed time for attention_prob_times_values (64x2048x2048x165): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x165): 61.647
Elapsed time for attention_linear_projection (4x2640x21120, b=2048): 0.0037
Throughput (in TFLOP/s) for attention_linear_projection (4x2640x21120, b=2048): 244.908
Elapsed time for mlp_h_to_4h (4x21120x10560, b=2048): 0.0147
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21120x10560, b=2048): 248.719
Elapsed time for mlp_4h_to_h (4x10560x21120, b=2048): 0.0164
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10560x21120, b=2048): 222.249

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 215.990
MLP duration (in seconds): 0.0311
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0489
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21248x7968, b=2048): 0.0113
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21248x7968, b=2048): 246.420
Elapsed time for attention_key_query_prob (64x2048x166x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x166x2048): 80.163
Elapsed time for attention_prob_times_values (64x2048x2048x166): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x166): 103.631
Elapsed time for attention_linear_projection (4x2656x21248, b=2048): 0.0038
Throughput (in TFLOP/s) for attention_linear_projection (4x2656x21248, b=2048): 242.628
Elapsed time for mlp_h_to_4h (4x21248x10624, b=2048): 0.0151
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21248x10624, b=2048): 244.159
Elapsed time for mlp_4h_to_h (4x10624x21248, b=2048): 0.0155
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10624x21248, b=2048): 238.385

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 227.518
MLP duration (in seconds): 0.0307
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0477
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21376x8016, b=2048): 0.0115
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21376x8016, b=2048): 244.799
Elapsed time for attention_key_query_prob (64x2048x167x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x167x2048): 57.765
Elapsed time for attention_prob_times_values (64x2048x2048x167): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x167): 60.791
Elapsed time for attention_linear_projection (4x2672x21376, b=2048): 0.0038
Throughput (in TFLOP/s) for attention_linear_projection (4x2672x21376, b=2048): 245.637
Elapsed time for mlp_h_to_4h (4x21376x10688, b=2048): 0.0153
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21376x10688, b=2048): 245.245
Elapsed time for mlp_4h_to_h (4x10688x21376, b=2048): 0.0158
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10688x21376, b=2048): 236.480

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 214.288
MLP duration (in seconds): 0.0311
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0494
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21504x8064, b=2048): 0.0118
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21504x8064, b=2048): 240.228
Elapsed time for attention_key_query_prob (64x2048x168x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x168x2048): 101.694
Elapsed time for attention_prob_times_values (64x2048x2048x168): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x168): 144.777
Elapsed time for attention_linear_projection (4x2688x21504, b=2048): 0.0038
Throughput (in TFLOP/s) for attention_linear_projection (4x2688x21504, b=2048): 248.509
Elapsed time for mlp_h_to_4h (4x21504x10752, b=2048): 0.0154
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21504x10752, b=2048): 246.661
Elapsed time for mlp_4h_to_h (4x10752x21504, b=2048): 0.0159
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10752x21504, b=2048): 238.497

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 231.436
MLP duration (in seconds): 0.0312
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0484
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21632x8112, b=2048): 0.0118
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21632x8112, b=2048): 244.397
Elapsed time for attention_key_query_prob (64x2048x169x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x169x2048): 58.682
Elapsed time for attention_prob_times_values (64x2048x2048x169): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x169): 61.104
Elapsed time for attention_linear_projection (4x2704x21632, b=2048): 0.0040
Throughput (in TFLOP/s) for attention_linear_projection (4x2704x21632, b=2048): 241.142
Elapsed time for mlp_h_to_4h (4x21632x10816, b=2048): 0.0151
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21632x10816, b=2048): 254.522
Elapsed time for mlp_4h_to_h (4x10816x21632, b=2048): 0.0157
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10816x21632, b=2048): 244.810

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 213.909
MLP duration (in seconds): 0.0307
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0495
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21760x8160, b=2048): 0.0119
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21760x8160, b=2048): 243.780
Elapsed time for attention_key_query_prob (64x2048x170x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x170x2048): 81.709
Elapsed time for attention_prob_times_values (64x2048x2048x170): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x170): 105.894
Elapsed time for attention_linear_projection (4x2720x21760, b=2048): 0.0040
Throughput (in TFLOP/s) for attention_linear_projection (4x2720x21760, b=2048): 244.960
Elapsed time for mlp_h_to_4h (4x21760x10880, b=2048): 0.0531
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21760x10880, b=2048): 73.002
Elapsed time for mlp_4h_to_h (4x10880x21760, b=2048): 0.0162
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10880x21760, b=2048): 238.776

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 227.262
MLP duration (in seconds): 0.0694
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0872
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21888x8208, b=2048): 0.0117
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21888x8208, b=2048): 250.699
Elapsed time for attention_key_query_prob (64x2048x171x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x171x2048): 59.140
Elapsed time for attention_prob_times_values (64x2048x2048x171): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x171): 64.434
Elapsed time for attention_linear_projection (4x2736x21888, b=2048): 0.0040
Throughput (in TFLOP/s) for attention_linear_projection (4x2736x21888, b=2048): 246.411
Elapsed time for mlp_h_to_4h (4x21888x10944, b=2048): 0.0158
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21888x10944, b=2048): 247.929
Elapsed time for mlp_4h_to_h (4x10944x21888, b=2048): 0.0162
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10944x21888, b=2048): 241.629

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 219.692
MLP duration (in seconds): 0.0321
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0508
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22016x8256, b=2048): 0.0121
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22016x8256, b=2048): 246.628
Elapsed time for attention_key_query_prob (64x2048x172x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x172x2048): 82.406
Elapsed time for attention_prob_times_values (64x2048x2048x172): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x172): 106.844
Elapsed time for attention_linear_projection (4x2752x22016, b=2048): 0.0040
Throughput (in TFLOP/s) for attention_linear_projection (4x2752x22016, b=2048): 250.080
Elapsed time for mlp_h_to_4h (4x22016x11008, b=2048): 0.0161
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22016x11008, b=2048): 246.530
Elapsed time for mlp_4h_to_h (4x11008x22016, b=2048): 0.0164
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11008x22016, b=2048): 241.966

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 230.480
MLP duration (in seconds): 0.0325
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0505
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22144x8304, b=2048): 0.0125
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22144x8304, b=2048): 241.558
Elapsed time for attention_key_query_prob (64x2048x173x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x173x2048): 59.348
Elapsed time for attention_prob_times_values (64x2048x2048x173): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x173): 64.754
Elapsed time for attention_linear_projection (4x2768x22144, b=2048): 0.0041
Throughput (in TFLOP/s) for attention_linear_projection (4x2768x22144, b=2048): 243.251
Elapsed time for mlp_h_to_4h (4x22144x11072, b=2048): 0.0163
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22144x11072, b=2048): 246.021
Elapsed time for mlp_4h_to_h (4x11072x22144, b=2048): 0.0172
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11072x22144, b=2048): 233.641

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 214.427
MLP duration (in seconds): 0.0335
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0531
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22272x8352, b=2048): 0.0127
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22272x8352, b=2048): 240.710
Elapsed time for attention_key_query_prob (64x2048x174x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x174x2048): 83.205
Elapsed time for attention_prob_times_values (64x2048x2048x174): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x174): 105.213
Elapsed time for attention_linear_projection (4x2784x22272, b=2048): 0.0041
Throughput (in TFLOP/s) for attention_linear_projection (4x2784x22272, b=2048): 245.603
Elapsed time for mlp_h_to_4h (4x22272x11136, b=2048): 0.0165
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22272x11136, b=2048): 246.124
Elapsed time for mlp_4h_to_h (4x11136x22272, b=2048): 0.0172
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11136x22272, b=2048): 236.029

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 225.988
MLP duration (in seconds): 0.0337
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0525
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22400x8400, b=2048): 0.0124
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22400x8400, b=2048): 247.858
Elapsed time for attention_key_query_prob (64x2048x175x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x175x2048): 59.907
Elapsed time for attention_prob_times_values (64x2048x2048x175): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x175): 63.192
Elapsed time for attention_linear_projection (4x2800x22400, b=2048): 0.0042
Throughput (in TFLOP/s) for attention_linear_projection (4x2800x22400, b=2048): 246.206
Elapsed time for mlp_h_to_4h (4x22400x11200, b=2048): 0.0162
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22400x11200, b=2048): 253.979
Elapsed time for mlp_4h_to_h (4x11200x22400, b=2048): 0.0173
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11200x22400, b=2048): 237.023

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 218.559
MLP duration (in seconds): 0.0335
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0532
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22528x8448, b=2048): 0.0125
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22528x8448, b=2048): 248.749
Elapsed time for attention_key_query_prob (64x2048x176x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x176x2048): 106.795
Elapsed time for attention_prob_times_values (64x2048x2048x176): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x176): 154.209
Elapsed time for attention_linear_projection (4x2816x22528, b=2048): 0.0042
Throughput (in TFLOP/s) for attention_linear_projection (4x2816x22528, b=2048): 247.080
Elapsed time for mlp_h_to_4h (4x22528x11264, b=2048): 0.0166
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22528x11264, b=2048): 249.809
Elapsed time for mlp_4h_to_h (4x11264x22528, b=2048): 0.0173
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11264x22528, b=2048): 239.746

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 238.302
MLP duration (in seconds): 0.0340
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0522
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22656x8496, b=2048): 0.0129
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22656x8496, b=2048): 243.690
Elapsed time for attention_key_query_prob (64x2048x177x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x177x2048): 59.971
Elapsed time for attention_prob_times_values (64x2048x2048x177): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x177): 63.416
Elapsed time for attention_linear_projection (4x2832x22656, b=2048): 0.0043
Throughput (in TFLOP/s) for attention_linear_projection (4x2832x22656, b=2048): 244.180
Elapsed time for mlp_h_to_4h (4x22656x11328, b=2048): 0.0170
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22656x11328, b=2048): 247.939
Elapsed time for mlp_4h_to_h (4x11328x22656, b=2048): 0.0178
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11328x22656, b=2048): 236.810

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 216.187
MLP duration (in seconds): 0.0347
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0550
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22784x8544, b=2048): 0.0132
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22784x8544, b=2048): 242.221
Elapsed time for attention_key_query_prob (64x2048x178x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x178x2048): 84.348
Elapsed time for attention_prob_times_values (64x2048x2048x178): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x178): 108.184
Elapsed time for attention_linear_projection (4x2848x22784, b=2048): 0.0043
Throughput (in TFLOP/s) for attention_linear_projection (4x2848x22784, b=2048): 247.647
Elapsed time for mlp_h_to_4h (4x22784x11392, b=2048): 0.0171
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22784x11392, b=2048): 248.922
Elapsed time for mlp_4h_to_h (4x11392x22784, b=2048): 0.0175
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11392x22784, b=2048): 242.707

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 228.154
MLP duration (in seconds): 0.0346
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0541
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22912x8592, b=2048): 0.0129
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22912x8592, b=2048): 250.539
Elapsed time for attention_key_query_prob (64x2048x179x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x179x2048): 60.008
Elapsed time for attention_prob_times_values (64x2048x2048x179): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x179): 66.579
Elapsed time for attention_linear_projection (4x2864x22912, b=2048): 0.0044
Throughput (in TFLOP/s) for attention_linear_projection (4x2864x22912, b=2048): 243.342
Elapsed time for mlp_h_to_4h (4x22912x11456, b=2048): 0.0174
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22912x11456, b=2048): 247.126
Elapsed time for mlp_4h_to_h (4x11456x22912, b=2048): 0.0180
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11456x22912, b=2048): 239.367

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 220.915
MLP duration (in seconds): 0.0354
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0557
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23040x8640, b=2048): 0.0138
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23040x8640, b=2048): 237.009
Elapsed time for attention_key_query_prob (64x2048x180x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x180x2048): 84.902
Elapsed time for attention_prob_times_values (64x2048x2048x180): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x180): 108.000
Elapsed time for attention_linear_projection (4x2880x23040, b=2048): 0.0044
Throughput (in TFLOP/s) for attention_linear_projection (4x2880x23040, b=2048): 248.726
Elapsed time for mlp_h_to_4h (4x23040x11520, b=2048): 0.0184
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23040x11520, b=2048): 236.123
Elapsed time for mlp_4h_to_h (4x11520x23040, b=2048): 0.0179
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11520x23040, b=2048): 242.896

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 225.239
MLP duration (in seconds): 0.0363
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0565
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23168x8688, b=2048): 0.0141
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23168x8688, b=2048): 234.697
Elapsed time for attention_key_query_prob (64x2048x181x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x181x2048): 60.417
Elapsed time for attention_prob_times_values (64x2048x2048x181): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x181): 66.652
Elapsed time for attention_linear_projection (4x2896x23168, b=2048): 0.0045
Throughput (in TFLOP/s) for attention_linear_projection (4x2896x23168, b=2048): 242.924
Elapsed time for mlp_h_to_4h (4x23168x11584, b=2048): 0.0190
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23168x11584, b=2048): 231.130
Elapsed time for mlp_4h_to_h (4x11584x23168, b=2048): 0.0183
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11584x23168, b=2048): 239.688

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 212.146
MLP duration (in seconds): 0.0374
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0590
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23296x8736, b=2048): 0.0151
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23296x8736, b=2048): 221.001
Elapsed time for attention_key_query_prob (64x2048x182x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x182x2048): 86.189
Elapsed time for attention_prob_times_values (64x2048x2048x182): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x182): 111.761
Elapsed time for attention_linear_projection (4x2912x23296, b=2048): 0.0045
Throughput (in TFLOP/s) for attention_linear_projection (4x2912x23296, b=2048): 248.562
Elapsed time for mlp_h_to_4h (4x23296x11648, b=2048): 0.0192
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23296x11648, b=2048): 231.719
Elapsed time for mlp_4h_to_h (4x11648x23296, b=2048): 0.0211
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11648x23296, b=2048): 210.579

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 215.200
MLP duration (in seconds): 0.0403
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0619
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23424x8784, b=2048): 0.0153
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23424x8784, b=2048): 220.902
Elapsed time for attention_key_query_prob (64x2048x183x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x183x2048): 60.752
Elapsed time for attention_prob_times_values (64x2048x2048x183): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x183): 65.161
Elapsed time for attention_linear_projection (4x2928x23424, b=2048): 0.0046
Throughput (in TFLOP/s) for attention_linear_projection (4x2928x23424, b=2048): 246.517
Elapsed time for mlp_h_to_4h (4x23424x11712, b=2048): 0.0189
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23424x11712, b=2048): 238.068
Elapsed time for mlp_4h_to_h (4x11712x23424, b=2048): 0.0187
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11712x23424, b=2048): 239.904

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 204.468
MLP duration (in seconds): 0.0376
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0606
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23552x8832, b=2048): 0.0154
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23552x8832, b=2048): 221.746
Elapsed time for attention_key_query_prob (64x2048x184x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x184x2048): 109.149
Elapsed time for attention_prob_times_values (64x2048x2048x184): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x184): 155.881
Elapsed time for attention_linear_projection (4x2944x23552, b=2048): 0.0045
Throughput (in TFLOP/s) for attention_linear_projection (4x2944x23552, b=2048): 250.700
Elapsed time for mlp_h_to_4h (4x23552x11776, b=2048): 0.0189
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23552x11776, b=2048): 240.419
Elapsed time for mlp_4h_to_h (4x11776x23552, b=2048): 0.0189
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11776x23552, b=2048): 240.240

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 221.166
MLP duration (in seconds): 0.0378
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0593
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23680x8880, b=2048): 0.0146
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23680x8880, b=2048): 235.507
Elapsed time for attention_key_query_prob (64x2048x185x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x185x2048): 60.806
Elapsed time for attention_prob_times_values (64x2048x2048x185): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x185): 65.367
Elapsed time for attention_linear_projection (4x2960x23680, b=2048): 0.0048
Throughput (in TFLOP/s) for attention_linear_projection (4x2960x23680, b=2048): 239.579
Elapsed time for mlp_h_to_4h (4x23680x11840, b=2048): 0.0196
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23680x11840, b=2048): 234.551
Elapsed time for mlp_4h_to_h (4x11840x23680, b=2048): 0.0191
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11840x23680, b=2048): 240.174

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 212.280
MLP duration (in seconds): 0.0387
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0613
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23808x8928, b=2048): 0.0140
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23808x8928, b=2048): 248.860
Elapsed time for attention_key_query_prob (64x2048x186x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x186x2048): 88.343
Elapsed time for attention_prob_times_values (64x2048x2048x186): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x186): 111.928
Elapsed time for attention_linear_projection (4x2976x23808, b=2048): 0.0048
Throughput (in TFLOP/s) for attention_linear_projection (4x2976x23808, b=2048): 243.569
Elapsed time for mlp_h_to_4h (4x23808x11904, b=2048): 0.0198
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23808x11904, b=2048): 234.279
Elapsed time for mlp_4h_to_h (4x11904x23808, b=2048): 0.0193
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11904x23808, b=2048): 240.134

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 233.038
MLP duration (in seconds): 0.0392
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0599
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23936x8976, b=2048): 0.0145
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23936x8976, b=2048): 241.967
Elapsed time for attention_key_query_prob (64x2048x187x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x187x2048): 61.815
Elapsed time for attention_prob_times_values (64x2048x2048x187): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x187): 69.440
Elapsed time for attention_linear_projection (4x2992x23936, b=2048): 0.0047
Throughput (in TFLOP/s) for attention_linear_projection (4x2992x23936, b=2048): 247.832
Elapsed time for mlp_h_to_4h (4x23936x11968, b=2048): 0.0192
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23936x11968, b=2048): 243.835
Elapsed time for mlp_4h_to_h (4x11968x23936, b=2048): 0.0192
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11968x23936, b=2048): 244.326

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 218.960
MLP duration (in seconds): 0.0385
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0608
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24064x9024, b=2048): 0.0148
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24064x9024, b=2048): 240.235
Elapsed time for attention_key_query_prob (64x2048x188x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x188x2048): 88.843
Elapsed time for attention_prob_times_values (64x2048x2048x188): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x188): 113.011
Elapsed time for attention_linear_projection (4x3008x24064, b=2048): 0.0047
Throughput (in TFLOP/s) for attention_linear_projection (4x3008x24064, b=2048): 251.135
Elapsed time for mlp_h_to_4h (4x24064x12032, b=2048): 0.0198
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24064x12032, b=2048): 239.353
Elapsed time for mlp_4h_to_h (4x12032x24064, b=2048): 0.0196
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12032x24064, b=2048): 241.993

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 229.376
MLP duration (in seconds): 0.0394
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0610
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24192x9072, b=2048): 0.0150
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24192x9072, b=2048): 239.026
Elapsed time for attention_key_query_prob (64x2048x189x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x189x2048): 62.112
Elapsed time for attention_prob_times_values (64x2048x2048x189): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x189): 69.918
Elapsed time for attention_linear_projection (4x3024x24192, b=2048): 0.0048
Throughput (in TFLOP/s) for attention_linear_projection (4x3024x24192, b=2048): 248.175
Elapsed time for mlp_h_to_4h (4x24192x12096, b=2048): 0.0287
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24192x12096, b=2048): 167.295
Elapsed time for mlp_4h_to_h (4x12096x24192, b=2048): 0.0197
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12096x24192, b=2048): 242.923

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 217.672
MLP duration (in seconds): 0.0484
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0714
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24320x9120, b=2048): 0.0151
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24320x9120, b=2048): 239.927
Elapsed time for attention_key_query_prob (64x2048x190x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x190x2048): 90.129
Elapsed time for attention_prob_times_values (64x2048x2048x190): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x190): 114.396
Elapsed time for attention_linear_projection (4x3040x24320, b=2048): 0.0049
Throughput (in TFLOP/s) for attention_linear_projection (4x3040x24320, b=2048): 247.329
Elapsed time for mlp_h_to_4h (4x24320x12160, b=2048): 0.0204
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24320x12160, b=2048): 236.958
Elapsed time for mlp_4h_to_h (4x12160x24320, b=2048): 0.0197
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12160x24320, b=2048): 246.035

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 228.815
MLP duration (in seconds): 0.0401
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0622
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24448x9168, b=2048): 0.0152
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24448x9168, b=2048): 241.158
Elapsed time for attention_key_query_prob (64x2048x191x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x191x2048): 63.193
Elapsed time for attention_prob_times_values (64x2048x2048x191): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x191): 69.147
Elapsed time for attention_linear_projection (4x3056x24448, b=2048): 0.0049
Throughput (in TFLOP/s) for attention_linear_projection (4x3056x24448, b=2048): 249.042
Elapsed time for mlp_h_to_4h (4x24448x12224, b=2048): 0.0198
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24448x12224, b=2048): 246.726
Elapsed time for mlp_4h_to_h (4x12224x24448, b=2048): 0.0196
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12224x24448, b=2048): 249.496

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 219.431
MLP duration (in seconds): 0.0395
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0627
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24576x9216, b=2048): 0.0154
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24576x9216, b=2048): 241.036
Elapsed time for attention_key_query_prob (64x2048x192x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x192x2048): 112.123
Elapsed time for attention_prob_times_values (64x2048x2048x192): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x192): 163.211
Elapsed time for attention_linear_projection (4x3072x24576, b=2048): 0.0049
Throughput (in TFLOP/s) for attention_linear_projection (4x3072x24576, b=2048): 252.305
Elapsed time for mlp_h_to_4h (4x24576x12288, b=2048): 0.0202
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24576x12288, b=2048): 245.540
Elapsed time for mlp_4h_to_h (4x12288x24576, b=2048): 0.0196
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12288x24576, b=2048): 251.931

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 235.891
MLP duration (in seconds): 0.0398
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0616
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24704x9264, b=2048): 0.0170
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24704x9264, b=2048): 219.983
Elapsed time for attention_key_query_prob (64x2048x193x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x193x2048): 60.580
Elapsed time for attention_prob_times_values (64x2048x2048x193): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x193): 68.755
Elapsed time for attention_linear_projection (4x3088x24704, b=2048): 0.0051
Throughput (in TFLOP/s) for attention_linear_projection (4x3088x24704, b=2048): 245.807
Elapsed time for mlp_h_to_4h (4x24704x12352, b=2048): 0.0209
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24704x12352, b=2048): 238.994
Elapsed time for mlp_4h_to_h (4x12352x24704, b=2048): 0.0203
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12352x24704, b=2048): 246.562

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 205.415
MLP duration (in seconds): 0.0412
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0665
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24832x9312, b=2048): 0.0168
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24832x9312, b=2048): 225.629
Elapsed time for attention_key_query_prob (64x2048x194x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x194x2048): 86.436
Elapsed time for attention_prob_times_values (64x2048x2048x194): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x194): 115.599
Elapsed time for attention_linear_projection (4x3104x24832, b=2048): 0.0051
Throughput (in TFLOP/s) for attention_linear_projection (4x3104x24832, b=2048): 248.280
Elapsed time for mlp_h_to_4h (4x24832x12416, b=2048): 0.0211
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24832x12416, b=2048): 238.987
Elapsed time for mlp_4h_to_h (4x12416x24832, b=2048): 0.0199
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12416x24832, b=2048): 253.423

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 219.306
MLP duration (in seconds): 0.0411
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0651
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24960x9360, b=2048): 0.0163
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24960x9360, b=2048): 235.474
Elapsed time for attention_key_query_prob (64x2048x195x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x195x2048): 60.775
Elapsed time for attention_prob_times_values (64x2048x2048x195): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x195): 70.880
Elapsed time for attention_linear_projection (4x3120x24960, b=2048): 0.0052
Throughput (in TFLOP/s) for attention_linear_projection (4x3120x24960, b=2048): 245.461
Elapsed time for mlp_h_to_4h (4x24960x12480, b=2048): 0.0204
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24960x12480, b=2048): 249.617
Elapsed time for mlp_4h_to_h (4x12480x24960, b=2048): 0.0207
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12480x24960, b=2048): 246.328

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 215.512
MLP duration (in seconds): 0.0412
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0658
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25088x9408, b=2048): 0.0162
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25088x9408, b=2048): 238.364
Elapsed time for attention_key_query_prob (64x2048x196x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x196x2048): 86.540
Elapsed time for attention_prob_times_values (64x2048x2048x196): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x196): 115.055
Elapsed time for attention_linear_projection (4x3136x25088, b=2048): 0.0052
Throughput (in TFLOP/s) for attention_linear_projection (4x3136x25088, b=2048): 249.946
Elapsed time for mlp_h_to_4h (4x25088x12544, b=2048): 0.0208
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25088x12544, b=2048): 248.136
Elapsed time for mlp_4h_to_h (4x12544x25088, b=2048): 0.0202
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12544x25088, b=2048): 255.705

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 228.256
MLP duration (in seconds): 0.0409
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0645
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25216x9456, b=2048): 0.0165
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25216x9456, b=2048): 237.466
Elapsed time for attention_key_query_prob (64x2048x197x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x197x2048): 61.509
Elapsed time for attention_prob_times_values (64x2048x2048x197): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x197): 71.193
Elapsed time for attention_linear_projection (4x3152x25216, b=2048): 0.0053
Throughput (in TFLOP/s) for attention_linear_projection (4x3152x25216, b=2048): 247.267
Elapsed time for mlp_h_to_4h (4x25216x12608, b=2048): 0.0209
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25216x12608, b=2048): 248.720
Elapsed time for mlp_4h_to_h (4x12608x25216, b=2048): 0.0207
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12608x25216, b=2048): 251.987

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 217.487
MLP duration (in seconds): 0.0416
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0665
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25344x9504, b=2048): 0.0179
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25344x9504, b=2048): 220.970
Elapsed time for attention_key_query_prob (64x2048x198x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x198x2048): 86.742
Elapsed time for attention_prob_times_values (64x2048x2048x198): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x198): 113.449
Elapsed time for attention_linear_projection (4x3168x25344, b=2048): 0.0053
Throughput (in TFLOP/s) for attention_linear_projection (4x3168x25344, b=2048): 247.387
Elapsed time for mlp_h_to_4h (4x25344x12672, b=2048): 0.0218
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25344x12672, b=2048): 241.267
Elapsed time for mlp_4h_to_h (4x12672x25344, b=2048): 0.0207
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12672x25344, b=2048): 254.666

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 216.046
MLP duration (in seconds): 0.0425
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0678
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25472x9552, b=2048): 0.0170
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25472x9552, b=2048): 234.420
Elapsed time for attention_key_query_prob (64x2048x199x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x199x2048): 61.309
Elapsed time for attention_prob_times_values (64x2048x2048x199): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x199): 68.919
Elapsed time for attention_linear_projection (4x3184x25472, b=2048): 0.0053
Throughput (in TFLOP/s) for attention_linear_projection (4x3184x25472, b=2048): 251.516
Elapsed time for mlp_h_to_4h (4x25472x12736, b=2048): 0.0212
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25472x12736, b=2048): 251.306
Elapsed time for mlp_4h_to_h (4x12736x25472, b=2048): 0.0205
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12736x25472, b=2048): 258.768

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 216.129
MLP duration (in seconds): 0.0417
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0673
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25600x9600, b=2048): 0.0169
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25600x9600, b=2048): 238.155
Elapsed time for attention_key_query_prob (64x2048x200x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x200x2048): 110.328
Elapsed time for attention_prob_times_values (64x2048x2048x200): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x200): 169.245
Elapsed time for attention_linear_projection (4x3200x25600, b=2048): 0.0053
Throughput (in TFLOP/s) for attention_linear_projection (4x3200x25600, b=2048): 251.250
Elapsed time for mlp_h_to_4h (4x25600x12800, b=2048): 0.0217
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25600x12800, b=2048): 246.840
Elapsed time for mlp_4h_to_h (4x12800x25600, b=2048): 0.0212
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12800x25600, b=2048): 253.384

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 234.040
MLP duration (in seconds): 0.0429
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0668
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25728x9648, b=2048): 0.0171
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25728x9648, b=2048): 237.832
Elapsed time for attention_key_query_prob (64x2048x201x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x201x2048): 61.647
Elapsed time for attention_prob_times_values (64x2048x2048x201): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x201): 69.313
Elapsed time for attention_linear_projection (4x3216x25728, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_linear_projection (4x3216x25728, b=2048): 246.507
Elapsed time for mlp_h_to_4h (4x25728x12864, b=2048): 0.0216
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25728x12864, b=2048): 251.201
Elapsed time for mlp_4h_to_h (4x12864x25728, b=2048): 0.0213
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12864x25728, b=2048): 254.758

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 217.642
MLP duration (in seconds): 0.0429
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0688
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25856x9696, b=2048): 0.0172
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25856x9696, b=2048): 238.978
Elapsed time for attention_key_query_prob (64x2048x202x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x202x2048): 88.323
Elapsed time for attention_prob_times_values (64x2048x2048x202): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x202): 113.291
Elapsed time for attention_linear_projection (4x3232x25856, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_linear_projection (4x3232x25856, b=2048): 249.843
Elapsed time for mlp_h_to_4h (4x25856x12928, b=2048): 0.0219
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25856x12928, b=2048): 250.203
Elapsed time for mlp_4h_to_h (4x12928x25856, b=2048): 0.0212
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12928x25856, b=2048): 258.498

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 229.090
MLP duration (in seconds): 0.0431
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0679
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25984x9744, b=2048): 0.0190
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25984x9744, b=2048): 218.391
Elapsed time for attention_key_query_prob (64x2048x203x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x203x2048): 62.507
Elapsed time for attention_prob_times_values (64x2048x2048x203): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x203): 71.637
Elapsed time for attention_linear_projection (4x3248x25984, b=2048): 0.0077
Throughput (in TFLOP/s) for attention_linear_projection (4x3248x25984, b=2048): 178.528
Elapsed time for mlp_h_to_4h (4x25984x12992, b=2048): 0.0223
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25984x12992, b=2048): 248.206
Elapsed time for mlp_4h_to_h (4x12992x25984, b=2048): 0.0218
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12992x25984, b=2048): 253.276

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 191.601
MLP duration (in seconds): 0.0441
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0741
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26112x9792, b=2048): 0.0186
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26112x9792, b=2048): 224.875
Elapsed time for attention_key_query_prob (64x2048x204x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x204x2048): 89.843
Elapsed time for attention_prob_times_values (64x2048x2048x204): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x204): 114.986
Elapsed time for attention_linear_projection (4x3264x26112, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_linear_projection (4x3264x26112, b=2048): 251.922
Elapsed time for mlp_h_to_4h (4x26112x13056, b=2048): 0.0221
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26112x13056, b=2048): 252.538
Elapsed time for mlp_4h_to_h (4x13056x26112, b=2048): 0.0219
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13056x26112, b=2048): 254.489

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 220.344
MLP duration (in seconds): 0.0441
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0704
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26240x9840, b=2048): 0.0183
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26240x9840, b=2048): 231.726
Elapsed time for attention_key_query_prob (64x2048x205x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x205x2048): 62.746
Elapsed time for attention_prob_times_values (64x2048x2048x205): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x205): 60.715
Elapsed time for attention_linear_projection (4x3280x26240, b=2048): 0.0056
Throughput (in TFLOP/s) for attention_linear_projection (4x3280x26240, b=2048): 249.673
Elapsed time for mlp_h_to_4h (4x26240x13120, b=2048): 0.0227
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26240x13120, b=2048): 248.122
Elapsed time for mlp_4h_to_h (4x13120x26240, b=2048): 0.0221
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13120x26240, b=2048): 254.930

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 213.342
MLP duration (in seconds): 0.0449
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0723
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26368x9888, b=2048): 0.0178
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26368x9888, b=2048): 240.474
Elapsed time for attention_key_query_prob (64x2048x206x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x206x2048): 89.275
Elapsed time for attention_prob_times_values (64x2048x2048x206): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x206): 117.080
Elapsed time for attention_linear_projection (4x3296x26368, b=2048): 0.0057
Throughput (in TFLOP/s) for attention_linear_projection (4x3296x26368, b=2048): 248.878
Elapsed time for mlp_h_to_4h (4x26368x13184, b=2048): 0.0231
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26368x13184, b=2048): 246.361
Elapsed time for mlp_4h_to_h (4x13184x26368, b=2048): 0.0225
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13184x26368, b=2048): 252.682

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 230.509
MLP duration (in seconds): 0.0457
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0713
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26496x9936, b=2048): 0.0186
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26496x9936, b=2048): 232.322
Elapsed time for attention_key_query_prob (64x2048x207x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x207x2048): 62.939
Elapsed time for attention_prob_times_values (64x2048x2048x207): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x207): 71.283
Elapsed time for attention_linear_projection (4x3312x26496, b=2048): 0.0057
Throughput (in TFLOP/s) for attention_linear_projection (4x3312x26496, b=2048): 250.279
Elapsed time for mlp_h_to_4h (4x26496x13248, b=2048): 0.0228
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26496x13248, b=2048): 251.744
Elapsed time for mlp_4h_to_h (4x13248x26496, b=2048): 0.0224
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13248x26496, b=2048): 257.047

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 216.147
MLP duration (in seconds): 0.0452
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0729
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26624x9984, b=2048): 0.0182
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26624x9984, b=2048): 239.666
Elapsed time for attention_key_query_prob (64x2048x208x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x208x2048): 114.713
Elapsed time for attention_prob_times_values (64x2048x2048x208): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x208): 177.415
Elapsed time for attention_linear_projection (4x3328x26624, b=2048): 0.0057
Throughput (in TFLOP/s) for attention_linear_projection (4x3328x26624, b=2048): 253.671
Elapsed time for mlp_h_to_4h (4x26624x13312, b=2048): 0.0236
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26624x13312, b=2048): 246.296
Elapsed time for mlp_4h_to_h (4x13312x26624, b=2048): 0.0231
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13312x26624, b=2048): 251.778

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 236.502
MLP duration (in seconds): 0.0466
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0721
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26752x10032, b=2048): 0.0194
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26752x10032, b=2048): 227.135
Elapsed time for attention_key_query_prob (64x2048x209x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x209x2048): 63.299
Elapsed time for attention_prob_times_values (64x2048x2048x209): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x209): 71.145
Elapsed time for attention_linear_projection (4x3344x26752, b=2048): 0.0059
Throughput (in TFLOP/s) for attention_linear_projection (4x3344x26752, b=2048): 250.542
Elapsed time for mlp_h_to_4h (4x26752x13376, b=2048): 0.0234
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26752x13376, b=2048): 251.064
Elapsed time for mlp_4h_to_h (4x13376x26752, b=2048): 0.0226
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13376x26752, b=2048): 258.893

Attention duration (in seconds): 0.0286
Attention throughput (in TFLOP/s): 213.146
MLP duration (in seconds): 0.0460
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0746
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26880x10080, b=2048): 0.0194
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26880x10080, b=2048): 229.141
Elapsed time for attention_key_query_prob (64x2048x210x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x210x2048): 90.538
Elapsed time for attention_prob_times_values (64x2048x2048x210): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x210): 116.645
Elapsed time for attention_linear_projection (4x3360x26880, b=2048): 0.0061
Throughput (in TFLOP/s) for attention_linear_projection (4x3360x26880, b=2048): 241.678
Elapsed time for mlp_h_to_4h (4x26880x13440, b=2048): 0.0238
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26880x13440, b=2048): 249.183
Elapsed time for mlp_4h_to_h (4x13440x26880, b=2048): 0.0233
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13440x26880, b=2048): 253.861

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 221.758
MLP duration (in seconds): 0.0471
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0748
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27008x10128, b=2048): 0.0189
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27008x10128, b=2048): 236.602
Elapsed time for attention_key_query_prob (64x2048x211x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x211x2048): 63.784
Elapsed time for attention_prob_times_values (64x2048x2048x211): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x211): 73.119
Elapsed time for attention_linear_projection (4x3376x27008, b=2048): 0.0061
Throughput (in TFLOP/s) for attention_linear_projection (4x3376x27008, b=2048): 245.697
Elapsed time for mlp_h_to_4h (4x27008x13504, b=2048): 0.0238
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27008x13504, b=2048): 251.126
Elapsed time for mlp_4h_to_h (4x13504x27008, b=2048): 0.0232
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13504x27008, b=2048): 257.594

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 218.791
MLP duration (in seconds): 0.0470
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0753
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27136x10176, b=2048): 0.0188
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27136x10176, b=2048): 240.771
Elapsed time for attention_key_query_prob (64x2048x212x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x212x2048): 91.875
Elapsed time for attention_prob_times_values (64x2048x2048x212): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x212): 116.719
Elapsed time for attention_linear_projection (4x3392x27136, b=2048): 0.0063
Throughput (in TFLOP/s) for attention_linear_projection (4x3392x27136, b=2048): 239.495
Elapsed time for mlp_h_to_4h (4x27136x13568, b=2048): 0.0242
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27136x13568, b=2048): 249.394
Elapsed time for mlp_4h_to_h (4x13568x27136, b=2048): 0.0233
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13568x27136, b=2048): 258.609

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 229.290
MLP duration (in seconds): 0.0475
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0748
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27264x10224, b=2048): 0.0191
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27264x10224, b=2048): 238.863
Elapsed time for attention_key_query_prob (64x2048x213x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x213x2048): 64.002
Elapsed time for attention_prob_times_values (64x2048x2048x213): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x213): 73.338
Elapsed time for attention_linear_projection (4x3408x27264, b=2048): 0.0065
Throughput (in TFLOP/s) for attention_linear_projection (4x3408x27264, b=2048): 234.445
Elapsed time for mlp_h_to_4h (4x27264x13632, b=2048): 0.0247
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27264x13632, b=2048): 246.639
Elapsed time for mlp_4h_to_h (4x13632x27264, b=2048): 0.0240
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13632x27264, b=2048): 253.539

Attention duration (in seconds): 0.0290
Attention throughput (in TFLOP/s): 218.171
MLP duration (in seconds): 0.0487
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0777
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27392x10272, b=2048): 0.0198
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27392x10272, b=2048): 232.842
Elapsed time for attention_key_query_prob (64x2048x214x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x214x2048): 93.461
Elapsed time for attention_prob_times_values (64x2048x2048x214): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x214): 119.161
Elapsed time for attention_linear_projection (4x3424x27392, b=2048): 0.0064
Throughput (in TFLOP/s) for attention_linear_projection (4x3424x27392, b=2048): 239.519
Elapsed time for mlp_h_to_4h (4x27392x13696, b=2048): 0.0248
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27392x13696, b=2048): 247.929
Elapsed time for mlp_4h_to_h (4x13696x27392, b=2048): 0.0244
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13696x27392, b=2048): 251.533

Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 224.460
MLP duration (in seconds): 0.0492
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0776
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27520x10320, b=2048): 0.0197
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27520x10320, b=2048): 236.530
Elapsed time for attention_key_query_prob (64x2048x215x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x215x2048): 64.286
Elapsed time for attention_prob_times_values (64x2048x2048x215): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x215): 72.432
Elapsed time for attention_linear_projection (4x3440x27520, b=2048): 0.0068
Throughput (in TFLOP/s) for attention_linear_projection (4x3440x27520, b=2048): 229.345
Elapsed time for mlp_h_to_4h (4x27520x13760, b=2048): 0.0249
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27520x13760, b=2048): 249.178
Elapsed time for mlp_4h_to_h (4x13760x27520, b=2048): 0.0240
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13760x27520, b=2048): 258.410

Attention duration (in seconds): 0.0298
Attention throughput (in TFLOP/s): 215.763
MLP duration (in seconds): 0.0489
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0787
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27648x10368, b=2048): 0.0196
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27648x10368, b=2048): 239.205
Elapsed time for attention_key_query_prob (64x2048x216x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x216x2048): 118.747
Elapsed time for attention_prob_times_values (64x2048x2048x216): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x216): 179.612
Elapsed time for attention_linear_projection (4x3456x27648, b=2048): 0.0064
Throughput (in TFLOP/s) for attention_linear_projection (4x3456x27648, b=2048): 246.379
Elapsed time for mlp_h_to_4h (4x27648x13824, b=2048): 0.0247
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27648x13824, b=2048): 253.491
Elapsed time for mlp_4h_to_h (4x13824x27648, b=2048): 0.0242
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13824x27648, b=2048): 259.292

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 235.202
MLP duration (in seconds): 0.0489
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0765
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27776x10416, b=2048): 0.0196
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27776x10416, b=2048): 242.228
Elapsed time for attention_key_query_prob (64x2048x217x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x217x2048): 65.022
Elapsed time for attention_prob_times_values (64x2048x2048x217): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x217): 73.128
Elapsed time for attention_linear_projection (4x3472x27776, b=2048): 0.0068
Throughput (in TFLOP/s) for attention_linear_projection (4x3472x27776, b=2048): 232.305
Elapsed time for mlp_h_to_4h (4x27776x13888, b=2048): 0.0250
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27776x13888, b=2048): 252.308
Elapsed time for mlp_4h_to_h (4x13888x27776, b=2048): 0.0385
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13888x27776, b=2048): 164.329

Attention duration (in seconds): 0.0298
Attention throughput (in TFLOP/s): 220.235
MLP duration (in seconds): 0.0635
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0933
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27904x10464, b=2048): 0.0204
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27904x10464, b=2048): 234.555
Elapsed time for attention_key_query_prob (64x2048x218x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x218x2048): 94.475
Elapsed time for attention_prob_times_values (64x2048x2048x218): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x218): 116.160
Elapsed time for attention_linear_projection (4x3488x27904, b=2048): 0.0067
Throughput (in TFLOP/s) for attention_linear_projection (4x3488x27904, b=2048): 237.751
Elapsed time for mlp_h_to_4h (4x27904x13952, b=2048): 0.0257
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27904x13952, b=2048): 248.151
Elapsed time for mlp_4h_to_h (4x13952x27904, b=2048): 0.0247
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13952x27904, b=2048): 258.035

Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 225.308
MLP duration (in seconds): 0.0504
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0798
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28032x10512, b=2048): 0.0202
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28032x10512, b=2048): 238.591
Elapsed time for attention_key_query_prob (64x2048x219x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x219x2048): 65.902
Elapsed time for attention_prob_times_values (64x2048x2048x219): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x219): 75.381
Elapsed time for attention_linear_projection (4x3504x28032, b=2048): 0.0067
Throughput (in TFLOP/s) for attention_linear_projection (4x3504x28032, b=2048): 238.766
Elapsed time for mlp_h_to_4h (4x28032x14016, b=2048): 0.0259
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28032x14016, b=2048): 248.847
Elapsed time for mlp_4h_to_h (4x14016x28032, b=2048): 0.0248
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14016x28032, b=2048): 259.052

Attention duration (in seconds): 0.0303
Attention throughput (in TFLOP/s): 220.072
MLP duration (in seconds): 0.0507
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0810
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28160x10560, b=2048): 0.0204
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28160x10560, b=2048): 239.124
Elapsed time for attention_key_query_prob (64x2048x220x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x220x2048): 94.523
Elapsed time for attention_prob_times_values (64x2048x2048x220): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x220): 121.421
Elapsed time for attention_linear_projection (4x3520x28160, b=2048): 0.0067
Throughput (in TFLOP/s) for attention_linear_projection (4x3520x28160, b=2048): 241.310
Elapsed time for mlp_h_to_4h (4x28160x14080, b=2048): 0.0259
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28160x14080, b=2048): 250.340
Elapsed time for mlp_4h_to_h (4x14080x28160, b=2048): 0.0256
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14080x28160, b=2048): 253.732

Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 229.561
MLP duration (in seconds): 0.0516
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0809
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28288x10608, b=2048): 0.0204
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28288x10608, b=2048): 240.900
Elapsed time for attention_key_query_prob (64x2048x221x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x221x2048): 66.450
Elapsed time for attention_prob_times_values (64x2048x2048x221): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x221): 76.703
Elapsed time for attention_linear_projection (4x3536x28288, b=2048): 0.0071
Throughput (in TFLOP/s) for attention_linear_projection (4x3536x28288, b=2048): 230.123
Elapsed time for mlp_h_to_4h (4x28288x14144, b=2048): 0.0260
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28288x14144, b=2048): 252.334
Elapsed time for mlp_4h_to_h (4x14144x28288, b=2048): 0.0254
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14144x28288, b=2048): 257.685

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 220.091
MLP duration (in seconds): 0.0514
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0823
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28416x10656, b=2048): 0.0204
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28416x10656, b=2048): 243.500
Elapsed time for attention_key_query_prob (64x2048x222x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x222x2048): 96.618
Elapsed time for attention_prob_times_values (64x2048x2048x222): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x222): 123.067
Elapsed time for attention_linear_projection (4x3552x28416, b=2048): 0.0069
Throughput (in TFLOP/s) for attention_linear_projection (4x3552x28416, b=2048): 238.863
Elapsed time for mlp_h_to_4h (4x28416x14208, b=2048): 0.0264
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28416x14208, b=2048): 250.259
Elapsed time for mlp_4h_to_h (4x14208x28416, b=2048): 0.0258
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14208x28416, b=2048): 256.077

Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 232.316
MLP duration (in seconds): 0.0523
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0818
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28544x10704, b=2048): 0.0201
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28544x10704, b=2048): 249.480
Elapsed time for attention_key_query_prob (64x2048x223x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x223x2048): 67.712
Elapsed time for attention_prob_times_values (64x2048x2048x223): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x223): 76.176
Elapsed time for attention_linear_projection (4x3568x28544, b=2048): 0.0073
Throughput (in TFLOP/s) for attention_linear_projection (4x3568x28544, b=2048): 228.359
Elapsed time for mlp_h_to_4h (4x28544x14272, b=2048): 0.0265
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28544x14272, b=2048): 252.114
Elapsed time for mlp_4h_to_h (4x14272x28544, b=2048): 0.0260
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14272x28544, b=2048): 256.830

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 225.121
MLP duration (in seconds): 0.0525
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0832
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28672x10752, b=2048): 0.0202
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28672x10752, b=2048): 249.914
Elapsed time for attention_key_query_prob (64x2048x224x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x224x2048): 122.845
Elapsed time for attention_prob_times_values (64x2048x2048x224): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x224): 186.816
Elapsed time for attention_linear_projection (4x3584x28672, b=2048): 0.0070
Throughput (in TFLOP/s) for attention_linear_projection (4x3584x28672, b=2048): 242.144
Elapsed time for mlp_h_to_4h (4x28672x14336, b=2048): 0.0269
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28672x14336, b=2048): 250.176
Elapsed time for mlp_4h_to_h (4x14336x28672, b=2048): 0.0261
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14336x28672, b=2048): 257.727

Attention duration (in seconds): 0.0288
Attention throughput (in TFLOP/s): 242.305
MLP duration (in seconds): 0.0530
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0818
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28800x10800, b=2048): 0.0213
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28800x10800, b=2048): 239.681
Elapsed time for attention_key_query_prob (64x2048x225x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x225x2048): 65.350
Elapsed time for attention_prob_times_values (64x2048x2048x225): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x225): 76.074
Elapsed time for attention_linear_projection (4x3600x28800, b=2048): 0.0073
Throughput (in TFLOP/s) for attention_linear_projection (4x3600x28800, b=2048): 231.236
Elapsed time for mlp_h_to_4h (4x28800x14400, b=2048): 0.0270
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28800x14400, b=2048): 251.674
Elapsed time for mlp_4h_to_h (4x14400x28800, b=2048): 0.0263
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14400x28800, b=2048): 258.467

Attention duration (in seconds): 0.0320
Attention throughput (in TFLOP/s): 219.582
MLP duration (in seconds): 0.0533
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0853
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28928x10848, b=2048): 0.0213
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28928x10848, b=2048): 240.903
Elapsed time for attention_key_query_prob (64x2048x226x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x226x2048): 92.277
Elapsed time for attention_prob_times_values (64x2048x2048x226): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x226): 124.854
Elapsed time for attention_linear_projection (4x3616x28928, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_linear_projection (4x3616x28928, b=2048): 230.247
Elapsed time for mlp_h_to_4h (4x28928x14464, b=2048): 0.0271
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28928x14464, b=2048): 253.312
Elapsed time for mlp_4h_to_h (4x14464x28928, b=2048): 0.0264
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14464x28928, b=2048): 259.571

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 228.432
MLP duration (in seconds): 0.0535
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0845
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29056x10896, b=2048): 0.0333
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29056x10896, b=2048): 155.839
Elapsed time for attention_key_query_prob (64x2048x227x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x227x2048): 65.500
Elapsed time for attention_prob_times_values (64x2048x2048x227): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x227): 78.471
Elapsed time for attention_linear_projection (4x3632x29056, b=2048): 0.0075
Throughput (in TFLOP/s) for attention_linear_projection (4x3632x29056, b=2048): 229.213
Elapsed time for mlp_h_to_4h (4x29056x14528, b=2048): 0.0274
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29056x14528, b=2048): 252.112
Elapsed time for mlp_4h_to_h (4x14528x29056, b=2048): 0.0269
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14528x29056, b=2048): 257.323

Attention duration (in seconds): 0.0442
Attention throughput (in TFLOP/s): 161.834
MLP duration (in seconds): 0.0543
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0986
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29184x10944, b=2048): 0.0213
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29184x10944, b=2048): 245.531
Elapsed time for attention_key_query_prob (64x2048x228x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x228x2048): 93.043
Elapsed time for attention_prob_times_values (64x2048x2048x228): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x228): 126.238
Elapsed time for attention_linear_projection (4x3648x29184, b=2048): 0.0073
Throughput (in TFLOP/s) for attention_linear_projection (4x3648x29184, b=2048): 240.242
Elapsed time for mlp_h_to_4h (4x29184x14592, b=2048): 0.0276
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29184x14592, b=2048): 253.218
Elapsed time for mlp_4h_to_h (4x14592x29184, b=2048): 0.0274
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14592x29184, b=2048): 254.799

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 234.037
MLP duration (in seconds): 0.0549
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0858
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29312x10992, b=2048): 0.0625
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29312x10992, b=2048): 84.431
Elapsed time for attention_key_query_prob (64x2048x229x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x229x2048): 65.681
Elapsed time for attention_prob_times_values (64x2048x2048x229): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x229): 78.703
Elapsed time for attention_linear_projection (4x3664x29312, b=2048): 0.0077
Throughput (in TFLOP/s) for attention_linear_projection (4x3664x29312, b=2048): 229.248
Elapsed time for mlp_h_to_4h (4x29312x14656, b=2048): 0.0283
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29312x14656, b=2048): 249.107
Elapsed time for mlp_4h_to_h (4x14656x29312, b=2048): 0.0271
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14656x29312, b=2048): 259.333

Attention duration (in seconds): 0.0736
Attention throughput (in TFLOP/s): 98.929
MLP duration (in seconds): 0.0554
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1290
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29440x11040, b=2048): 0.0222
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29440x11040, b=2048): 239.703
Elapsed time for attention_key_query_prob (64x2048x230x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x230x2048): 77.999
Elapsed time for attention_prob_times_values (64x2048x2048x230): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x230): 125.342
Elapsed time for attention_linear_projection (4x3680x29440, b=2048): 0.0077
Throughput (in TFLOP/s) for attention_linear_projection (4x3680x29440, b=2048): 231.636
Elapsed time for mlp_h_to_4h (4x29440x14720, b=2048): 0.0280
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29440x14720, b=2048): 253.421
Elapsed time for mlp_4h_to_h (4x14720x29440, b=2048): 0.0279
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14720x29440, b=2048): 254.259

Attention duration (in seconds): 0.0324
Attention throughput (in TFLOP/s): 226.436
MLP duration (in seconds): 0.0559
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0884
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29568x11088, b=2048): 0.0224
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29568x11088, b=2048): 240.084
Elapsed time for attention_key_query_prob (64x2048x231x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x231x2048): 65.611
Elapsed time for attention_prob_times_values (64x2048x2048x231): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x231): 77.486
Elapsed time for attention_linear_projection (4x3696x29568, b=2048): 0.0077
Throughput (in TFLOP/s) for attention_linear_projection (4x3696x29568, b=2048): 232.814
Elapsed time for mlp_h_to_4h (4x29568x14784, b=2048): 0.0290
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29568x14784, b=2048): 247.054
Elapsed time for mlp_4h_to_h (4x14784x29568, b=2048): 0.0278
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14784x29568, b=2048): 258.025

Attention duration (in seconds): 0.0336
Attention throughput (in TFLOP/s): 220.834
MLP duration (in seconds): 0.0567
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0903
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29696x11136, b=2048): 0.0224
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29696x11136, b=2048): 241.778
Elapsed time for attention_key_query_prob (64x2048x232x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x232x2048): 119.492
Elapsed time for attention_prob_times_values (64x2048x2048x232): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x232): 188.598
Elapsed time for attention_linear_projection (4x3712x29696, b=2048): 0.0075
Throughput (in TFLOP/s) for attention_linear_projection (4x3712x29696, b=2048): 241.644
Elapsed time for mlp_h_to_4h (4x29696x14848, b=2048): 0.0285
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29696x14848, b=2048): 253.304
Elapsed time for mlp_4h_to_h (4x14848x29696, b=2048): 0.0282
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14848x29696, b=2048): 256.492

Attention duration (in seconds): 0.0316
Attention throughput (in TFLOP/s): 236.599
MLP duration (in seconds): 0.0567
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0883
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29824x11184, b=2048): 0.0220
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29824x11184, b=2048): 248.114
Elapsed time for attention_key_query_prob (64x2048x233x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x233x2048): 66.464
Elapsed time for attention_prob_times_values (64x2048x2048x233): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x233): 78.146
Elapsed time for attention_linear_projection (4x3728x29824, b=2048): 0.0078
Throughput (in TFLOP/s) for attention_linear_projection (4x3728x29824, b=2048): 233.611
Elapsed time for mlp_h_to_4h (4x29824x14912, b=2048): 0.0295
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29824x14912, b=2048): 246.950
Elapsed time for mlp_4h_to_h (4x14912x29824, b=2048): 0.0284
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14912x29824, b=2048): 256.940

Attention duration (in seconds): 0.0333
Attention throughput (in TFLOP/s): 226.285
MLP duration (in seconds): 0.0579
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0912
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29952x11232, b=2048): 0.0221
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29952x11232, b=2048): 248.989
Elapsed time for attention_key_query_prob (64x2048x234x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x234x2048): 95.164
Elapsed time for attention_prob_times_values (64x2048x2048x234): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x234): 128.958
Elapsed time for attention_linear_projection (4x3744x29952, b=2048): 0.0080
Throughput (in TFLOP/s) for attention_linear_projection (4x3744x29952, b=2048): 229.598
Elapsed time for mlp_h_to_4h (4x29952x14976, b=2048): 0.0295
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29952x14976, b=2048): 248.973
Elapsed time for mlp_4h_to_h (4x14976x29952, b=2048): 0.0331
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14976x29952, b=2048): 222.165

Attention duration (in seconds): 0.0324
Attention throughput (in TFLOP/s): 234.339
MLP duration (in seconds): 0.0626
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0950
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30080x11280, b=2048): 0.0226
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30080x11280, b=2048): 245.723
Elapsed time for attention_key_query_prob (64x2048x235x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x235x2048): 67.180
Elapsed time for attention_prob_times_values (64x2048x2048x235): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x235): 81.074
Elapsed time for attention_linear_projection (4x3760x30080, b=2048): 0.0081
Throughput (in TFLOP/s) for attention_linear_projection (4x3760x30080, b=2048): 227.671
Elapsed time for mlp_h_to_4h (4x30080x15040, b=2048): 0.0296
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30080x15040, b=2048): 250.438
Elapsed time for mlp_4h_to_h (4x15040x30080, b=2048): 0.0287
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15040x30080, b=2048): 258.660

Attention duration (in seconds): 0.0342
Attention throughput (in TFLOP/s): 224.129
MLP duration (in seconds): 0.0583
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0924
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30208x11328, b=2048): 0.0230
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30208x11328, b=2048): 244.018
Elapsed time for attention_key_query_prob (64x2048x236x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x236x2048): 95.460
Elapsed time for attention_prob_times_values (64x2048x2048x236): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x236): 131.346
Elapsed time for attention_linear_projection (4x3776x30208, b=2048): 0.0077
Throughput (in TFLOP/s) for attention_linear_projection (4x3776x30208, b=2048): 241.862
Elapsed time for mlp_h_to_4h (4x30208x15104, b=2048): 0.0298
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30208x15104, b=2048): 251.206
Elapsed time for mlp_4h_to_h (4x15104x30208, b=2048): 0.0287
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15104x30208, b=2048): 260.303

Attention duration (in seconds): 0.0330
Attention throughput (in TFLOP/s): 234.243
MLP duration (in seconds): 0.0585
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0915
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30336x11376, b=2048): 0.0225
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30336x11376, b=2048): 251.154
Elapsed time for attention_key_query_prob (64x2048x237x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x237x2048): 67.486
Elapsed time for attention_prob_times_values (64x2048x2048x237): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x237): 81.577
Elapsed time for attention_linear_projection (4x3792x30336, b=2048): 0.0082
Throughput (in TFLOP/s) for attention_linear_projection (4x3792x30336, b=2048): 228.451
Elapsed time for mlp_h_to_4h (4x30336x15168, b=2048): 0.0302
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30336x15168, b=2048): 249.837
Elapsed time for mlp_4h_to_h (4x15168x30336, b=2048): 0.0293
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15168x30336, b=2048): 257.568

Attention duration (in seconds): 0.0342
Attention throughput (in TFLOP/s): 227.824
MLP duration (in seconds): 0.0594
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0937
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30464x11424, b=2048): 0.0231
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30464x11424, b=2048): 247.118
Elapsed time for attention_key_query_prob (64x2048x238x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x238x2048): 96.843
Elapsed time for attention_prob_times_values (64x2048x2048x238): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x238): 132.295
Elapsed time for attention_linear_projection (4x3808x30464, b=2048): 0.0082
Throughput (in TFLOP/s) for attention_linear_projection (4x3808x30464, b=2048): 232.982
Elapsed time for mlp_h_to_4h (4x30464x15232, b=2048): 0.0302
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30464x15232, b=2048): 252.115
Elapsed time for mlp_4h_to_h (4x15232x30464, b=2048): 0.0294
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15232x30464, b=2048): 258.399

Attention duration (in seconds): 0.0335
Attention throughput (in TFLOP/s): 234.453
MLP duration (in seconds): 0.0596
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0931
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30592x11472, b=2048): 0.0232
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30592x11472, b=2048): 248.155
Elapsed time for attention_key_query_prob (64x2048x239x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x239x2048): 67.568
Elapsed time for attention_prob_times_values (64x2048x2048x239): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x239): 77.952
Elapsed time for attention_linear_projection (4x3824x30592, b=2048): 0.0081
Throughput (in TFLOP/s) for attention_linear_projection (4x3824x30592, b=2048): 238.004
Elapsed time for mlp_h_to_4h (4x30592x15296, b=2048): 0.0306
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30592x15296, b=2048): 250.308
Elapsed time for mlp_4h_to_h (4x15296x30592, b=2048): 0.0295
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15296x30592, b=2048): 259.480

Attention duration (in seconds): 0.0348
Attention throughput (in TFLOP/s): 227.883
MLP duration (in seconds): 0.0602
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0949
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30720x11520, b=2048): 0.0231
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30720x11520, b=2048): 251.494
Elapsed time for attention_key_query_prob (64x2048x240x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x240x2048): 123.584
Elapsed time for attention_prob_times_values (64x2048x2048x240): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x240): 197.743
Elapsed time for attention_linear_projection (4x3840x30720, b=2048): 0.0081
Throughput (in TFLOP/s) for attention_linear_projection (4x3840x30720, b=2048): 239.666
Elapsed time for mlp_h_to_4h (4x30720x15360, b=2048): 0.0307
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30720x15360, b=2048): 252.055
Elapsed time for mlp_4h_to_h (4x15360x30720, b=2048): 0.0300
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15360x30720, b=2048): 257.453

Attention duration (in seconds): 0.0328
Attention throughput (in TFLOP/s): 243.456
MLP duration (in seconds): 0.0607
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0935
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30848x11568, b=2048): 0.0245
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30848x11568, b=2048): 239.060
Elapsed time for attention_key_query_prob (64x2048x241x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x241x2048): 68.633
Elapsed time for attention_prob_times_values (64x2048x2048x241): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x241): 81.656
Elapsed time for attention_linear_projection (4x3856x30848, b=2048): 0.0085
Throughput (in TFLOP/s) for attention_linear_projection (4x3856x30848, b=2048): 229.116
Elapsed time for mlp_h_to_4h (4x30848x15424, b=2048): 0.0311
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30848x15424, b=2048): 250.934
Elapsed time for mlp_4h_to_h (4x15424x30848, b=2048): 0.0301
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15424x30848, b=2048): 258.613

Attention duration (in seconds): 0.0364
Attention throughput (in TFLOP/s): 221.074
MLP duration (in seconds): 0.0612
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0976
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30976x11616, b=2048): 0.0247
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30976x11616, b=2048): 239.078
Elapsed time for attention_key_query_prob (64x2048x242x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x242x2048): 89.894
Elapsed time for attention_prob_times_values (64x2048x2048x242): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x242): 133.530
Elapsed time for attention_linear_projection (4x3872x30976, b=2048): 0.0084
Throughput (in TFLOP/s) for attention_linear_projection (4x3872x30976, b=2048): 233.027
Elapsed time for mlp_h_to_4h (4x30976x15488, b=2048): 0.0313
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30976x15488, b=2048): 251.425
Elapsed time for mlp_4h_to_h (4x15488x30976, b=2048): 0.0303
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15488x30976, b=2048): 259.726

Attention duration (in seconds): 0.0355
Attention throughput (in TFLOP/s): 228.677
MLP duration (in seconds): 0.0615
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0970
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31104x11664, b=2048): 0.0239
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31104x11664, b=2048): 249.081
Elapsed time for attention_key_query_prob (64x2048x243x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x243x2048): 67.999
Elapsed time for attention_prob_times_values (64x2048x2048x243): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x243): 82.445
Elapsed time for attention_linear_projection (4x3888x31104, b=2048): 0.0084
Throughput (in TFLOP/s) for attention_linear_projection (4x3888x31104, b=2048): 235.209
Elapsed time for mlp_h_to_4h (4x31104x15552, b=2048): 0.0320
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31104x15552, b=2048): 247.630
Elapsed time for mlp_4h_to_h (4x15552x31104, b=2048): 0.0305
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15552x31104, b=2048): 259.879

Attention duration (in seconds): 0.0358
Attention throughput (in TFLOP/s): 228.740
MLP duration (in seconds): 0.0625
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0983
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31232x11712, b=2048): 0.0240
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31232x11712, b=2048): 249.767
Elapsed time for attention_key_query_prob (64x2048x244x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x244x2048): 98.873
Elapsed time for attention_prob_times_values (64x2048x2048x244): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x244): 135.130
Elapsed time for attention_linear_projection (4x3904x31232, b=2048): 0.0082
Throughput (in TFLOP/s) for attention_linear_projection (4x3904x31232, b=2048): 243.532
Elapsed time for mlp_h_to_4h (4x31232x15616, b=2048): 0.0319
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31232x15616, b=2048): 250.278
Elapsed time for mlp_4h_to_h (4x15616x31232, b=2048): 0.0309
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15616x31232, b=2048): 258.336

Attention duration (in seconds): 0.0345
Attention throughput (in TFLOP/s): 239.266
MLP duration (in seconds): 0.0629
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0974
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31360x11760, b=2048): 0.0238
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31360x11760, b=2048): 253.964
Elapsed time for attention_key_query_prob (64x2048x245x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x245x2048): 69.737
Elapsed time for attention_prob_times_values (64x2048x2048x245): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x245): 85.111
Elapsed time for attention_linear_projection (4x3920x31360, b=2048): 0.0088
Throughput (in TFLOP/s) for attention_linear_projection (4x3920x31360, b=2048): 229.802
Elapsed time for mlp_h_to_4h (4x31360x15680, b=2048): 0.0319
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31360x15680, b=2048): 252.640
Elapsed time for mlp_4h_to_h (4x15680x31360, b=2048): 0.0308
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15680x31360, b=2048): 261.600

Attention duration (in seconds): 0.0360
Attention throughput (in TFLOP/s): 231.173
MLP duration (in seconds): 0.0627
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0987
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31488x11808, b=2048): 0.0255
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31488x11808, b=2048): 238.686
Elapsed time for attention_key_query_prob (64x2048x246x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x246x2048): 99.523
Elapsed time for attention_prob_times_values (64x2048x2048x246): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x246): 135.671
Elapsed time for attention_linear_projection (4x3936x31488, b=2048): 0.0089
Throughput (in TFLOP/s) for attention_linear_projection (4x3936x31488, b=2048): 228.396
Elapsed time for mlp_h_to_4h (4x31488x15744, b=2048): 0.0332
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31488x15744, b=2048): 244.357
Elapsed time for mlp_4h_to_h (4x15744x31488, b=2048): 0.0313
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15744x31488, b=2048): 259.506

Attention duration (in seconds): 0.0367
Attention throughput (in TFLOP/s): 228.432
MLP duration (in seconds): 0.0645
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1013
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31616x11856, b=2048): 0.0259
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31616x11856, b=2048): 237.436
Elapsed time for attention_key_query_prob (64x2048x247x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x247x2048): 68.734
Elapsed time for attention_prob_times_values (64x2048x2048x247): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x247): 84.425
Elapsed time for attention_linear_projection (4x3952x31616, b=2048): 0.0089
Throughput (in TFLOP/s) for attention_linear_projection (4x3952x31616, b=2048): 230.776
Elapsed time for mlp_h_to_4h (4x31616x15808, b=2048): 0.0329
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31616x15808, b=2048): 248.549
Elapsed time for mlp_4h_to_h (4x15808x31616, b=2048): 0.0318
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15808x31616, b=2048): 257.768

Attention duration (in seconds): 0.0382
Attention throughput (in TFLOP/s): 221.093
MLP duration (in seconds): 0.0647
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31744x11904, b=2048): 0.0258
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31744x11904, b=2048): 240.391
Elapsed time for attention_key_query_prob (64x2048x248x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x248x2048): 126.431
Elapsed time for attention_prob_times_values (64x2048x2048x248): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x248): 198.948
Elapsed time for attention_linear_projection (4x3968x31744, b=2048): 0.0084
Throughput (in TFLOP/s) for attention_linear_projection (4x3968x31744, b=2048): 244.241
Elapsed time for mlp_h_to_4h (4x31744x15872, b=2048): 0.0329
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31744x15872, b=2048): 250.914
Elapsed time for mlp_4h_to_h (4x15872x31744, b=2048): 0.0803
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15872x31744, b=2048): 102.790

Attention duration (in seconds): 0.0359
Attention throughput (in TFLOP/s): 237.184
MLP duration (in seconds): 0.1132
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1491
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31872x11952, b=2048): 0.0247
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31872x11952, b=2048): 252.219
Elapsed time for attention_key_query_prob (64x2048x249x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x249x2048): 69.765
Elapsed time for attention_prob_times_values (64x2048x2048x249): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x249): 85.122
Elapsed time for attention_linear_projection (4x3984x31872, b=2048): 0.0093
Throughput (in TFLOP/s) for attention_linear_projection (4x3984x31872, b=2048): 224.604
Elapsed time for mlp_h_to_4h (4x31872x15936, b=2048): 0.0334
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31872x15936, b=2048): 249.245
Elapsed time for mlp_4h_to_h (4x15936x31872, b=2048): 0.0321
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15936x31872, b=2048): 259.413

Attention duration (in seconds): 0.0375
Attention throughput (in TFLOP/s): 229.074
MLP duration (in seconds): 0.0655
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32000x12000, b=2048): 0.0248
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32000x12000, b=2048): 253.368
Elapsed time for attention_key_query_prob (64x2048x250x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x250x2048): 102.169
Elapsed time for attention_prob_times_values (64x2048x2048x250): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x250): 139.551
Elapsed time for attention_linear_projection (4x4000x32000, b=2048): 0.0090
Throughput (in TFLOP/s) for attention_linear_projection (4x4000x32000, b=2048): 233.145
Elapsed time for mlp_h_to_4h (4x32000x16000, b=2048): 0.0336
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32000x16000, b=2048): 249.635
Elapsed time for mlp_4h_to_h (4x16000x32000, b=2048): 0.0390
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16000x32000, b=2048): 215.108

Attention duration (in seconds): 0.0361
Attention throughput (in TFLOP/s): 239.795
MLP duration (in seconds): 0.0726
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32128x12048, b=2048): 0.0266
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32128x12048, b=2048): 238.452
Elapsed time for attention_key_query_prob (64x2048x251x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x251x2048): 71.014
Elapsed time for attention_prob_times_values (64x2048x2048x251): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x251): 87.533
Elapsed time for attention_linear_projection (4x4016x32128, b=2048): 0.0091
Throughput (in TFLOP/s) for attention_linear_projection (4x4016x32128, b=2048): 231.244
Elapsed time for mlp_h_to_4h (4x32128x16064, b=2048): 0.0340
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32128x16064, b=2048): 248.620
Elapsed time for mlp_4h_to_h (4x16064x32128, b=2048): 0.0327
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16064x32128, b=2048): 258.513

Attention duration (in seconds): 0.0392
Attention throughput (in TFLOP/s): 222.729
MLP duration (in seconds): 0.0667
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32256x12096, b=2048): 0.0267
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32256x12096, b=2048): 239.051
Elapsed time for attention_key_query_prob (64x2048x252x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x252x2048): 103.117
Elapsed time for attention_prob_times_values (64x2048x2048x252): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x252): 142.827
Elapsed time for attention_linear_projection (4x4032x32256, b=2048): 0.0088
Throughput (in TFLOP/s) for attention_linear_projection (4x4032x32256, b=2048): 242.160
Elapsed time for mlp_h_to_4h (4x32256x16128, b=2048): 0.0391
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32256x16128, b=2048): 218.219
Elapsed time for mlp_4h_to_h (4x16128x32256, b=2048): 0.0329
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16128x32256, b=2048): 259.158

Attention duration (in seconds): 0.0378
Attention throughput (in TFLOP/s): 232.645
MLP duration (in seconds): 0.0719
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32384x12144, b=2048): 0.0271
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32384x12144, b=2048): 237.822
Elapsed time for attention_key_query_prob (64x2048x253x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x253x2048): 72.206
Elapsed time for attention_prob_times_values (64x2048x2048x253): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x253): 88.656
Elapsed time for attention_linear_projection (4x4048x32384, b=2048): 0.0093
Throughput (in TFLOP/s) for attention_linear_projection (4x4048x32384, b=2048): 231.040
Elapsed time for mlp_h_to_4h (4x32384x16192, b=2048): 0.0344
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32384x16192, b=2048): 250.062
Elapsed time for mlp_4h_to_h (4x16192x32384, b=2048): 0.0330
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16192x32384, b=2048): 260.430

Attention duration (in seconds): 0.0398
Attention throughput (in TFLOP/s): 222.669
MLP duration (in seconds): 0.0673
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32512x12192, b=2048): 0.0257
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32512x12192, b=2048): 252.428
Elapsed time for attention_key_query_prob (64x2048x254x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x254x2048): 104.639
Elapsed time for attention_prob_times_values (64x2048x2048x254): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x254): 143.204
Elapsed time for attention_linear_projection (4x4064x32512, b=2048): 0.0094
Throughput (in TFLOP/s) for attention_linear_projection (4x4064x32512, b=2048): 230.933
Elapsed time for mlp_h_to_4h (4x32512x16256, b=2048): 0.0343
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32512x16256, b=2048): 252.215
Elapsed time for mlp_4h_to_h (4x16256x32512, b=2048): 0.0335
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16256x32512, b=2048): 258.116

Attention duration (in seconds): 0.0374
Attention throughput (in TFLOP/s): 239.094
MLP duration (in seconds): 0.0679
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32640x12240, b=2048): 0.0262
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32640x12240, b=2048): 250.012
Elapsed time for attention_key_query_prob (64x2048x255x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x255x2048): 71.938
Elapsed time for attention_prob_times_values (64x2048x2048x255): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x255): 88.340
Elapsed time for attention_linear_projection (4x4080x32640, b=2048): 0.0094
Throughput (in TFLOP/s) for attention_linear_projection (4x4080x32640, b=2048): 231.958
Elapsed time for mlp_h_to_4h (4x32640x16320, b=2048): 0.0354
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32640x16320, b=2048): 246.297
Elapsed time for mlp_4h_to_h (4x16320x32640, b=2048): 0.0337
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16320x32640, b=2048): 258.828

Attention duration (in seconds): 0.0390
Attention throughput (in TFLOP/s): 230.564
MLP duration (in seconds): 0.0692
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
