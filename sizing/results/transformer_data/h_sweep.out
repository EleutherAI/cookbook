num_attention_heads: 64, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8192x6144, b=2048): 0.0036
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8192x6144, b=2048): 228.497
Elapsed time for attention_key_query_prob (64x2048x128x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x128x2048): 91.589
Elapsed time for attention_prob_times_values (64x2048x2048x128): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x128): 134.750
Elapsed time for attention_linear_projection (4x2048x8192, b=2048): 0.0013
Throughput (in TFLOP/s) for attention_linear_projection (4x2048x8192, b=2048): 216.633
Elapsed time for mlp_h_to_4h (4x8192x8192, b=2048): 0.0043
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8192x8192, b=2048): 253.989
Elapsed time for mlp_4h_to_h (4x8192x8192, b=2048): 0.0043
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8192x8192, b=2048): 257.478

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 201.521
MLP duration (in seconds): 0.0086
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8320x6240, b=2048): 0.0041
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8320x6240, b=2048): 209.815
Elapsed time for attention_key_query_prob (64x2048x130x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x130x2048): 69.467
Elapsed time for attention_prob_times_values (64x2048x2048x130): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x130): 88.627
Elapsed time for attention_linear_projection (4x2080x8320, b=2048): 0.0015
Throughput (in TFLOP/s) for attention_linear_projection (4x2080x8320, b=2048): 184.606
Elapsed time for mlp_h_to_4h (4x8320x8320, b=2048): 0.0045
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8320x8320, b=2048): 251.290
Elapsed time for mlp_4h_to_h (4x8320x8320, b=2048): 0.0046
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8320x8320, b=2048): 247.229

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 172.541
MLP duration (in seconds): 0.0091
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8448x6336, b=2048): 0.0035
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8448x6336, b=2048): 250.277
Elapsed time for attention_key_query_prob (64x2048x132x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x132x2048): 61.110
Elapsed time for attention_prob_times_values (64x2048x2048x132): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x132): 58.305
Elapsed time for attention_linear_projection (4x2112x8448, b=2048): 0.0013
Throughput (in TFLOP/s) for attention_linear_projection (4x2112x8448, b=2048): 217.048
Elapsed time for mlp_h_to_4h (4x8448x8448, b=2048): 0.0046
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8448x8448, b=2048): 252.506
Elapsed time for mlp_4h_to_h (4x8448x8448, b=2048): 0.0046
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8448x8448, b=2048): 252.753

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 181.434
MLP duration (in seconds): 0.0093
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8576x6432, b=2048): 0.0036
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8576x6432, b=2048): 248.272
Elapsed time for attention_key_query_prob (64x2048x134x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x134x2048): 70.566
Elapsed time for attention_prob_times_values (64x2048x2048x134): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x134): 88.617
Elapsed time for attention_linear_projection (4x2144x8576, b=2048): 0.0015
Throughput (in TFLOP/s) for attention_linear_projection (4x2144x8576, b=2048): 205.454
Elapsed time for mlp_h_to_4h (4x8576x8576, b=2048): 0.0053
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8576x8576, b=2048): 225.804
Elapsed time for mlp_4h_to_h (4x8576x8576, b=2048): 0.0053
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8576x8576, b=2048): 227.778

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 194.428
MLP duration (in seconds): 0.0106
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8704x6528, b=2048): 0.0038
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8704x6528, b=2048): 246.379
Elapsed time for attention_key_query_prob (64x2048x136x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x136x2048): 88.844
Elapsed time for attention_prob_times_values (64x2048x2048x136): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x136): 115.564
Elapsed time for attention_linear_projection (4x2176x8704, b=2048): 0.0014
Throughput (in TFLOP/s) for attention_linear_projection (4x2176x8704, b=2048): 218.526
Elapsed time for mlp_h_to_4h (4x8704x8704, b=2048): 0.0055
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8704x8704, b=2048): 225.473
Elapsed time for mlp_4h_to_h (4x8704x8704, b=2048): 0.0050
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8704x8704, b=2048): 248.801

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 208.546
MLP duration (in seconds): 0.0105
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8832x6624, b=2048): 0.0042
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8832x6624, b=2048): 230.337
Elapsed time for attention_key_query_prob (64x2048x138x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x138x2048): 72.267
Elapsed time for attention_prob_times_values (64x2048x2048x138): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x138): 88.056
Elapsed time for attention_linear_projection (4x2208x8832, b=2048): 0.0015
Throughput (in TFLOP/s) for attention_linear_projection (4x2208x8832, b=2048): 207.350
Elapsed time for mlp_h_to_4h (4x8832x8832, b=2048): 0.0052
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8832x8832, b=2048): 247.846
Elapsed time for mlp_4h_to_h (4x8832x8832, b=2048): 0.0050
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8832x8832, b=2048): 254.337

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 188.430
MLP duration (in seconds): 0.0102
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8960x6720, b=2048): 0.0040
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8960x6720, b=2048): 249.348
Elapsed time for attention_key_query_prob (64x2048x140x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x140x2048): 73.520
Elapsed time for attention_prob_times_values (64x2048x2048x140): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x140): 89.029
Elapsed time for attention_linear_projection (4x2240x8960, b=2048): 0.0017
Throughput (in TFLOP/s) for attention_linear_projection (4x2240x8960, b=2048): 192.764
Elapsed time for mlp_h_to_4h (4x8960x8960, b=2048): 0.0052
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8960x8960, b=2048): 252.052
Elapsed time for mlp_4h_to_h (4x8960x8960, b=2048): 0.0051
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8960x8960, b=2048): 256.505

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 194.674
MLP duration (in seconds): 0.0103
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9088x6816, b=2048): 0.0040
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9088x6816, b=2048): 253.725
Elapsed time for attention_key_query_prob (64x2048x142x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x142x2048): 73.727
Elapsed time for attention_prob_times_values (64x2048x2048x142): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x142): 89.668
Elapsed time for attention_linear_projection (4x2272x9088, b=2048): 0.0016
Throughput (in TFLOP/s) for attention_linear_projection (4x2272x9088, b=2048): 206.388
Elapsed time for mlp_h_to_4h (4x9088x9088, b=2048): 0.0055
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9088x9088, b=2048): 245.402
Elapsed time for mlp_4h_to_h (4x9088x9088, b=2048): 0.0055
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9088x9088, b=2048): 245.307

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 200.132
MLP duration (in seconds): 0.0110
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9216x6912, b=2048): 0.0040
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9216x6912, b=2048): 259.962
Elapsed time for attention_key_query_prob (64x2048x144x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x144x2048): 93.312
Elapsed time for attention_prob_times_values (64x2048x2048x144): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x144): 127.111
Elapsed time for attention_linear_projection (4x2304x9216, b=2048): 0.0016
Throughput (in TFLOP/s) for attention_linear_projection (4x2304x9216, b=2048): 216.526
Elapsed time for mlp_h_to_4h (4x9216x9216, b=2048): 0.0055
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9216x9216, b=2048): 254.332
Elapsed time for mlp_4h_to_h (4x9216x9216, b=2048): 0.0055
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9216x9216, b=2048): 252.560

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 219.064
MLP duration (in seconds): 0.0110
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9344x7008, b=2048): 0.0043
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9344x7008, b=2048): 250.191
Elapsed time for attention_key_query_prob (64x2048x146x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x146x2048): 76.297
Elapsed time for attention_prob_times_values (64x2048x2048x146): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x146): 91.501
Elapsed time for attention_linear_projection (4x2336x9344, b=2048): 0.0021
Throughput (in TFLOP/s) for attention_linear_projection (4x2336x9344, b=2048): 172.412
Elapsed time for mlp_h_to_4h (4x9344x9344, b=2048): 0.0063
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9344x9344, b=2048): 227.477
Elapsed time for mlp_4h_to_h (4x9344x9344, b=2048): 0.0056
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9344x9344, b=2048): 254.676

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 192.478
MLP duration (in seconds): 0.0119
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9472x7104, b=2048): 0.0046
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9472x7104, b=2048): 238.921
Elapsed time for attention_key_query_prob (64x2048x148x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x148x2048): 76.385
Elapsed time for attention_prob_times_values (64x2048x2048x148): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x148): 92.729
Elapsed time for attention_linear_projection (4x2368x9472, b=2048): 0.0017
Throughput (in TFLOP/s) for attention_linear_projection (4x2368x9472, b=2048): 216.422
Elapsed time for mlp_h_to_4h (4x9472x9472, b=2048): 0.0060
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9472x9472, b=2048): 245.332
Elapsed time for mlp_4h_to_h (4x9472x9472, b=2048): 0.0060
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9472x9472, b=2048): 243.808

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 198.413
MLP duration (in seconds): 0.0120
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9600x7200, b=2048): 0.0048
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9600x7200, b=2048): 236.560
Elapsed time for attention_key_query_prob (64x2048x150x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x150x2048): 77.971
Elapsed time for attention_prob_times_values (64x2048x2048x150): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x150): 93.695
Elapsed time for attention_linear_projection (4x2400x9600, b=2048): 0.0018
Throughput (in TFLOP/s) for attention_linear_projection (4x2400x9600, b=2048): 209.986
Elapsed time for mlp_h_to_4h (4x9600x9600, b=2048): 0.0066
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9600x9600, b=2048): 229.738
Elapsed time for mlp_4h_to_h (4x9600x9600, b=2048): 0.0064
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9600x9600, b=2048): 235.662

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 197.118
MLP duration (in seconds): 0.0130
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9728x7296, b=2048): 0.0047
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9728x7296, b=2048): 246.047
Elapsed time for attention_key_query_prob (64x2048x152x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x152x2048): 98.213
Elapsed time for attention_prob_times_values (64x2048x2048x152): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x152): 133.440
Elapsed time for attention_linear_projection (4x2432x9728, b=2048): 0.0017
Throughput (in TFLOP/s) for attention_linear_projection (4x2432x9728, b=2048): 225.086
Elapsed time for mlp_h_to_4h (4x9728x9728, b=2048): 0.0069
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9728x9728, b=2048): 226.277
Elapsed time for mlp_4h_to_h (4x9728x9728, b=2048): 0.0066
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9728x9728, b=2048): 234.806

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 217.179
MLP duration (in seconds): 0.0135
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9856x7392, b=2048): 0.0050
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9856x7392, b=2048): 237.651
Elapsed time for attention_key_query_prob (64x2048x154x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x154x2048): 79.554
Elapsed time for attention_prob_times_values (64x2048x2048x154): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x154): 95.768
Elapsed time for attention_linear_projection (4x2464x9856, b=2048): 0.0026
Throughput (in TFLOP/s) for attention_linear_projection (4x2464x9856, b=2048): 153.983
Elapsed time for mlp_h_to_4h (4x9856x9856, b=2048): 0.0071
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9856x9856, b=2048): 225.127
Elapsed time for mlp_4h_to_h (4x9856x9856, b=2048): 0.0069
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9856x9856, b=2048): 230.873

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 184.757
MLP duration (in seconds): 0.0140
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9984x7488, b=2048): 0.0050
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9984x7488, b=2048): 242.735
Elapsed time for attention_key_query_prob (64x2048x156x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x156x2048): 80.532
Elapsed time for attention_prob_times_values (64x2048x2048x156): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x156): 97.066
Elapsed time for attention_linear_projection (4x2496x9984, b=2048): 0.0020
Throughput (in TFLOP/s) for attention_linear_projection (4x2496x9984, b=2048): 206.773
Elapsed time for mlp_h_to_4h (4x9984x9984, b=2048): 0.0072
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9984x9984, b=2048): 227.008
Elapsed time for mlp_4h_to_h (4x9984x9984, b=2048): 0.0073
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9984x9984, b=2048): 224.192

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 201.788
MLP duration (in seconds): 0.0145
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10112x7584, b=2048): 0.0052
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10112x7584, b=2048): 240.840
Elapsed time for attention_key_query_prob (64x2048x158x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x158x2048): 81.266
Elapsed time for attention_prob_times_values (64x2048x2048x158): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x158): 97.985
Elapsed time for attention_linear_projection (4x2528x10112, b=2048): 0.0021
Throughput (in TFLOP/s) for attention_linear_projection (4x2528x10112, b=2048): 201.501
Elapsed time for mlp_h_to_4h (4x10112x10112, b=2048): 0.0074
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10112x10112, b=2048): 225.802
Elapsed time for mlp_4h_to_h (4x10112x10112, b=2048): 0.0076
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10112x10112, b=2048): 220.585

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 200.428
MLP duration (in seconds): 0.0150
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10240x7680, b=2048): 0.0053
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10240x7680, b=2048): 244.750
Elapsed time for attention_key_query_prob (64x2048x160x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x160x2048): 103.680
Elapsed time for attention_prob_times_values (64x2048x2048x160): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x160): 139.269
Elapsed time for attention_linear_projection (4x2560x10240, b=2048): 0.0026
Throughput (in TFLOP/s) for attention_linear_projection (4x2560x10240, b=2048): 163.263
Elapsed time for mlp_h_to_4h (4x10240x10240, b=2048): 0.0075
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10240x10240, b=2048): 228.240
Elapsed time for mlp_4h_to_h (4x10240x10240, b=2048): 0.0074
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10240x10240, b=2048): 231.050

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 202.321
MLP duration (in seconds): 0.0150
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10368x7776, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10368x7776, b=2048): 240.819
Elapsed time for attention_key_query_prob (64x2048x162x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x162x2048): 77.549
Elapsed time for attention_prob_times_values (64x2048x2048x162): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x162): 99.263
Elapsed time for attention_linear_projection (4x2592x10368, b=2048): 0.0021
Throughput (in TFLOP/s) for attention_linear_projection (4x2592x10368, b=2048): 209.193
Elapsed time for mlp_h_to_4h (4x10368x10368, b=2048): 0.0075
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10368x10368, b=2048): 234.501
Elapsed time for mlp_4h_to_h (4x10368x10368, b=2048): 0.0077
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10368x10368, b=2048): 228.658

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 201.841
MLP duration (in seconds): 0.0152
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10496x7872, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10496x7872, b=2048): 246.159
Elapsed time for attention_key_query_prob (64x2048x164x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x164x2048): 79.163
Elapsed time for attention_prob_times_values (64x2048x2048x164): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x164): 100.352
Elapsed time for attention_linear_projection (4x2624x10496, b=2048): 0.0021
Throughput (in TFLOP/s) for attention_linear_projection (4x2624x10496, b=2048): 214.415
Elapsed time for mlp_h_to_4h (4x10496x10496, b=2048): 0.0076
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10496x10496, b=2048): 237.798
Elapsed time for mlp_4h_to_h (4x10496x10496, b=2048): 0.0078
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10496x10496, b=2048): 230.010

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 206.500
MLP duration (in seconds): 0.0154
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10624x7968, b=2048): 0.0058
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10624x7968, b=2048): 238.070
Elapsed time for attention_key_query_prob (64x2048x166x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x166x2048): 79.447
Elapsed time for attention_prob_times_values (64x2048x2048x166): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x166): 101.410
Elapsed time for attention_linear_projection (4x2656x10624, b=2048): 0.0021
Throughput (in TFLOP/s) for attention_linear_projection (4x2656x10624, b=2048): 215.143
Elapsed time for mlp_h_to_4h (4x10624x10624, b=2048): 0.0080
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10624x10624, b=2048): 231.104
Elapsed time for mlp_4h_to_h (4x10624x10624, b=2048): 0.0080
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10624x10624, b=2048): 229.776

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 203.253
MLP duration (in seconds): 0.0160
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10752x8064, b=2048): 0.0058
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10752x8064, b=2048): 245.317
Elapsed time for attention_key_query_prob (64x2048x168x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x168x2048): 100.746
Elapsed time for attention_prob_times_values (64x2048x2048x168): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x168): 144.722
Elapsed time for attention_linear_projection (4x2688x10752, b=2048): 0.0022
Throughput (in TFLOP/s) for attention_linear_projection (4x2688x10752, b=2048): 216.988
Elapsed time for mlp_h_to_4h (4x10752x10752, b=2048): 0.0079
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10752x10752, b=2048): 238.763
Elapsed time for mlp_4h_to_h (4x10752x10752, b=2048): 0.0077
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10752x10752, b=2048): 245.037

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 218.562
MLP duration (in seconds): 0.0157
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10880x8160, b=2048): 0.0059
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10880x8160, b=2048): 245.078
Elapsed time for attention_key_query_prob (64x2048x170x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x170x2048): 81.206
Elapsed time for attention_prob_times_values (64x2048x2048x170): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x170): 103.854
Elapsed time for attention_linear_projection (4x2720x10880, b=2048): 0.0023
Throughput (in TFLOP/s) for attention_linear_projection (4x2720x10880, b=2048): 211.311
Elapsed time for mlp_h_to_4h (4x10880x10880, b=2048): 0.0085
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10880x10880, b=2048): 229.151
Elapsed time for mlp_4h_to_h (4x10880x10880, b=2048): 0.0085
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10880x10880, b=2048): 228.990

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 207.378
MLP duration (in seconds): 0.0169
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0272
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11008x8256, b=2048): 0.0062
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11008x8256, b=2048): 239.130
Elapsed time for attention_key_query_prob (64x2048x172x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x172x2048): 82.005
Elapsed time for attention_prob_times_values (64x2048x2048x172): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x172): 105.707
Elapsed time for attention_linear_projection (4x2752x11008, b=2048): 0.0023
Throughput (in TFLOP/s) for attention_linear_projection (4x2752x11008, b=2048): 218.882
Elapsed time for mlp_h_to_4h (4x11008x11008, b=2048): 0.0082
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11008x11008, b=2048): 241.745
Elapsed time for mlp_4h_to_h (4x11008x11008, b=2048): 0.0081
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11008x11008, b=2048): 244.313

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 206.788
MLP duration (in seconds): 0.0163
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11136x8352, b=2048): 0.0067
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11136x8352, b=2048): 227.187
Elapsed time for attention_key_query_prob (64x2048x174x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x174x2048): 82.210
Elapsed time for attention_prob_times_values (64x2048x2048x174): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x174): 100.157
Elapsed time for attention_linear_projection (4x2784x11136, b=2048): 0.0024
Throughput (in TFLOP/s) for attention_linear_projection (4x2784x11136, b=2048): 212.581
Elapsed time for mlp_h_to_4h (4x11136x11136, b=2048): 0.0086
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11136x11136, b=2048): 235.999
Elapsed time for mlp_4h_to_h (4x11136x11136, b=2048): 0.0086
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11136x11136, b=2048): 236.039

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 198.697
MLP duration (in seconds): 0.0172
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11264x8448, b=2048): 0.0066
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11264x8448, b=2048): 235.131
Elapsed time for attention_key_query_prob (64x2048x176x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x176x2048): 104.212
Elapsed time for attention_prob_times_values (64x2048x2048x176): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x176): 151.555
Elapsed time for attention_linear_projection (4x2816x11264, b=2048): 0.0024
Throughput (in TFLOP/s) for attention_linear_projection (4x2816x11264, b=2048): 220.176
Elapsed time for mlp_h_to_4h (4x11264x11264, b=2048): 0.0091
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11264x11264, b=2048): 229.133
Elapsed time for mlp_4h_to_h (4x11264x11264, b=2048): 0.0090
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11264x11264, b=2048): 232.073

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 215.541
MLP duration (in seconds): 0.0180
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0286
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11392x8544, b=2048): 0.0069
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11392x8544, b=2048): 231.908
Elapsed time for attention_key_query_prob (64x2048x178x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x178x2048): 50.904
Elapsed time for attention_prob_times_values (64x2048x2048x178): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x178): 108.125
Elapsed time for attention_linear_projection (4x2848x11392, b=2048): 0.0025
Throughput (in TFLOP/s) for attention_linear_projection (4x2848x11392, b=2048): 212.158
Elapsed time for mlp_h_to_4h (4x11392x11392, b=2048): 0.0088
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11392x11392, b=2048): 242.568
Elapsed time for mlp_4h_to_h (4x11392x11392, b=2048): 0.0088
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11392x11392, b=2048): 241.976

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 190.841
MLP duration (in seconds): 0.0176
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11520x8640, b=2048): 0.0073
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11520x8640, b=2048): 224.125
Elapsed time for attention_key_query_prob (64x2048x180x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x180x2048): 85.242
Elapsed time for attention_prob_times_values (64x2048x2048x180): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x180): 108.086
Elapsed time for attention_linear_projection (4x2880x11520, b=2048): 0.0025
Throughput (in TFLOP/s) for attention_linear_projection (4x2880x11520, b=2048): 220.775
Elapsed time for mlp_h_to_4h (4x11520x11520, b=2048): 0.0095
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11520x11520, b=2048): 229.967
Elapsed time for mlp_4h_to_h (4x11520x11520, b=2048): 0.0090
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11520x11520, b=2048): 241.821

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 201.225
MLP duration (in seconds): 0.0184
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0302
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11648x8736, b=2048): 0.0071
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11648x8736, b=2048): 233.650
Elapsed time for attention_key_query_prob (64x2048x182x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x182x2048): 86.189
Elapsed time for attention_prob_times_values (64x2048x2048x182): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x182): 109.317
Elapsed time for attention_linear_projection (4x2912x11648, b=2048): 0.0026
Throughput (in TFLOP/s) for attention_linear_projection (4x2912x11648, b=2048): 214.948
Elapsed time for mlp_h_to_4h (4x11648x11648, b=2048): 0.0091
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11648x11648, b=2048): 243.074
Elapsed time for mlp_4h_to_h (4x11648x11648, b=2048): 0.0090
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11648x11648, b=2048): 246.382

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 205.845
MLP duration (in seconds): 0.0182
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11776x8832, b=2048): 0.0073
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11776x8832, b=2048): 233.898
Elapsed time for attention_key_query_prob (64x2048x184x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x184x2048): 108.549
Elapsed time for attention_prob_times_values (64x2048x2048x184): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x184): 155.998
Elapsed time for attention_linear_projection (4x2944x11776, b=2048): 0.0025
Throughput (in TFLOP/s) for attention_linear_projection (4x2944x11776, b=2048): 223.322
Elapsed time for mlp_h_to_4h (4x11776x11776, b=2048): 0.0096
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11776x11776, b=2048): 236.543
Elapsed time for mlp_4h_to_h (4x11776x11776, b=2048): 0.0093
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11776x11776, b=2048): 244.562

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 217.164
MLP duration (in seconds): 0.0189
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11904x8928, b=2048): 0.0075
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11904x8928, b=2048): 230.800
Elapsed time for attention_key_query_prob (64x2048x186x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x186x2048): 87.185
Elapsed time for attention_prob_times_values (64x2048x2048x186): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x186): 112.651
Elapsed time for attention_linear_projection (4x2976x11904, b=2048): 0.0027
Throughput (in TFLOP/s) for attention_linear_projection (4x2976x11904, b=2048): 217.209
Elapsed time for mlp_h_to_4h (4x11904x11904, b=2048): 0.0100
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11904x11904, b=2048): 233.054
Elapsed time for mlp_4h_to_h (4x11904x11904, b=2048): 0.0099
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11904x11904, b=2048): 235.636

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 205.855
MLP duration (in seconds): 0.0198
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0321
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12032x9024, b=2048): 0.0079
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12032x9024, b=2048): 225.930
Elapsed time for attention_key_query_prob (64x2048x188x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x188x2048): 87.684
Elapsed time for attention_prob_times_values (64x2048x2048x188): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x188): 113.101
Elapsed time for attention_linear_projection (4x3008x12032, b=2048): 0.0027
Throughput (in TFLOP/s) for attention_linear_projection (4x3008x12032, b=2048): 221.609
Elapsed time for mlp_h_to_4h (4x12032x12032, b=2048): 0.0094
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12032x12032, b=2048): 253.425
Elapsed time for mlp_4h_to_h (4x12032x12032, b=2048): 0.0094
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12032x12032, b=2048): 253.367

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 204.380
MLP duration (in seconds): 0.0187
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0313
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12160x9120, b=2048): 0.0075
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12160x9120, b=2048): 241.245
Elapsed time for attention_key_query_prob (64x2048x190x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x190x2048): 89.264
Elapsed time for attention_prob_times_values (64x2048x2048x190): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x190): 115.727
Elapsed time for attention_linear_projection (4x3040x12160, b=2048): 0.0028
Throughput (in TFLOP/s) for attention_linear_projection (4x3040x12160, b=2048): 218.860
Elapsed time for mlp_h_to_4h (4x12160x12160, b=2048): 0.0102
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12160x12160, b=2048): 238.533
Elapsed time for mlp_4h_to_h (4x12160x12160, b=2048): 0.0104
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12160x12160, b=2048): 232.592

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 213.147
MLP duration (in seconds): 0.0206
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0329
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12288x9216, b=2048): 0.0075
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12288x9216, b=2048): 247.369
Elapsed time for attention_key_query_prob (64x2048x192x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x192x2048): 114.075
Elapsed time for attention_prob_times_values (64x2048x2048x192): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x192): 165.270
Elapsed time for attention_linear_projection (4x3072x12288, b=2048): 0.0027
Throughput (in TFLOP/s) for attention_linear_projection (4x3072x12288, b=2048): 225.630
Elapsed time for mlp_h_to_4h (4x12288x12288, b=2048): 0.0101
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12288x12288, b=2048): 244.482
Elapsed time for mlp_4h_to_h (4x12288x12288, b=2048): 0.0096
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12288x12288, b=2048): 256.922

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 227.721
MLP duration (in seconds): 0.0197
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12416x9312, b=2048): 0.0077
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12416x9312, b=2048): 245.389
Elapsed time for attention_key_query_prob (64x2048x194x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x194x2048): 86.436
Elapsed time for attention_prob_times_values (64x2048x2048x194): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x194): 103.055
Elapsed time for attention_linear_projection (4x3104x12416, b=2048): 0.0030
Throughput (in TFLOP/s) for attention_linear_projection (4x3104x12416, b=2048): 213.926
Elapsed time for mlp_h_to_4h (4x12416x12416, b=2048): 0.0100
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12416x12416, b=2048): 253.102
Elapsed time for mlp_4h_to_h (4x12416x12416, b=2048): 0.0099
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12416x12416, b=2048): 255.655

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 212.157
MLP duration (in seconds): 0.0199
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0327
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12544x9408, b=2048): 0.0078
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12544x9408, b=2048): 247.955
Elapsed time for attention_key_query_prob (64x2048x196x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x196x2048): 86.134
Elapsed time for attention_prob_times_values (64x2048x2048x196): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x196): 114.518
Elapsed time for attention_linear_projection (4x3136x12544, b=2048): 0.0029
Throughput (in TFLOP/s) for attention_linear_projection (4x3136x12544, b=2048): 225.744
Elapsed time for mlp_h_to_4h (4x12544x12544, b=2048): 0.0111
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12544x12544, b=2048): 232.491
Elapsed time for mlp_4h_to_h (4x12544x12544, b=2048): 0.0110
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12544x12544, b=2048): 235.252

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 217.962
MLP duration (in seconds): 0.0220
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0348
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12672x9504, b=2048): 0.0089
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12672x9504, b=2048): 220.617
Elapsed time for attention_key_query_prob (64x2048x198x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x198x2048): 87.389
Elapsed time for attention_prob_times_values (64x2048x2048x198): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x198): 113.047
Elapsed time for attention_linear_projection (4x3168x12672, b=2048): 0.0033
Throughput (in TFLOP/s) for attention_linear_projection (4x3168x12672, b=2048): 198.313
Elapsed time for mlp_h_to_4h (4x12672x12672, b=2048): 0.0108
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12672x12672, b=2048): 244.098
Elapsed time for mlp_4h_to_h (4x12672x12672, b=2048): 0.0106
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12672x12672, b=2048): 247.055

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 197.229
MLP duration (in seconds): 0.0214
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0358
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12800x9600, b=2048): 0.0089
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12800x9600, b=2048): 226.066
Elapsed time for attention_key_query_prob (64x2048x200x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x200x2048): 108.914
Elapsed time for attention_prob_times_values (64x2048x2048x200): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x200): 169.118
Elapsed time for attention_linear_projection (4x3200x12800, b=2048): 0.0030
Throughput (in TFLOP/s) for attention_linear_projection (4x3200x12800, b=2048): 223.784
Elapsed time for mlp_h_to_4h (4x12800x12800, b=2048): 0.0108
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12800x12800, b=2048): 249.258
Elapsed time for mlp_4h_to_h (4x12800x12800, b=2048): 0.0107
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12800x12800, b=2048): 249.989

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 214.347
MLP duration (in seconds): 0.0215
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0350
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12928x9696, b=2048): 0.0081
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12928x9696, b=2048): 252.306
Elapsed time for attention_key_query_prob (64x2048x202x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x202x2048): 87.981
Elapsed time for attention_prob_times_values (64x2048x2048x202): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x202): 112.869
Elapsed time for attention_linear_projection (4x3232x12928, b=2048): 0.0031
Throughput (in TFLOP/s) for attention_linear_projection (4x3232x12928, b=2048): 217.558
Elapsed time for mlp_h_to_4h (4x12928x12928, b=2048): 0.0111
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12928x12928, b=2048): 247.352
Elapsed time for mlp_4h_to_h (4x12928x12928, b=2048): 0.0117
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12928x12928, b=2048): 234.169

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 219.230
MLP duration (in seconds): 0.0228
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0362
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x9792, b=2048): 0.0084
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x9792, b=2048): 248.294
Elapsed time for attention_key_query_prob (64x2048x204x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x204x2048): 88.647
Elapsed time for attention_prob_times_values (64x2048x2048x204): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x204): 114.871
Elapsed time for attention_linear_projection (4x3264x13056, b=2048): 0.0031
Throughput (in TFLOP/s) for attention_linear_projection (4x3264x13056, b=2048): 224.610
Elapsed time for mlp_h_to_4h (4x13056x13056, b=2048): 0.0112
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x13056, b=2048): 249.619
Elapsed time for mlp_4h_to_h (4x13056x13056, b=2048): 0.0111
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13056x13056, b=2048): 252.236

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 219.308
MLP duration (in seconds): 0.0223
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0360
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13184x9888, b=2048): 0.0093
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13184x9888, b=2048): 229.976
Elapsed time for attention_key_query_prob (64x2048x206x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x206x2048): 89.429
Elapsed time for attention_prob_times_values (64x2048x2048x206): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x206): 114.962
Elapsed time for attention_linear_projection (4x3296x13184, b=2048): 0.0032
Throughput (in TFLOP/s) for attention_linear_projection (4x3296x13184, b=2048): 223.499
Elapsed time for mlp_h_to_4h (4x13184x13184, b=2048): 0.0112
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13184x13184, b=2048): 255.032
Elapsed time for mlp_4h_to_h (4x13184x13184, b=2048): 0.0116
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13184x13184, b=2048): 246.338

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 209.182
MLP duration (in seconds): 0.0227
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0374
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13312x9984, b=2048): 0.0095
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13312x9984, b=2048): 229.060
Elapsed time for attention_key_query_prob (64x2048x208x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x208x2048): 113.960
Elapsed time for attention_prob_times_values (64x2048x2048x208): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x208): 173.923
Elapsed time for attention_linear_projection (4x3328x13312, b=2048): 0.0032
Throughput (in TFLOP/s) for attention_linear_projection (4x3328x13312, b=2048): 227.655
Elapsed time for mlp_h_to_4h (4x13312x13312, b=2048): 0.0117
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13312x13312, b=2048): 248.059
Elapsed time for mlp_4h_to_h (4x13312x13312, b=2048): 0.0116
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13312x13312, b=2048): 250.648

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 218.396
MLP duration (in seconds): 0.0233
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0376
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13440x10080, b=2048): 0.0087
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13440x10080, b=2048): 255.869
Elapsed time for attention_key_query_prob (64x2048x210x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x210x2048): 91.430
Elapsed time for attention_prob_times_values (64x2048x2048x210): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x210): 116.301
Elapsed time for attention_linear_projection (4x3360x13440, b=2048): 0.0034
Throughput (in TFLOP/s) for attention_linear_projection (4x3360x13440, b=2048): 218.801
Elapsed time for mlp_h_to_4h (4x13440x13440, b=2048): 0.0117
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13440x13440, b=2048): 252.806
Elapsed time for mlp_4h_to_h (4x13440x13440, b=2048): 0.0116
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13440x13440, b=2048): 255.334

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 223.369
MLP duration (in seconds): 0.0233
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0376
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13568x10176, b=2048): 0.0091
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13568x10176, b=2048): 247.702
Elapsed time for attention_key_query_prob (64x2048x212x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x212x2048): 91.734
Elapsed time for attention_prob_times_values (64x2048x2048x212): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x212): 117.843
Elapsed time for attention_linear_projection (4x3392x13568, b=2048): 0.0033
Throughput (in TFLOP/s) for attention_linear_projection (4x3392x13568, b=2048): 226.681
Elapsed time for mlp_h_to_4h (4x13568x13568, b=2048): 0.0120
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13568x13568, b=2048): 251.824
Elapsed time for mlp_4h_to_h (4x13568x13568, b=2048): 0.0120
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13568x13568, b=2048): 251.413

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 221.186
MLP duration (in seconds): 0.0240
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0386
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13696x10272, b=2048): 0.0093
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13696x10272, b=2048): 248.728
Elapsed time for attention_key_query_prob (64x2048x214x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x214x2048): 59.529
Elapsed time for attention_prob_times_values (64x2048x2048x214): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x214): 119.426
Elapsed time for attention_linear_projection (4x3424x13696, b=2048): 0.0035
Throughput (in TFLOP/s) for attention_linear_projection (4x3424x13696, b=2048): 221.439
Elapsed time for mlp_h_to_4h (4x13696x13696, b=2048): 0.0121
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13696x13696, b=2048): 253.076
Elapsed time for mlp_4h_to_h (4x13696x13696, b=2048): 0.0121
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13696x13696, b=2048): 253.649

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 211.347
MLP duration (in seconds): 0.0243
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0399
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13824x10368, b=2048): 0.0094
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13824x10368, b=2048): 250.869
Elapsed time for attention_key_query_prob (64x2048x216x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x216x2048): 117.656
Elapsed time for attention_prob_times_values (64x2048x2048x216): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x216): 178.754
Elapsed time for attention_linear_projection (4x3456x13824, b=2048): 0.0035
Throughput (in TFLOP/s) for attention_linear_projection (4x3456x13824, b=2048): 225.986
Elapsed time for mlp_h_to_4h (4x13824x13824, b=2048): 0.0124
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13824x13824, b=2048): 251.614
Elapsed time for mlp_4h_to_h (4x13824x13824, b=2048): 0.0124
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13824x13824, b=2048): 251.590

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 232.591
MLP duration (in seconds): 0.0249
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0393
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13952x10464, b=2048): 0.0096
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13952x10464, b=2048): 247.982
Elapsed time for attention_key_query_prob (64x2048x218x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x218x2048): 93.379
Elapsed time for attention_prob_times_values (64x2048x2048x218): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x218): 119.964
Elapsed time for attention_linear_projection (4x3488x13952, b=2048): 0.0036
Throughput (in TFLOP/s) for attention_linear_projection (4x3488x13952, b=2048): 224.263
Elapsed time for mlp_h_to_4h (4x13952x13952, b=2048): 0.0127
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13952x13952, b=2048): 250.939
Elapsed time for mlp_4h_to_h (4x13952x13952, b=2048): 0.0126
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13952x13952, b=2048): 253.974

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 221.864
MLP duration (in seconds): 0.0253
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0407
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14080x10560, b=2048): 0.0096
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14080x10560, b=2048): 253.217
Elapsed time for attention_key_query_prob (64x2048x220x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x220x2048): 94.325
Elapsed time for attention_prob_times_values (64x2048x2048x220): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x220): 122.836
Elapsed time for attention_linear_projection (4x3520x14080, b=2048): 0.0036
Throughput (in TFLOP/s) for attention_linear_projection (4x3520x14080, b=2048): 225.912
Elapsed time for mlp_h_to_4h (4x14080x14080, b=2048): 0.0128
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14080x14080, b=2048): 253.162
Elapsed time for mlp_4h_to_h (4x14080x14080, b=2048): 0.0130
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14080x14080, b=2048): 250.067

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 225.834
MLP duration (in seconds): 0.0258
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0412
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14208x10656, b=2048): 0.0101
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14208x10656, b=2048): 245.422
Elapsed time for attention_key_query_prob (64x2048x222x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x222x2048): 95.987
Elapsed time for attention_prob_times_values (64x2048x2048x222): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x222): 115.184
Elapsed time for attention_linear_projection (4x3552x14208, b=2048): 0.0037
Throughput (in TFLOP/s) for attention_linear_projection (4x3552x14208, b=2048): 224.979
Elapsed time for mlp_h_to_4h (4x14208x14208, b=2048): 0.0132
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14208x14208, b=2048): 251.004
Elapsed time for mlp_4h_to_h (4x14208x14208, b=2048): 0.0133
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14208x14208, b=2048): 248.161

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 220.797
MLP duration (in seconds): 0.0265
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0426
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14336x10752, b=2048): 0.0101
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14336x10752, b=2048): 250.876
Elapsed time for attention_key_query_prob (64x2048x224x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x224x2048): 72.212
Elapsed time for attention_prob_times_values (64x2048x2048x224): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x224): 186.264
Elapsed time for attention_linear_projection (4x3584x14336, b=2048): 0.0037
Throughput (in TFLOP/s) for attention_linear_projection (4x3584x14336, b=2048): 230.066
Elapsed time for mlp_h_to_4h (4x14336x14336, b=2048): 0.0134
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14336x14336, b=2048): 250.626
Elapsed time for mlp_4h_to_h (4x14336x14336, b=2048): 0.0136
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14336x14336, b=2048): 247.192

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 224.972
MLP duration (in seconds): 0.0271
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0431
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14464x10848, b=2048): 0.0107
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14464x10848, b=2048): 240.160
Elapsed time for attention_key_query_prob (64x2048x226x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x226x2048): 91.877
Elapsed time for attention_prob_times_values (64x2048x2048x226): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x226): 125.316
Elapsed time for attention_linear_projection (4x3616x14464, b=2048): 0.0038
Throughput (in TFLOP/s) for attention_linear_projection (4x3616x14464, b=2048): 225.311
Elapsed time for mlp_h_to_4h (4x14464x14464, b=2048): 0.0136
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14464x14464, b=2048): 252.638
Elapsed time for mlp_4h_to_h (4x14464x14464, b=2048): 0.0138
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14464x14464, b=2048): 249.235

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 218.519
MLP duration (in seconds): 0.0273
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0441
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14592x10944, b=2048): 0.0106
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14592x10944, b=2048): 246.643
Elapsed time for attention_key_query_prob (64x2048x228x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x228x2048): 92.224
Elapsed time for attention_prob_times_values (64x2048x2048x228): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x228): 126.114
Elapsed time for attention_linear_projection (4x3648x14592, b=2048): 0.0038
Throughput (in TFLOP/s) for attention_linear_projection (4x3648x14592, b=2048): 230.733
Elapsed time for mlp_h_to_4h (4x14592x14592, b=2048): 0.0139
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14592x14592, b=2048): 251.360
Elapsed time for mlp_4h_to_h (4x14592x14592, b=2048): 0.0139
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14592x14592, b=2048): 251.300

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 223.745
MLP duration (in seconds): 0.0278
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0444
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14720x11040, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14720x11040, b=2048): 246.198
Elapsed time for attention_key_query_prob (64x2048x230x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x230x2048): 69.453
Elapsed time for attention_prob_times_values (64x2048x2048x230): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x230): 126.044
Elapsed time for attention_linear_projection (4x3680x14720, b=2048): 0.0044
Throughput (in TFLOP/s) for attention_linear_projection (4x3680x14720, b=2048): 202.014
Elapsed time for mlp_h_to_4h (4x14720x14720, b=2048): 0.0142
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14720x14720, b=2048): 250.337
Elapsed time for mlp_4h_to_h (4x14720x14720, b=2048): 0.0138
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14720x14720, b=2048): 256.680

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 211.350
MLP duration (in seconds): 0.0280
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0460
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14848x11136, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14848x11136, b=2048): 247.745
Elapsed time for attention_key_query_prob (64x2048x232x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x232x2048): 118.087
Elapsed time for attention_prob_times_values (64x2048x2048x232): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x232): 139.386
Elapsed time for attention_linear_projection (4x3712x14848, b=2048): 0.0039
Throughput (in TFLOP/s) for attention_linear_projection (4x3712x14848, b=2048): 231.031
Elapsed time for mlp_h_to_4h (4x14848x14848, b=2048): 0.0145
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14848x14848, b=2048): 249.003
Elapsed time for mlp_4h_to_h (4x14848x14848, b=2048): 0.0143
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14848x14848, b=2048): 252.170

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 229.944
MLP duration (in seconds): 0.0288
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0456
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14976x11232, b=2048): 0.0113
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14976x11232, b=2048): 243.899
Elapsed time for attention_key_query_prob (64x2048x234x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x234x2048): 95.561
Elapsed time for attention_prob_times_values (64x2048x2048x234): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x234): 126.512
Elapsed time for attention_linear_projection (4x3744x14976, b=2048): 0.0041
Throughput (in TFLOP/s) for attention_linear_projection (4x3744x14976, b=2048): 224.174
Elapsed time for mlp_h_to_4h (4x14976x14976, b=2048): 0.0146
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14976x14976, b=2048): 252.047
Elapsed time for mlp_4h_to_h (4x14976x14976, b=2048): 0.0145
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14976x14976, b=2048): 254.188

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 221.735
MLP duration (in seconds): 0.0290
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0467
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15104x11328, b=2048): 0.0112
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15104x11328, b=2048): 249.253
Elapsed time for attention_key_query_prob (64x2048x236x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x236x2048): 67.654
Elapsed time for attention_prob_times_values (64x2048x2048x236): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x236): 130.315
Elapsed time for attention_linear_projection (4x3776x15104, b=2048): 0.0041
Throughput (in TFLOP/s) for attention_linear_projection (4x3776x15104, b=2048): 230.612
Elapsed time for mlp_h_to_4h (4x15104x15104, b=2048): 0.0150
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15104x15104, b=2048): 249.031
Elapsed time for mlp_4h_to_h (4x15104x15104, b=2048): 0.0149
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15104x15104, b=2048): 251.202

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 219.972
MLP duration (in seconds): 0.0299
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0480
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15232x11424, b=2048): 0.0111
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15232x11424, b=2048): 257.369
Elapsed time for attention_key_query_prob (64x2048x238x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x238x2048): 96.407
Elapsed time for attention_prob_times_values (64x2048x2048x238): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x238): 127.663
Elapsed time for attention_linear_projection (4x3808x15232, b=2048): 0.0042
Throughput (in TFLOP/s) for attention_linear_projection (4x3808x15232, b=2048): 227.796
Elapsed time for mlp_h_to_4h (4x15232x15232, b=2048): 0.0147
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15232x15232, b=2048): 257.925
Elapsed time for mlp_4h_to_h (4x15232x15232, b=2048): 0.0151
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15232x15232, b=2048): 251.711

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 230.825
MLP duration (in seconds): 0.0298
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0474
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15360x11520, b=2048): 0.0117
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15360x11520, b=2048): 248.102
Elapsed time for attention_key_query_prob (64x2048x240x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x240x2048): 122.270
Elapsed time for attention_prob_times_values (64x2048x2048x240): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x240): 196.735
Elapsed time for attention_linear_projection (4x3840x15360, b=2048): 0.0042
Throughput (in TFLOP/s) for attention_linear_projection (4x3840x15360, b=2048): 232.371
Elapsed time for mlp_h_to_4h (4x15360x15360, b=2048): 0.0154
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15360x15360, b=2048): 251.352
Elapsed time for mlp_4h_to_h (4x15360x15360, b=2048): 0.0153
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15360x15360, b=2048): 253.331

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 234.903
MLP duration (in seconds): 0.0306
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0482
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15488x11616, b=2048): 0.0119
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15488x11616, b=2048): 247.859
Elapsed time for attention_key_query_prob (64x2048x242x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x242x2048): 97.379
Elapsed time for attention_prob_times_values (64x2048x2048x242): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x242): 133.759
Elapsed time for attention_linear_projection (4x3872x15488, b=2048): 0.0047
Throughput (in TFLOP/s) for attention_linear_projection (4x3872x15488, b=2048): 208.651
Elapsed time for mlp_h_to_4h (4x15488x15488, b=2048): 0.0156
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15488x15488, b=2048): 252.637
Elapsed time for mlp_4h_to_h (4x15488x15488, b=2048): 0.0156
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15488x15488, b=2048): 252.165

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 221.613
MLP duration (in seconds): 0.0311
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0500
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15616x11712, b=2048): 0.0118
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15616x11712, b=2048): 254.442
Elapsed time for attention_key_query_prob (64x2048x244x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x244x2048): 77.158
Elapsed time for attention_prob_times_values (64x2048x2048x244): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x244): 134.633
Elapsed time for attention_linear_projection (4x3904x15616, b=2048): 0.0044
Throughput (in TFLOP/s) for attention_linear_projection (4x3904x15616, b=2048): 227.244
Elapsed time for mlp_h_to_4h (4x15616x15616, b=2048): 0.0160
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15616x15616, b=2048): 249.181
Elapsed time for mlp_4h_to_h (4x15616x15616, b=2048): 0.0159
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15616x15616, b=2048): 251.507

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 225.938
MLP duration (in seconds): 0.0319
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0508
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15744x11808, b=2048): 0.0123
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15744x11808, b=2048): 247.200
Elapsed time for attention_key_query_prob (64x2048x246x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x246x2048): 99.308
Elapsed time for attention_prob_times_values (64x2048x2048x246): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x246): 137.081
Elapsed time for attention_linear_projection (4x3936x15744, b=2048): 0.0044
Throughput (in TFLOP/s) for attention_linear_projection (4x3936x15744, b=2048): 228.457
Elapsed time for mlp_h_to_4h (4x15744x15744, b=2048): 0.0162
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15744x15744, b=2048): 250.022
Elapsed time for mlp_4h_to_h (4x15744x15744, b=2048): 0.0160
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15744x15744, b=2048): 253.392

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 226.943
MLP duration (in seconds): 0.0323
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0513
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15872x11904, b=2048): 0.0123
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15872x11904, b=2048): 252.634
Elapsed time for attention_key_query_prob (64x2048x248x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x248x2048): 124.320
Elapsed time for attention_prob_times_values (64x2048x2048x248): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x248): 101.462
Elapsed time for attention_linear_projection (4x3968x15872, b=2048): 0.0044
Throughput (in TFLOP/s) for attention_linear_projection (4x3968x15872, b=2048): 232.749
Elapsed time for mlp_h_to_4h (4x15872x15872, b=2048): 0.0168
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15872x15872, b=2048): 245.568
Elapsed time for mlp_4h_to_h (4x15872x15872, b=2048): 0.0166
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15872x15872, b=2048): 249.364

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 230.402
MLP duration (in seconds): 0.0334
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0524
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16000x12000, b=2048): 0.0127
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16000x12000, b=2048): 247.875
Elapsed time for attention_key_query_prob (64x2048x250x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x250x2048): 77.541
Elapsed time for attention_prob_times_values (64x2048x2048x250): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x250): 140.352
Elapsed time for attention_linear_projection (4x4000x16000, b=2048): 0.0046
Throughput (in TFLOP/s) for attention_linear_projection (4x4000x16000, b=2048): 227.772
Elapsed time for mlp_h_to_4h (4x16000x16000, b=2048): 0.0165
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16000x16000, b=2048): 254.660
Elapsed time for mlp_4h_to_h (4x16000x16000, b=2048): 0.0168
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16000x16000, b=2048): 250.348

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 223.342
MLP duration (in seconds): 0.0332
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0532
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16128x12096, b=2048): 0.0126
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16128x12096, b=2048): 254.559
Elapsed time for attention_key_query_prob (64x2048x252x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x252x2048): 75.782
Elapsed time for attention_prob_times_values (64x2048x2048x252): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x252): 141.545
Elapsed time for attention_linear_projection (4x4032x16128, b=2048): 0.0046
Throughput (in TFLOP/s) for attention_linear_projection (4x4032x16128, b=2048): 234.098
Elapsed time for mlp_h_to_4h (4x16128x16128, b=2048): 0.0167
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16128x16128, b=2048): 255.694
Elapsed time for mlp_4h_to_h (4x16128x16128, b=2048): 0.0167
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16128x16128, b=2048): 255.840

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 228.345
MLP duration (in seconds): 0.0333
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0532
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16256x12192, b=2048): 0.0127
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16256x12192, b=2048): 255.895
Elapsed time for attention_key_query_prob (64x2048x254x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x254x2048): 101.519
Elapsed time for attention_prob_times_values (64x2048x2048x254): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x254): 142.775
Elapsed time for attention_linear_projection (4x4064x16256, b=2048): 0.0047
Throughput (in TFLOP/s) for attention_linear_projection (4x4064x16256, b=2048): 231.734
Elapsed time for mlp_h_to_4h (4x16256x16256, b=2048): 0.0172
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16256x16256, b=2048): 251.390
Elapsed time for mlp_4h_to_h (4x16256x16256, b=2048): 0.0172
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16256x16256, b=2048): 251.101

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 234.110
MLP duration (in seconds): 0.0345
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0541
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16384x12288, b=2048): 0.0129
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16384x12288, b=2048): 254.982
Elapsed time for attention_key_query_prob (64x2048x256x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x256x2048): 128.216
Elapsed time for attention_prob_times_values (64x2048x2048x256): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x256): 157.460
Elapsed time for attention_linear_projection (4x4096x16384, b=2048): 0.0047
Throughput (in TFLOP/s) for attention_linear_projection (4x4096x16384, b=2048): 235.892
Elapsed time for mlp_h_to_4h (4x16384x16384, b=2048): 0.0174
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16384x16384, b=2048): 252.879
Elapsed time for mlp_4h_to_h (4x16384x16384, b=2048): 0.0175
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16384x16384, b=2048): 251.998

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 239.119
MLP duration (in seconds): 0.0348
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0544
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16512x12384, b=2048): 0.0135
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16512x12384, b=2048): 247.553
Elapsed time for attention_key_query_prob (64x2048x258x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x258x2048): 99.124
Elapsed time for attention_prob_times_values (64x2048x2048x258): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x258): 94.007
Elapsed time for attention_linear_projection (4x4128x16512, b=2048): 0.0046
Throughput (in TFLOP/s) for attention_linear_projection (4x4128x16512, b=2048): 244.290
Elapsed time for mlp_h_to_4h (4x16512x16512, b=2048): 0.0177
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16512x16512, b=2048): 252.307
Elapsed time for mlp_4h_to_h (4x16512x16512, b=2048): 0.0177
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16512x16512, b=2048): 252.192

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 226.168
MLP duration (in seconds): 0.0354
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0564
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16640x12480, b=2048): 0.0134
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16640x12480, b=2048): 253.965
Elapsed time for attention_key_query_prob (64x2048x260x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x260x2048): 79.547
Elapsed time for attention_prob_times_values (64x2048x2048x260): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x260): 94.233
Elapsed time for attention_linear_projection (4x4160x16640, b=2048): 0.0046
Throughput (in TFLOP/s) for attention_linear_projection (4x4160x16640, b=2048): 249.067
Elapsed time for mlp_h_to_4h (4x16640x16640, b=2048): 0.0181
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16640x16640, b=2048): 250.704
Elapsed time for mlp_4h_to_h (4x16640x16640, b=2048): 0.0181
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16640x16640, b=2048): 250.622

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 227.298
MLP duration (in seconds): 0.0362
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0574
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16768x12576, b=2048): 0.0135
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16768x12576, b=2048): 255.342
Elapsed time for attention_key_query_prob (64x2048x262x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x262x2048): 99.674
Elapsed time for attention_prob_times_values (64x2048x2048x262): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x262): 85.355
Elapsed time for attention_linear_projection (4x4192x16768, b=2048): 0.0048
Throughput (in TFLOP/s) for attention_linear_projection (4x4192x16768, b=2048): 242.161
Elapsed time for mlp_h_to_4h (4x16768x16768, b=2048): 0.0185
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16768x16768, b=2048): 249.549
Elapsed time for mlp_4h_to_h (4x16768x16768, b=2048): 0.0183
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16768x16768, b=2048): 252.381

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 228.990
MLP duration (in seconds): 0.0367
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0581
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16896x12672, b=2048): 0.0138
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16896x12672, b=2048): 254.519
Elapsed time for attention_key_query_prob (64x2048x264x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x264x2048): 123.823
Elapsed time for attention_prob_times_values (64x2048x2048x264): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x264): 131.521
Elapsed time for attention_linear_projection (4x4224x16896, b=2048): 0.0047
Throughput (in TFLOP/s) for attention_linear_projection (4x4224x16896, b=2048): 246.478
Elapsed time for mlp_h_to_4h (4x16896x16896, b=2048): 0.0186
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16896x16896, b=2048): 251.990
Elapsed time for mlp_4h_to_h (4x16896x16896, b=2048): 0.0188
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16896x16896, b=2048): 248.460

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 239.082
MLP duration (in seconds): 0.0374
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0581
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17024x12768, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17024x12768, b=2048): 247.302
Elapsed time for attention_key_query_prob (64x2048x266x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x266x2048): 100.097
Elapsed time for attention_prob_times_values (64x2048x2048x266): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x266): 92.449
Elapsed time for attention_linear_projection (4x4256x17024, b=2048): 0.0048
Throughput (in TFLOP/s) for attention_linear_projection (4x4256x17024, b=2048): 245.853
Elapsed time for mlp_h_to_4h (4x17024x17024, b=2048): 0.0190
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17024x17024, b=2048): 250.035
Elapsed time for mlp_4h_to_h (4x17024x17024, b=2048): 0.0189
Throughput (in TFLOP/s) for mlp_4h_to_h (4x17024x17024, b=2048): 250.756

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 226.752
MLP duration (in seconds): 0.0379
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0601
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17152x12864, b=2048): 0.0146
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17152x12864, b=2048): 247.960
Elapsed time for attention_key_query_prob (64x2048x268x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x268x2048): 100.614
Elapsed time for attention_prob_times_values (64x2048x2048x268): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x268): 69.175
Elapsed time for attention_linear_projection (4x4288x17152, b=2048): 0.0048
Throughput (in TFLOP/s) for attention_linear_projection (4x4288x17152, b=2048): 252.456
Elapsed time for mlp_h_to_4h (4x17152x17152, b=2048): 0.0193
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17152x17152, b=2048): 249.109
Elapsed time for mlp_4h_to_h (4x17152x17152, b=2048): 0.0193
Throughput (in TFLOP/s) for mlp_4h_to_h (4x17152x17152, b=2048): 249.142

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 223.416
MLP duration (in seconds): 0.0387
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0616
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17280x12960, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17280x12960, b=2048): 255.188
Elapsed time for attention_key_query_prob (64x2048x270x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x270x2048): 101.585
Elapsed time for attention_prob_times_values (64x2048x2048x270): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x270): 94.790
Elapsed time for attention_linear_projection (4x4320x17280, b=2048): 0.0050
Throughput (in TFLOP/s) for attention_linear_projection (4x4320x17280, b=2048): 245.625
Elapsed time for mlp_h_to_4h (4x17280x17280, b=2048): 0.0201
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17280x17280, b=2048): 243.500
Elapsed time for mlp_4h_to_h (4x17280x17280, b=2048): 0.0201
Throughput (in TFLOP/s) for mlp_4h_to_h (4x17280x17280, b=2048): 243.633

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 232.239
MLP duration (in seconds): 0.0402
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0625
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17408x13056, b=2048): 0.0149
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17408x13056, b=2048): 249.393
Elapsed time for attention_key_query_prob (64x2048x272x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x272x2048): 106.761
Elapsed time for attention_prob_times_values (64x2048x2048x272): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x272): 136.018
Elapsed time for attention_linear_projection (4x4352x17408, b=2048): 0.0050
Throughput (in TFLOP/s) for attention_linear_projection (4x4352x17408, b=2048): 249.756
Elapsed time for mlp_h_to_4h (4x17408x17408, b=2048): 0.0196
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17408x17408, b=2048): 252.723
Elapsed time for mlp_4h_to_h (4x17408x17408, b=2048): 0.0195
Throughput (in TFLOP/s) for mlp_4h_to_h (4x17408x17408, b=2048): 254.397

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 235.294
MLP duration (in seconds): 0.0392
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0615
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17536x13152, b=2048): 0.0150
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17536x13152, b=2048): 252.546
Elapsed time for attention_key_query_prob (64x2048x274x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x274x2048): 102.644
Elapsed time for attention_prob_times_values (64x2048x2048x274): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x274): 96.105
Elapsed time for attention_linear_projection (4x4384x17536, b=2048): 0.0054
Throughput (in TFLOP/s) for attention_linear_projection (4x4384x17536, b=2048): 234.446
Elapsed time for mlp_h_to_4h (4x17536x17536, b=2048): 0.0202
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17536x17536, b=2048): 249.844
Elapsed time for mlp_4h_to_h (4x17536x17536, b=2048): 0.0200
Throughput (in TFLOP/s) for mlp_4h_to_h (4x17536x17536, b=2048): 251.290

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 228.874
MLP duration (in seconds): 0.0402
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0635
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17664x13248, b=2048): 0.0155
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17664x13248, b=2048): 247.850
Elapsed time for attention_key_query_prob (64x2048x276x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x276x2048): 103.050
Elapsed time for attention_prob_times_values (64x2048x2048x276): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x276): 95.866
Elapsed time for attention_linear_projection (4x4416x17664, b=2048): 0.0051
Throughput (in TFLOP/s) for attention_linear_projection (4x4416x17664, b=2048): 251.108
Elapsed time for mlp_h_to_4h (4x17664x17664, b=2048): 0.0203
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17664x17664, b=2048): 252.171
Elapsed time for mlp_4h_to_h (4x17664x17664, b=2048): 0.0203
Throughput (in TFLOP/s) for mlp_4h_to_h (4x17664x17664, b=2048): 252.329

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 229.732
MLP duration (in seconds): 0.0405
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0641
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17792x13344, b=2048): 0.0156
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17792x13344, b=2048): 248.892
Elapsed time for attention_key_query_prob (64x2048x278x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x278x2048): 104.194
Elapsed time for attention_prob_times_values (64x2048x2048x278): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x278): 96.709
Elapsed time for attention_linear_projection (4x4448x17792, b=2048): 0.0052
Throughput (in TFLOP/s) for attention_linear_projection (4x4448x17792, b=2048): 249.158
Elapsed time for mlp_h_to_4h (4x17792x17792, b=2048): 0.0205
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17792x17792, b=2048): 252.745
Elapsed time for mlp_4h_to_h (4x17792x17792, b=2048): 0.0208
Throughput (in TFLOP/s) for mlp_4h_to_h (4x17792x17792, b=2048): 248.808

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 230.380
MLP duration (in seconds): 0.0414
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0652
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17920x13440, b=2048): 0.0156
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17920x13440, b=2048): 252.155
Elapsed time for attention_key_query_prob (64x2048x280x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x280x2048): 130.729
Elapsed time for attention_prob_times_values (64x2048x2048x280): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x280): 138.208
Elapsed time for attention_linear_projection (4x4480x17920, b=2048): 0.0052
Throughput (in TFLOP/s) for attention_linear_projection (4x4480x17920, b=2048): 251.856
Elapsed time for mlp_h_to_4h (4x17920x17920, b=2048): 0.0210
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17920x17920, b=2048): 251.022
Elapsed time for mlp_4h_to_h (4x17920x17920, b=2048): 0.0522
Throughput (in TFLOP/s) for mlp_4h_to_h (4x17920x17920, b=2048): 100.854

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 240.683
MLP duration (in seconds): 0.0731
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0962
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18048x13536, b=2048): 0.0157
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18048x13536, b=2048): 255.032
Elapsed time for attention_key_query_prob (64x2048x282x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x282x2048): 105.151
Elapsed time for attention_prob_times_values (64x2048x2048x282): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x282): 97.379
Elapsed time for attention_linear_projection (4x4512x18048, b=2048): 0.0054
Throughput (in TFLOP/s) for attention_linear_projection (4x4512x18048, b=2048): 245.859
Elapsed time for mlp_h_to_4h (4x18048x18048, b=2048): 0.0208
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18048x18048, b=2048): 256.122
Elapsed time for mlp_4h_to_h (4x18048x18048, b=2048): 0.0209
Throughput (in TFLOP/s) for mlp_4h_to_h (4x18048x18048, b=2048): 255.686

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 233.856
MLP duration (in seconds): 0.0417
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0658
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18176x13632, b=2048): 0.0260
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18176x13632, b=2048): 156.056
Elapsed time for attention_key_query_prob (64x2048x284x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x284x2048): 105.967
Elapsed time for attention_prob_times_values (64x2048x2048x284): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x284): 98.100
Elapsed time for attention_linear_projection (4x4544x18176, b=2048): 0.0054
Throughput (in TFLOP/s) for attention_linear_projection (4x4544x18176, b=2048): 248.725
Elapsed time for mlp_h_to_4h (4x18176x18176, b=2048): 0.0213
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18176x18176, b=2048): 254.534
Elapsed time for mlp_4h_to_h (4x18176x18176, b=2048): 0.0214
Throughput (in TFLOP/s) for mlp_4h_to_h (4x18176x18176, b=2048): 252.684

Attention duration (in seconds): 0.0344
Attention throughput (in TFLOP/s): 165.985
MLP duration (in seconds): 0.0427
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0771
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18304x13728, b=2048): 0.0166
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18304x13728, b=2048): 248.334
Elapsed time for attention_key_query_prob (64x2048x286x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x286x2048): 106.908
Elapsed time for attention_prob_times_values (64x2048x2048x286): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x286): 98.639
Elapsed time for attention_linear_projection (4x4576x18304, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_linear_projection (4x4576x18304, b=2048): 249.865
Elapsed time for mlp_h_to_4h (4x18304x18304, b=2048): 0.0216
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18304x18304, b=2048): 253.924
Elapsed time for mlp_4h_to_h (4x18304x18304, b=2048): 0.0216
Throughput (in TFLOP/s) for mlp_4h_to_h (4x18304x18304, b=2048): 253.899

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 231.268
MLP duration (in seconds): 0.0432
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0683
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18432x13824, b=2048): 0.0165
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18432x13824, b=2048): 252.958
Elapsed time for attention_key_query_prob (64x2048x288x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x288x2048): 135.108
Elapsed time for attention_prob_times_values (64x2048x2048x288): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x288): 137.777
Elapsed time for attention_linear_projection (4x4608x18432, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_linear_projection (4x4608x18432, b=2048): 254.066
Elapsed time for mlp_h_to_4h (4x18432x18432, b=2048): 0.0218
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18432x18432, b=2048): 255.641
Elapsed time for mlp_4h_to_h (4x18432x18432, b=2048): 0.0220
Throughput (in TFLOP/s) for mlp_4h_to_h (4x18432x18432, b=2048): 253.443

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 242.315
MLP duration (in seconds): 0.0437
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0680
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18560x13920, b=2048): 0.0166
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18560x13920, b=2048): 254.469
Elapsed time for attention_key_query_prob (64x2048x290x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x290x2048): 80.780
Elapsed time for attention_prob_times_values (64x2048x2048x290): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x290): 99.713
Elapsed time for attention_linear_projection (4x4640x18560, b=2048): 0.0057
Throughput (in TFLOP/s) for attention_linear_projection (4x4640x18560, b=2048): 246.676
Elapsed time for mlp_h_to_4h (4x18560x18560, b=2048): 0.0220
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18560x18560, b=2048): 256.321
Elapsed time for mlp_4h_to_h (4x18560x18560, b=2048): 0.0223
Throughput (in TFLOP/s) for mlp_4h_to_h (4x18560x18560, b=2048): 253.009

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 230.440
MLP duration (in seconds): 0.0443
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0702
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18688x14016, b=2048): 0.0169
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18688x14016, b=2048): 253.200
Elapsed time for attention_key_query_prob (64x2048x292x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x292x2048): 103.060
Elapsed time for attention_prob_times_values (64x2048x2048x292): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x292): 81.579
Elapsed time for attention_linear_projection (4x4672x18688, b=2048): 0.0057
Throughput (in TFLOP/s) for attention_linear_projection (4x4672x18688, b=2048): 251.918
Elapsed time for mlp_h_to_4h (4x18688x18688, b=2048): 0.0226
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18688x18688, b=2048): 253.474
Elapsed time for mlp_4h_to_h (4x18688x18688, b=2048): 0.0226
Throughput (in TFLOP/s) for mlp_4h_to_h (4x18688x18688, b=2048): 253.533

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 231.510
MLP duration (in seconds): 0.0451
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0712
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18816x14112, b=2048): 0.0174
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18816x14112, b=2048): 250.476
Elapsed time for attention_key_query_prob (64x2048x294x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x294x2048): 103.007
Elapsed time for attention_prob_times_values (64x2048x2048x294): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x294): 98.634
Elapsed time for attention_linear_projection (4x4704x18816, b=2048): 0.0058
Throughput (in TFLOP/s) for attention_linear_projection (4x4704x18816, b=2048): 251.755
Elapsed time for mlp_h_to_4h (4x18816x18816, b=2048): 0.0227
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18816x18816, b=2048): 255.104
Elapsed time for mlp_4h_to_h (4x18816x18816, b=2048): 0.0228
Throughput (in TFLOP/s) for mlp_4h_to_h (4x18816x18816, b=2048): 254.971

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 232.899
MLP duration (in seconds): 0.0455
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0717
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18944x14208, b=2048): 0.0175
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18944x14208, b=2048): 252.151
Elapsed time for attention_key_query_prob (64x2048x296x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x296x2048): 131.207
Elapsed time for attention_prob_times_values (64x2048x2048x296): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x296): 143.217
Elapsed time for attention_linear_projection (4x4736x18944, b=2048): 0.0058
Throughput (in TFLOP/s) for attention_linear_projection (4x4736x18944, b=2048): 253.586
Elapsed time for mlp_h_to_4h (4x18944x18944, b=2048): 0.0232
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18944x18944, b=2048): 253.182
Elapsed time for mlp_4h_to_h (4x18944x18944, b=2048): 0.0233
Throughput (in TFLOP/s) for mlp_4h_to_h (4x18944x18944, b=2048): 252.741

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 242.035
MLP duration (in seconds): 0.0465
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0721
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19072x14304, b=2048): 0.0178
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19072x14304, b=2048): 251.022
Elapsed time for attention_key_query_prob (64x2048x298x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x298x2048): 104.817
Elapsed time for attention_prob_times_values (64x2048x2048x298): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x298): 101.749
Elapsed time for attention_linear_projection (4x4768x19072, b=2048): 0.0062
Throughput (in TFLOP/s) for attention_linear_projection (4x4768x19072, b=2048): 242.126
Elapsed time for mlp_h_to_4h (4x19072x19072, b=2048): 0.0235
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19072x19072, b=2048): 253.456
Elapsed time for mlp_4h_to_h (4x19072x19072, b=2048): 0.0238
Throughput (in TFLOP/s) for mlp_4h_to_h (4x19072x19072, b=2048): 250.231

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 232.077
MLP duration (in seconds): 0.0473
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0744
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19200x14400, b=2048): 0.0181
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19200x14400, b=2048): 249.761
Elapsed time for attention_key_query_prob (64x2048x300x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x300x2048): 104.379
Elapsed time for attention_prob_times_values (64x2048x2048x300): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x300): 80.078
Elapsed time for attention_linear_projection (4x4800x19200, b=2048): 0.0062
Throughput (in TFLOP/s) for attention_linear_projection (4x4800x19200, b=2048): 242.958
Elapsed time for mlp_h_to_4h (4x19200x19200, b=2048): 0.0240
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19200x19200, b=2048): 251.339
Elapsed time for mlp_4h_to_h (4x19200x19200, b=2048): 0.0237
Throughput (in TFLOP/s) for mlp_4h_to_h (4x19200x19200, b=2048): 254.659

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 227.977
MLP duration (in seconds): 0.0477
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0757
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19328x14496, b=2048): 0.0189
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19328x14496, b=2048): 243.170
Elapsed time for attention_key_query_prob (64x2048x302x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x302x2048): 103.053
Elapsed time for attention_prob_times_values (64x2048x2048x302): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x302): 100.109
Elapsed time for attention_linear_projection (4x4832x19328, b=2048): 0.0062
Throughput (in TFLOP/s) for attention_linear_projection (4x4832x19328, b=2048): 245.878
Elapsed time for mlp_h_to_4h (4x19328x19328, b=2048): 0.0241
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19328x19328, b=2048): 253.510
Elapsed time for mlp_4h_to_h (4x19328x19328, b=2048): 0.0242
Throughput (in TFLOP/s) for mlp_4h_to_h (4x19328x19328, b=2048): 252.845

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 227.785
MLP duration (in seconds): 0.0484
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0766
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19456x14592, b=2048): 0.0189
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19456x14592, b=2048): 246.318
Elapsed time for attention_key_query_prob (64x2048x304x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x304x2048): 98.510
Elapsed time for attention_prob_times_values (64x2048x2048x304): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x304): 149.824
Elapsed time for attention_linear_projection (4x4864x19456, b=2048): 0.0062
Throughput (in TFLOP/s) for attention_linear_projection (4x4864x19456, b=2048): 249.854
Elapsed time for mlp_h_to_4h (4x19456x19456, b=2048): 0.0245
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19456x19456, b=2048): 253.021
Elapsed time for mlp_4h_to_h (4x19456x19456, b=2048): 0.0248
Throughput (in TFLOP/s) for mlp_4h_to_h (4x19456x19456, b=2048): 250.487

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 234.532
MLP duration (in seconds): 0.0493
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0771
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19584x14688, b=2048): 0.0188
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19584x14688, b=2048): 250.741
Elapsed time for attention_key_query_prob (64x2048x306x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x306x2048): 105.311
Elapsed time for attention_prob_times_values (64x2048x2048x306): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x306): 101.615
Elapsed time for attention_linear_projection (4x4896x19584, b=2048): 0.0067
Throughput (in TFLOP/s) for attention_linear_projection (4x4896x19584, b=2048): 233.696
Elapsed time for mlp_h_to_4h (4x19584x19584, b=2048): 0.0256
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19584x19584, b=2048): 245.420
Elapsed time for mlp_4h_to_h (4x19584x19584, b=2048): 0.0249
Throughput (in TFLOP/s) for mlp_4h_to_h (4x19584x19584, b=2048): 252.611

Attention duration (in seconds): 0.0287
Attention throughput (in TFLOP/s): 230.439
MLP duration (in seconds): 0.0505
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0792
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19712x14784, b=2048): 0.0194
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19712x14784, b=2048): 245.885
Elapsed time for attention_key_query_prob (64x2048x308x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x308x2048): 104.941
Elapsed time for attention_prob_times_values (64x2048x2048x308): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x308): 103.008
Elapsed time for attention_linear_projection (4x4928x19712, b=2048): 0.0064
Throughput (in TFLOP/s) for attention_linear_projection (4x4928x19712, b=2048): 250.515
Elapsed time for mlp_h_to_4h (4x19712x19712, b=2048): 0.0252
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19712x19712, b=2048): 253.067
Elapsed time for mlp_4h_to_h (4x19712x19712, b=2048): 0.0252
Throughput (in TFLOP/s) for mlp_4h_to_h (4x19712x19712, b=2048): 252.450

Attention duration (in seconds): 0.0290
Attention throughput (in TFLOP/s): 231.309
MLP duration (in seconds): 0.0504
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0793
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19840x14880, b=2048): 0.0196
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19840x14880, b=2048): 247.370
Elapsed time for attention_key_query_prob (64x2048x310x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x310x2048): 104.531
Elapsed time for attention_prob_times_values (64x2048x2048x310): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x310): 104.048
Elapsed time for attention_linear_projection (4x4960x19840, b=2048): 0.0066
Throughput (in TFLOP/s) for attention_linear_projection (4x4960x19840, b=2048): 244.874
Elapsed time for mlp_h_to_4h (4x19840x19840, b=2048): 0.0255
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19840x19840, b=2048): 253.059
Elapsed time for mlp_4h_to_h (4x19840x19840, b=2048): 0.0253
Throughput (in TFLOP/s) for mlp_4h_to_h (4x19840x19840, b=2048): 254.583

Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 231.239
MLP duration (in seconds): 0.0508
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0801
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19968x14976, b=2048): 0.0199
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19968x14976, b=2048): 245.839
Elapsed time for attention_key_query_prob (64x2048x312x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x312x2048): 134.771
Elapsed time for attention_prob_times_values (64x2048x2048x312): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x312): 151.447
Elapsed time for attention_linear_projection (4x4992x19968, b=2048): 0.0066
Throughput (in TFLOP/s) for attention_linear_projection (4x4992x19968, b=2048): 247.810
Elapsed time for mlp_h_to_4h (4x19968x19968, b=2048): 0.0259
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19968x19968, b=2048): 251.935
Elapsed time for mlp_4h_to_h (4x19968x19968, b=2048): 0.0257
Throughput (in TFLOP/s) for mlp_4h_to_h (4x19968x19968, b=2048): 253.964

Attention duration (in seconds): 0.0289
Attention throughput (in TFLOP/s): 237.891
MLP duration (in seconds): 0.0517
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0805
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20096x15072, b=2048): 0.0197
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20096x15072, b=2048): 252.428
Elapsed time for attention_key_query_prob (64x2048x314x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x314x2048): 108.230
Elapsed time for attention_prob_times_values (64x2048x2048x314): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x314): 105.281
Elapsed time for attention_linear_projection (4x5024x20096, b=2048): 0.0069
Throughput (in TFLOP/s) for attention_linear_projection (4x5024x20096, b=2048): 241.450
Elapsed time for mlp_h_to_4h (4x20096x20096, b=2048): 0.0262
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20096x20096, b=2048): 252.106
Elapsed time for mlp_4h_to_h (4x20096x20096, b=2048): 0.0263
Throughput (in TFLOP/s) for mlp_4h_to_h (4x20096x20096, b=2048): 252.037

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 234.382
MLP duration (in seconds): 0.0525
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0822
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20224x15168, b=2048): 0.0200
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20224x15168, b=2048): 251.173
Elapsed time for attention_key_query_prob (64x2048x316x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x316x2048): 108.421
Elapsed time for attention_prob_times_values (64x2048x2048x316): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x316): 105.122
Elapsed time for attention_linear_projection (4x5056x20224, b=2048): 0.0067
Throughput (in TFLOP/s) for attention_linear_projection (4x5056x20224, b=2048): 250.615
Elapsed time for mlp_h_to_4h (4x20224x20224, b=2048): 0.0261
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20224x20224, b=2048): 256.649
Elapsed time for mlp_4h_to_h (4x20224x20224, b=2048): 0.0261
Throughput (in TFLOP/s) for mlp_4h_to_h (4x20224x20224, b=2048): 256.692

Attention duration (in seconds): 0.0299
Attention throughput (in TFLOP/s): 235.681
MLP duration (in seconds): 0.0522
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0821
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20352x15264, b=2048): 0.0204
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20352x15264, b=2048): 249.815
Elapsed time for attention_key_query_prob (64x2048x318x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x318x2048): 110.796
Elapsed time for attention_prob_times_values (64x2048x2048x318): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x318): 105.912
Elapsed time for attention_linear_projection (4x5088x20352, b=2048): 0.0072
Throughput (in TFLOP/s) for attention_linear_projection (4x5088x20352, b=2048): 234.317
Elapsed time for mlp_h_to_4h (4x20352x20352, b=2048): 0.0267
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20352x20352, b=2048): 253.750
Elapsed time for mlp_4h_to_h (4x20352x20352, b=2048): 0.0270
Throughput (in TFLOP/s) for mlp_4h_to_h (4x20352x20352, b=2048): 250.940

Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 231.666
MLP duration (in seconds): 0.0538
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0846
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20480x15360, b=2048): 0.0202
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20480x15360, b=2048): 254.936
Elapsed time for attention_key_query_prob (64x2048x320x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x320x2048): 141.041
Elapsed time for attention_prob_times_values (64x2048x2048x320): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x320): 104.872
Elapsed time for attention_linear_projection (4x5120x20480, b=2048): 0.0069
Throughput (in TFLOP/s) for attention_linear_projection (4x5120x20480, b=2048): 248.603
Elapsed time for mlp_h_to_4h (4x20480x20480, b=2048): 0.0273
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20480x20480, b=2048): 251.697
Elapsed time for mlp_4h_to_h (4x20480x20480, b=2048): 0.0272
Throughput (in TFLOP/s) for mlp_4h_to_h (4x20480x20480, b=2048): 252.369

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 240.650
MLP duration (in seconds): 0.0545
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0845
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20608x15456, b=2048): 0.0207
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20608x15456, b=2048): 252.682
Elapsed time for attention_key_query_prob (64x2048x322x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x322x2048): 106.708
Elapsed time for attention_prob_times_values (64x2048x2048x322): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x322): 107.819
Elapsed time for attention_linear_projection (4x5152x20608, b=2048): 0.0073
Throughput (in TFLOP/s) for attention_linear_projection (4x5152x20608, b=2048): 239.523
Elapsed time for mlp_h_to_4h (4x20608x20608, b=2048): 0.0275
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20608x20608, b=2048): 253.107
Elapsed time for mlp_4h_to_h (4x20608x20608, b=2048): 0.0277
Throughput (in TFLOP/s) for mlp_4h_to_h (4x20608x20608, b=2048): 250.816

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 234.559
MLP duration (in seconds): 0.0552
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0864
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20736x15552, b=2048): 0.0210
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20736x15552, b=2048): 251.559
Elapsed time for attention_key_query_prob (64x2048x324x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x324x2048): 106.400
Elapsed time for attention_prob_times_values (64x2048x2048x324): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x324): 107.434
Elapsed time for attention_linear_projection (4x5184x20736, b=2048): 0.0072
Throughput (in TFLOP/s) for attention_linear_projection (4x5184x20736, b=2048): 245.147
Elapsed time for mlp_h_to_4h (4x20736x20736, b=2048): 0.0278
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20736x20736, b=2048): 253.708
Elapsed time for mlp_4h_to_h (4x20736x20736, b=2048): 0.0278
Throughput (in TFLOP/s) for mlp_4h_to_h (4x20736x20736, b=2048): 253.551

Attention duration (in seconds): 0.0314
Attention throughput (in TFLOP/s): 235.125
MLP duration (in seconds): 0.0556
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0870
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20864x15648, b=2048): 0.0212
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20864x15648, b=2048): 252.686
Elapsed time for attention_key_query_prob (64x2048x326x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x326x2048): 106.776
Elapsed time for attention_prob_times_values (64x2048x2048x326): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x326): 107.922
Elapsed time for attention_linear_projection (4x5216x20864, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_linear_projection (4x5216x20864, b=2048): 241.242
Elapsed time for mlp_h_to_4h (4x20864x20864, b=2048): 0.0287
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20864x20864, b=2048): 248.873
Elapsed time for mlp_4h_to_h (4x20864x20864, b=2048): 0.0280
Throughput (in TFLOP/s) for mlp_4h_to_h (4x20864x20864, b=2048): 254.520

Attention duration (in seconds): 0.0318
Attention throughput (in TFLOP/s): 235.134
MLP duration (in seconds): 0.0567
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0885
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20992x15744, b=2048): 0.0215
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20992x15744, b=2048): 252.189
Elapsed time for attention_key_query_prob (64x2048x328x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x328x2048): 135.050
Elapsed time for attention_prob_times_values (64x2048x2048x328): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x328): 159.419
Elapsed time for attention_linear_projection (4x5248x20992, b=2048): 0.0072
Throughput (in TFLOP/s) for attention_linear_projection (4x5248x20992, b=2048): 250.481
Elapsed time for mlp_h_to_4h (4x20992x20992, b=2048): 0.0282
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20992x20992, b=2048): 256.002
Elapsed time for mlp_4h_to_h (4x20992x20992, b=2048): 0.0285
Throughput (in TFLOP/s) for mlp_4h_to_h (4x20992x20992, b=2048): 253.771

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 243.583
MLP duration (in seconds): 0.0567
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0877
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21120x15840, b=2048): 0.0216
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21120x15840, b=2048): 253.286
Elapsed time for attention_key_query_prob (64x2048x330x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x330x2048): 108.354
Elapsed time for attention_prob_times_values (64x2048x2048x330): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x330): 108.102
Elapsed time for attention_linear_projection (4x5280x21120, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_linear_projection (4x5280x21120, b=2048): 245.929
Elapsed time for mlp_h_to_4h (4x21120x21120, b=2048): 0.0289
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21120x21120, b=2048): 252.922
Elapsed time for mlp_4h_to_h (4x21120x21120, b=2048): 0.0288
Throughput (in TFLOP/s) for mlp_4h_to_h (4x21120x21120, b=2048): 253.667

Attention duration (in seconds): 0.0323
Attention throughput (in TFLOP/s): 236.912
MLP duration (in seconds): 0.0577
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0900
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21248x15936, b=2048): 0.0221
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21248x15936, b=2048): 251.285
Elapsed time for attention_key_query_prob (64x2048x332x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x332x2048): 109.634
Elapsed time for attention_prob_times_values (64x2048x2048x332): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x332): 108.900
Elapsed time for attention_linear_projection (4x5312x21248, b=2048): 0.0073
Throughput (in TFLOP/s) for attention_linear_projection (4x5312x21248, b=2048): 251.927
Elapsed time for mlp_h_to_4h (4x21248x21248, b=2048): 0.0292
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21248x21248, b=2048): 253.152
Elapsed time for mlp_4h_to_h (4x21248x21248, b=2048): 0.0294
Throughput (in TFLOP/s) for mlp_4h_to_h (4x21248x21248, b=2048): 251.933

Attention duration (in seconds): 0.0327
Attention throughput (in TFLOP/s): 237.251
MLP duration (in seconds): 0.0586
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0913
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21376x16032, b=2048): 0.0223
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21376x16032, b=2048): 252.084
Elapsed time for attention_key_query_prob (64x2048x334x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x334x2048): 108.732
Elapsed time for attention_prob_times_values (64x2048x2048x334): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x334): 105.351
Elapsed time for attention_linear_projection (4x5344x21376, b=2048): 0.0079
Throughput (in TFLOP/s) for attention_linear_projection (4x5344x21376, b=2048): 237.155
Elapsed time for mlp_h_to_4h (4x21376x21376, b=2048): 0.0296
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21376x21376, b=2048): 252.495
Elapsed time for mlp_4h_to_h (4x21376x21376, b=2048): 0.0298
Throughput (in TFLOP/s) for mlp_4h_to_h (4x21376x21376, b=2048): 251.009

Attention duration (in seconds): 0.0335
Attention throughput (in TFLOP/s): 234.064
MLP duration (in seconds): 0.0595
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0930
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21504x16128, b=2048): 0.0226
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21504x16128, b=2048): 251.170
Elapsed time for attention_key_query_prob (64x2048x336x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x336x2048): 138.980
Elapsed time for attention_prob_times_values (64x2048x2048x336): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x336): 164.874
Elapsed time for attention_linear_projection (4x5376x21504, b=2048): 0.0075
Throughput (in TFLOP/s) for attention_linear_projection (4x5376x21504, b=2048): 251.260
Elapsed time for mlp_h_to_4h (4x21504x21504, b=2048): 0.0298
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21504x21504, b=2048): 253.815
Elapsed time for mlp_4h_to_h (4x21504x21504, b=2048): 0.0295
Throughput (in TFLOP/s) for mlp_4h_to_h (4x21504x21504, b=2048): 257.057

Attention duration (in seconds): 0.0326
Attention throughput (in TFLOP/s): 243.818
MLP duration (in seconds): 0.0593
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0919
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21632x16224, b=2048): 0.0229
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21632x16224, b=2048): 251.621
Elapsed time for attention_key_query_prob (64x2048x338x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x338x2048): 109.670
Elapsed time for attention_prob_times_values (64x2048x2048x338): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x338): 111.062
Elapsed time for attention_linear_projection (4x5408x21632, b=2048): 0.0079
Throughput (in TFLOP/s) for attention_linear_projection (4x5408x21632, b=2048): 241.360
Elapsed time for mlp_h_to_4h (4x21632x21632, b=2048): 0.0304
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21632x21632, b=2048): 251.873
Elapsed time for mlp_4h_to_h (4x21632x21632, b=2048): 0.0303
Throughput (in TFLOP/s) for mlp_4h_to_h (4x21632x21632, b=2048): 252.803

Attention duration (in seconds): 0.0341
Attention throughput (in TFLOP/s): 235.600
MLP duration (in seconds): 0.0608
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0948
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21760x16320, b=2048): 0.0230
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21760x16320, b=2048): 252.615
Elapsed time for attention_key_query_prob (64x2048x340x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x340x2048): 111.768
Elapsed time for attention_prob_times_values (64x2048x2048x340): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x340): 111.475
Elapsed time for attention_linear_projection (4x5440x21760, b=2048): 0.0079
Throughput (in TFLOP/s) for attention_linear_projection (4x5440x21760, b=2048): 246.781
Elapsed time for mlp_h_to_4h (4x21760x21760, b=2048): 0.0304
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21760x21760, b=2048): 255.186
Elapsed time for mlp_4h_to_h (4x21760x21760, b=2048): 0.0304
Throughput (in TFLOP/s) for mlp_4h_to_h (4x21760x21760, b=2048): 255.142

Attention duration (in seconds): 0.0342
Attention throughput (in TFLOP/s): 237.774
MLP duration (in seconds): 0.0608
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0950
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21888x16416, b=2048): 0.0234
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21888x16416, b=2048): 251.701
Elapsed time for attention_key_query_prob (64x2048x342x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x342x2048): 112.887
Elapsed time for attention_prob_times_values (64x2048x2048x342): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x342): 111.514
Elapsed time for attention_linear_projection (4x5472x21888, b=2048): 0.0081
Throughput (in TFLOP/s) for attention_linear_projection (4x5472x21888, b=2048): 242.540
Elapsed time for mlp_h_to_4h (4x21888x21888, b=2048): 0.0309
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21888x21888, b=2048): 254.008
Elapsed time for mlp_4h_to_h (4x21888x21888, b=2048): 0.0308
Throughput (in TFLOP/s) for mlp_4h_to_h (4x21888x21888, b=2048): 254.739

Attention duration (in seconds): 0.0348
Attention throughput (in TFLOP/s): 236.429
MLP duration (in seconds): 0.0617
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0965
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22016x16512, b=2048): 0.0235
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22016x16512, b=2048): 253.220
Elapsed time for attention_key_query_prob (64x2048x344x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x344x2048): 180.144
Elapsed time for attention_prob_times_values (64x2048x2048x344): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x344): 166.120
Elapsed time for attention_linear_projection (4x5504x22016, b=2048): 0.0082
Throughput (in TFLOP/s) for attention_linear_projection (4x5504x22016, b=2048): 242.322
Elapsed time for mlp_h_to_4h (4x22016x22016, b=2048): 0.0313
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22016x22016, b=2048): 253.691
Elapsed time for mlp_4h_to_h (4x22016x22016, b=2048): 0.0310
Throughput (in TFLOP/s) for mlp_4h_to_h (4x22016x22016, b=2048): 255.852

Attention duration (in seconds): 0.0339
Attention throughput (in TFLOP/s): 245.509
MLP duration (in seconds): 0.0623
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0962
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22144x16608, b=2048): 0.0240
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22144x16608, b=2048): 251.033
Elapsed time for attention_key_query_prob (64x2048x346x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x346x2048): 111.927
Elapsed time for attention_prob_times_values (64x2048x2048x346): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x346): 111.606
Elapsed time for attention_linear_projection (4x5536x22144, b=2048): 0.0083
Throughput (in TFLOP/s) for attention_linear_projection (4x5536x22144, b=2048): 242.495
Elapsed time for mlp_h_to_4h (4x22144x22144, b=2048): 0.0319
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22144x22144, b=2048): 251.563
Elapsed time for mlp_4h_to_h (4x22144x22144, b=2048): 0.0319
Throughput (in TFLOP/s) for mlp_4h_to_h (4x22144x22144, b=2048): 251.700

Attention duration (in seconds): 0.0356
Attention throughput (in TFLOP/s): 236.047
MLP duration (in seconds): 0.0639
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0995
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22272x16704, b=2048): 0.0241
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22272x16704, b=2048): 252.422
Elapsed time for attention_key_query_prob (64x2048x348x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x348x2048): 112.364
Elapsed time for attention_prob_times_values (64x2048x2048x348): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x348): 112.931
Elapsed time for attention_linear_projection (4x5568x22272, b=2048): 0.0081
Throughput (in TFLOP/s) for attention_linear_projection (4x5568x22272, b=2048): 250.734
Elapsed time for mlp_h_to_4h (4x22272x22272, b=2048): 0.0317
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22272x22272, b=2048): 256.151
Elapsed time for mlp_4h_to_h (4x22272x22272, b=2048): 0.0319
Throughput (in TFLOP/s) for mlp_4h_to_h (4x22272x22272, b=2048): 255.041

Attention duration (in seconds): 0.0356
Attention throughput (in TFLOP/s): 239.002
MLP duration (in seconds): 0.0636
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0992
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22400x16800, b=2048): 0.0244
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22400x16800, b=2048): 252.229
Elapsed time for attention_key_query_prob (64x2048x350x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x350x2048): 114.238
Elapsed time for attention_prob_times_values (64x2048x2048x350): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x350): 115.342
Elapsed time for attention_linear_projection (4x5600x22400, b=2048): 0.0085
Throughput (in TFLOP/s) for attention_linear_projection (4x5600x22400, b=2048): 240.942
Elapsed time for mlp_h_to_4h (4x22400x22400, b=2048): 0.0321
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22400x22400, b=2048): 256.074
Elapsed time for mlp_4h_to_h (4x22400x22400, b=2048): 0.0321
Throughput (in TFLOP/s) for mlp_4h_to_h (4x22400x22400, b=2048): 256.198

Attention duration (in seconds): 0.0362
Attention throughput (in TFLOP/s): 237.159
MLP duration (in seconds): 0.0642
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1004
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22528x16896, b=2048): 0.0249
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22528x16896, b=2048): 250.114
Elapsed time for attention_key_query_prob (64x2048x352x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x352x2048): 194.178
Elapsed time for attention_prob_times_values (64x2048x2048x352): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x352): 170.093
Elapsed time for attention_linear_projection (4x5632x22528, b=2048): 0.0083
Throughput (in TFLOP/s) for attention_linear_projection (4x5632x22528, b=2048): 249.756
Elapsed time for mlp_h_to_4h (4x22528x22528, b=2048): 0.0333
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22528x22528, b=2048): 249.584
Elapsed time for mlp_4h_to_h (4x22528x22528, b=2048): 0.0331
Throughput (in TFLOP/s) for mlp_4h_to_h (4x22528x22528, b=2048): 251.300

Attention duration (in seconds): 0.0353
Attention throughput (in TFLOP/s): 245.973
MLP duration (in seconds): 0.0664
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22656x16992, b=2048): 0.0251
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22656x16992, b=2048): 250.932
Elapsed time for attention_key_query_prob (64x2048x354x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x354x2048): 109.647
Elapsed time for attention_prob_times_values (64x2048x2048x354): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x354): 115.427
Elapsed time for attention_linear_projection (4x5664x22656, b=2048): 0.0087
Throughput (in TFLOP/s) for attention_linear_projection (4x5664x22656, b=2048): 240.537
Elapsed time for mlp_h_to_4h (4x22656x22656, b=2048): 0.0335
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22656x22656, b=2048): 251.392
Elapsed time for mlp_4h_to_h (4x22656x22656, b=2048): 0.0330
Throughput (in TFLOP/s) for mlp_4h_to_h (4x22656x22656, b=2048): 254.729

Attention duration (in seconds): 0.0373
Attention throughput (in TFLOP/s): 235.931
MLP duration (in seconds): 0.0665
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22784x17088, b=2048): 0.0251
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22784x17088, b=2048): 254.357
Elapsed time for attention_key_query_prob (64x2048x356x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x356x2048): 110.025
Elapsed time for attention_prob_times_values (64x2048x2048x356): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x356): 115.543
Elapsed time for attention_linear_projection (4x5696x22784, b=2048): 0.0087
Throughput (in TFLOP/s) for attention_linear_projection (4x5696x22784, b=2048): 245.689
Elapsed time for mlp_h_to_4h (4x22784x22784, b=2048): 0.0335
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22784x22784, b=2048): 253.816
Elapsed time for mlp_4h_to_h (4x22784x22784, b=2048): 0.0335
Throughput (in TFLOP/s) for mlp_4h_to_h (4x22784x22784, b=2048): 253.513

Attention duration (in seconds): 0.0371
Attention throughput (in TFLOP/s): 239.397
MLP duration (in seconds): 0.0671
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22912x17184, b=2048): 0.0255
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22912x17184, b=2048): 252.982
Elapsed time for attention_key_query_prob (64x2048x358x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x358x2048): 110.280
Elapsed time for attention_prob_times_values (64x2048x2048x358): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x358): 116.293
Elapsed time for attention_linear_projection (4x5728x22912, b=2048): 0.0089
Throughput (in TFLOP/s) for attention_linear_projection (4x5728x22912, b=2048): 241.990
Elapsed time for mlp_h_to_4h (4x22912x22912, b=2048): 0.0343
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22912x22912, b=2048): 251.063
Elapsed time for mlp_4h_to_h (4x22912x22912, b=2048): 0.0343
Throughput (in TFLOP/s) for mlp_4h_to_h (4x22912x22912, b=2048): 250.913

Attention duration (in seconds): 0.0378
Attention throughput (in TFLOP/s): 237.834
MLP duration (in seconds): 0.0685
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23040x17280, b=2048): 0.0267
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23040x17280, b=2048): 244.737
Elapsed time for attention_key_query_prob (64x2048x360x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x360x2048): 178.596
Elapsed time for attention_prob_times_values (64x2048x2048x360): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x360): 168.219
Elapsed time for attention_linear_projection (4x5760x23040, b=2048): 0.0086
Throughput (in TFLOP/s) for attention_linear_projection (4x5760x23040, b=2048): 253.482
Elapsed time for mlp_h_to_4h (4x23040x23040, b=2048): 0.0362
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23040x23040, b=2048): 240.220
Elapsed time for mlp_4h_to_h (4x23040x23040, b=2048): 0.0403
Throughput (in TFLOP/s) for mlp_4h_to_h (4x23040x23040, b=2048): 215.780

Attention duration (in seconds): 0.0375
Attention throughput (in TFLOP/s): 242.482
MLP duration (in seconds): 0.0765
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23168x17376, b=2048): 0.0271
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23168x17376, b=2048): 243.564
Elapsed time for attention_key_query_prob (64x2048x362x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x362x2048): 112.125
Elapsed time for attention_prob_times_values (64x2048x2048x362): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x362): 117.187
Elapsed time for attention_linear_projection (4x5792x23168, b=2048): 0.0093
Throughput (in TFLOP/s) for attention_linear_projection (4x5792x23168, b=2048): 237.518
Elapsed time for mlp_h_to_4h (4x23168x23168, b=2048): 0.0364
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23168x23168, b=2048): 241.713
Elapsed time for mlp_4h_to_h (4x23168x23168, b=2048): 0.0363
Throughput (in TFLOP/s) for mlp_4h_to_h (4x23168x23168, b=2048): 242.159

Attention duration (in seconds): 0.0397
Attention throughput (in TFLOP/s): 231.145
MLP duration (in seconds): 0.0727
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23296x17472, b=2048): 0.0274
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23296x17472, b=2048): 243.471
Elapsed time for attention_key_query_prob (64x2048x364x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x364x2048): 111.518
Elapsed time for attention_prob_times_values (64x2048x2048x364): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x364): 118.328
Elapsed time for attention_linear_projection (4x5824x23296, b=2048): 0.0090
Throughput (in TFLOP/s) for attention_linear_projection (4x5824x23296, b=2048): 246.200
Elapsed time for mlp_h_to_4h (4x23296x23296, b=2048): 0.0350
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23296x23296, b=2048): 253.853
Elapsed time for mlp_4h_to_h (4x23296x23296, b=2048): 0.0347
Throughput (in TFLOP/s) for mlp_4h_to_h (4x23296x23296, b=2048): 256.167

Attention duration (in seconds): 0.0398
Attention throughput (in TFLOP/s): 233.094
MLP duration (in seconds): 0.0697
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23424x17568, b=2048): 0.0276
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23424x17568, b=2048): 244.668
Elapsed time for attention_key_query_prob (64x2048x366x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x366x2048): 111.343
Elapsed time for attention_prob_times_values (64x2048x2048x366): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x366): 118.176
Elapsed time for attention_linear_projection (4x5856x23424, b=2048): 0.0092
Throughput (in TFLOP/s) for attention_linear_projection (4x5856x23424, b=2048): 242.990
Elapsed time for mlp_h_to_4h (4x23424x23424, b=2048): 0.0373
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23424x23424, b=2048): 240.719
Elapsed time for mlp_4h_to_h (4x23424x23424, b=2048): 0.0370
Throughput (in TFLOP/s) for mlp_4h_to_h (4x23424x23424, b=2048): 242.968

Attention duration (in seconds): 0.0402
Attention throughput (in TFLOP/s): 233.206
MLP duration (in seconds): 0.0743
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23552x17664, b=2048): 0.0280
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23552x17664, b=2048): 243.227
Elapsed time for attention_key_query_prob (64x2048x368x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x368x2048): 187.778
Elapsed time for attention_prob_times_values (64x2048x2048x368): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x368): 177.065
Elapsed time for attention_linear_projection (4x5888x23552, b=2048): 0.0093
Throughput (in TFLOP/s) for attention_linear_projection (4x5888x23552, b=2048): 245.166
Elapsed time for mlp_h_to_4h (4x23552x23552, b=2048): 0.0362
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23552x23552, b=2048): 250.991
Elapsed time for mlp_4h_to_h (4x23552x23552, b=2048): 0.0360
Throughput (in TFLOP/s) for mlp_4h_to_h (4x23552x23552, b=2048): 252.116

Attention duration (in seconds): 0.0395
Attention throughput (in TFLOP/s): 240.333
MLP duration (in seconds): 0.0723
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23680x17760, b=2048): 0.0285
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23680x17760, b=2048): 241.798
Elapsed time for attention_key_query_prob (64x2048x370x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x370x2048): 110.838
Elapsed time for attention_prob_times_values (64x2048x2048x370): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x370): 121.135
Elapsed time for attention_linear_projection (4x5920x23680, b=2048): 0.0096
Throughput (in TFLOP/s) for attention_linear_projection (4x5920x23680, b=2048): 238.618
Elapsed time for mlp_h_to_4h (4x23680x23680, b=2048): 0.0362
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23680x23680, b=2048): 254.061
Elapsed time for mlp_4h_to_h (4x23680x23680, b=2048): 0.0361
Throughput (in TFLOP/s) for mlp_4h_to_h (4x23680x23680, b=2048): 254.687

Attention duration (in seconds): 0.0416
Attention throughput (in TFLOP/s): 230.652
MLP duration (in seconds): 0.0722
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23808x17856, b=2048): 0.0287
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23808x17856, b=2048): 242.379
Elapsed time for attention_key_query_prob (64x2048x372x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x372x2048): 112.303
Elapsed time for attention_prob_times_values (64x2048x2048x372): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x372): 120.476
Elapsed time for attention_linear_projection (4x5952x23808, b=2048): 0.0092
Throughput (in TFLOP/s) for attention_linear_projection (4x5952x23808, b=2048): 253.407
Elapsed time for mlp_h_to_4h (4x23808x23808, b=2048): 0.0385
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23808x23808, b=2048): 241.522
Elapsed time for mlp_4h_to_h (4x23808x23808, b=2048): 0.0383
Throughput (in TFLOP/s) for mlp_4h_to_h (4x23808x23808, b=2048): 242.579

Attention duration (in seconds): 0.0413
Attention throughput (in TFLOP/s): 234.338
MLP duration (in seconds): 0.0767
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23936x17952, b=2048): 0.0292
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23936x17952, b=2048): 241.169
Elapsed time for attention_key_query_prob (64x2048x374x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x374x2048): 112.320
Elapsed time for attention_prob_times_values (64x2048x2048x374): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x374): 121.966
Elapsed time for attention_linear_projection (4x5984x23936, b=2048): 0.0096
Throughput (in TFLOP/s) for attention_linear_projection (4x5984x23936, b=2048): 244.089
Elapsed time for mlp_h_to_4h (4x23936x23936, b=2048): 0.0372
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23936x23936, b=2048): 252.526
Elapsed time for mlp_4h_to_h (4x23936x23936, b=2048): 0.0371
Throughput (in TFLOP/s) for mlp_4h_to_h (4x23936x23936, b=2048): 253.265

Attention duration (in seconds): 0.0422
Attention throughput (in TFLOP/s): 231.735
MLP duration (in seconds): 0.0742
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24064x18048, b=2048): 0.0293
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24064x18048, b=2048): 242.785
Elapsed time for attention_key_query_prob (64x2048x376x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x376x2048): 184.180
Elapsed time for attention_prob_times_values (64x2048x2048x376): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x376): 176.722
Elapsed time for attention_linear_projection (4x6016x24064, b=2048): 0.0099
Throughput (in TFLOP/s) for attention_linear_projection (4x6016x24064, b=2048): 240.394
Elapsed time for mlp_h_to_4h (4x24064x24064, b=2048): 0.0373
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24064x24064, b=2048): 254.275
Elapsed time for mlp_4h_to_h (4x24064x24064, b=2048): 0.0374
Throughput (in TFLOP/s) for mlp_4h_to_h (4x24064x24064, b=2048): 253.743

Attention duration (in seconds): 0.0414
Attention throughput (in TFLOP/s): 238.842
MLP duration (in seconds): 0.0747
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24192x18144, b=2048): 0.0294
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24192x18144, b=2048): 244.738
Elapsed time for attention_key_query_prob (64x2048x378x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x378x2048): 112.874
Elapsed time for attention_prob_times_values (64x2048x2048x378): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x378): 122.384
Elapsed time for attention_linear_projection (4x6048x24192, b=2048): 0.0100
Throughput (in TFLOP/s) for attention_linear_projection (4x6048x24192, b=2048): 240.195
Elapsed time for mlp_h_to_4h (4x24192x24192, b=2048): 0.0375
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24192x24192, b=2048): 255.533
Elapsed time for mlp_4h_to_h (4x24192x24192, b=2048): 0.0375
Throughput (in TFLOP/s) for mlp_4h_to_h (4x24192x24192, b=2048): 255.663

Attention duration (in seconds): 0.0428
Attention throughput (in TFLOP/s): 233.405
MLP duration (in seconds): 0.0750
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24320x18240, b=2048): 0.0301
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24320x18240, b=2048): 241.633
Elapsed time for attention_key_query_prob (64x2048x380x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x380x2048): 114.305
Elapsed time for attention_prob_times_values (64x2048x2048x380): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x380): 123.173
Elapsed time for attention_linear_projection (4x6080x24320, b=2048): 0.0096
Throughput (in TFLOP/s) for attention_linear_projection (4x6080x24320, b=2048): 251.379
Elapsed time for mlp_h_to_4h (4x24320x24320, b=2048): 0.0401
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24320x24320, b=2048): 241.762
Elapsed time for mlp_4h_to_h (4x24320x24320, b=2048): 0.0399
Throughput (in TFLOP/s) for mlp_4h_to_h (4x24320x24320, b=2048): 242.754

Attention duration (in seconds): 0.0432
Attention throughput (in TFLOP/s): 233.997
MLP duration (in seconds): 0.0800
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24448x18336, b=2048): 0.0301
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24448x18336, b=2048): 243.768
Elapsed time for attention_key_query_prob (64x2048x382x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x382x2048): 113.902
Elapsed time for attention_prob_times_values (64x2048x2048x382): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x382): 125.887
Elapsed time for attention_linear_projection (4x6112x24448, b=2048): 0.0101
Throughput (in TFLOP/s) for attention_linear_projection (4x6112x24448, b=2048): 241.248
Elapsed time for mlp_h_to_4h (4x24448x24448, b=2048): 0.0384
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24448x24448, b=2048): 255.077
Elapsed time for mlp_4h_to_h (4x24448x24448, b=2048): 0.0385
Throughput (in TFLOP/s) for mlp_4h_to_h (4x24448x24448, b=2048): 254.686

Attention duration (in seconds): 0.0437
Attention throughput (in TFLOP/s): 233.439
MLP duration (in seconds): 0.0768
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24576x18432, b=2048): 0.0305
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24576x18432, b=2048): 243.529
Elapsed time for attention_key_query_prob (64x2048x384x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x384x2048): 198.006
Elapsed time for attention_prob_times_values (64x2048x2048x384): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x384): 184.645
Elapsed time for attention_linear_projection (4x6144x24576, b=2048): 0.0103
Throughput (in TFLOP/s) for attention_linear_projection (4x6144x24576, b=2048): 240.476
Elapsed time for mlp_h_to_4h (4x24576x24576, b=2048): 0.0391
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24576x24576, b=2048): 252.915
Elapsed time for mlp_4h_to_h (4x24576x24576, b=2048): 0.0388
Throughput (in TFLOP/s) for mlp_4h_to_h (4x24576x24576, b=2048): 255.200

Attention duration (in seconds): 0.0429
Attention throughput (in TFLOP/s): 240.161
MLP duration (in seconds): 0.0779
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24704x18528, b=2048): 0.0309
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24704x18528, b=2048): 242.500
Elapsed time for attention_key_query_prob (64x2048x386x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x386x2048): 105.446
Elapsed time for attention_prob_times_values (64x2048x2048x386): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x386): 125.461
Elapsed time for attention_linear_projection (4x6176x24704, b=2048): 0.0104
Throughput (in TFLOP/s) for attention_linear_projection (4x6176x24704, b=2048): 240.850
Elapsed time for mlp_h_to_4h (4x24704x24704, b=2048): 0.0410
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24704x24704, b=2048): 243.651
Elapsed time for mlp_4h_to_h (4x24704x24704, b=2048): 0.0410
Throughput (in TFLOP/s) for mlp_4h_to_h (4x24704x24704, b=2048): 244.096

Attention duration (in seconds): 0.0449
Attention throughput (in TFLOP/s): 231.819
MLP duration (in seconds): 0.0820
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24832x18624, b=2048): 0.0313
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24832x18624, b=2048): 241.710
Elapsed time for attention_key_query_prob (64x2048x388x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x388x2048): 107.771
Elapsed time for attention_prob_times_values (64x2048x2048x388): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x388): 125.297
Elapsed time for attention_linear_projection (4x6208x24832, b=2048): 0.0100
Throughput (in TFLOP/s) for attention_linear_projection (4x6208x24832, b=2048): 252.349
Elapsed time for mlp_h_to_4h (4x24832x24832, b=2048): 0.0395
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24832x24832, b=2048): 255.722
Elapsed time for mlp_4h_to_h (4x24832x24832, b=2048): 0.0398
Throughput (in TFLOP/s) for mlp_4h_to_h (4x24832x24832, b=2048): 253.573

Attention duration (in seconds): 0.0450
Attention throughput (in TFLOP/s): 234.014
MLP duration (in seconds): 0.0793
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24960x18720, b=2048): 0.0867
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24960x18720, b=2048): 88.338
Elapsed time for attention_key_query_prob (64x2048x390x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x390x2048): 108.020
Elapsed time for attention_prob_times_values (64x2048x2048x390): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x390): 124.356
Elapsed time for attention_linear_projection (4x6240x24960, b=2048): 0.0208
Throughput (in TFLOP/s) for attention_linear_projection (4x6240x24960, b=2048): 122.583
Elapsed time for mlp_h_to_4h (4x24960x24960, b=2048): 0.0401
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24960x24960, b=2048): 254.251
Elapsed time for mlp_4h_to_h (4x24960x24960, b=2048): 0.0398
Throughput (in TFLOP/s) for mlp_4h_to_h (4x24960x24960, b=2048): 256.177

Attention duration (in seconds): 0.1111
Attention throughput (in TFLOP/s): 95.644
MLP duration (in seconds): 0.0800
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1911
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25088x18816, b=2048): 0.0317
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25088x18816, b=2048): 243.781
Elapsed time for attention_key_query_prob (64x2048x392x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x392x2048): 181.105
Elapsed time for attention_prob_times_values (64x2048x2048x392): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x392): 184.166
Elapsed time for attention_linear_projection (4x6272x25088, b=2048): 0.0101
Throughput (in TFLOP/s) for attention_linear_projection (4x6272x25088, b=2048): 254.188
Elapsed time for mlp_h_to_4h (4x25088x25088, b=2048): 0.0422
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25088x25088, b=2048): 244.503
Elapsed time for mlp_4h_to_h (4x25088x25088, b=2048): 0.0425
Throughput (in TFLOP/s) for mlp_4h_to_h (4x25088x25088, b=2048): 242.449

Attention duration (in seconds): 0.0442
Attention throughput (in TFLOP/s): 242.979
MLP duration (in seconds): 0.0847
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1289
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25216x18912, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25216x18912, b=2048): 242.021
Elapsed time for attention_key_query_prob (64x2048x394x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x394x2048): 108.408
Elapsed time for attention_prob_times_values (64x2048x2048x394): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x394): 123.567
Elapsed time for attention_linear_projection (4x6304x25216, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x6304x25216, b=2048): 241.660
Elapsed time for mlp_h_to_4h (4x25216x25216, b=2048): 0.0410
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25216x25216, b=2048): 254.374
Elapsed time for mlp_4h_to_h (4x25216x25216, b=2048): 0.0408
Throughput (in TFLOP/s) for mlp_4h_to_h (4x25216x25216, b=2048): 255.407

Attention duration (in seconds): 0.0467
Attention throughput (in TFLOP/s): 232.018
MLP duration (in seconds): 0.0817
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25344x19008, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25344x19008, b=2048): 244.293
Elapsed time for attention_key_query_prob (64x2048x396x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x396x2048): 108.481
Elapsed time for attention_prob_times_values (64x2048x2048x396): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x396): 123.626
Elapsed time for attention_linear_projection (4x6336x25344, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x6336x25344, b=2048): 244.044
Elapsed time for mlp_h_to_4h (4x25344x25344, b=2048): 0.0410
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25344x25344, b=2048): 256.589
Elapsed time for mlp_4h_to_h (4x25344x25344, b=2048): 0.0413
Throughput (in TFLOP/s) for mlp_4h_to_h (4x25344x25344, b=2048): 255.044

Attention duration (in seconds): 0.0468
Attention throughput (in TFLOP/s): 234.107
MLP duration (in seconds): 0.0823
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1290
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25472x19104, b=2048): 0.0329
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25472x19104, b=2048): 242.442
Elapsed time for attention_key_query_prob (64x2048x398x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x398x2048): 110.168
Elapsed time for attention_prob_times_values (64x2048x2048x398): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x398): 124.216
Elapsed time for attention_linear_projection (4x6368x25472, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x6368x25472, b=2048): 243.234
Elapsed time for mlp_h_to_4h (4x25472x25472, b=2048): 0.0418
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25472x25472, b=2048): 254.157
Elapsed time for mlp_4h_to_h (4x25472x25472, b=2048): 0.0416
Throughput (in TFLOP/s) for mlp_4h_to_h (4x25472x25472, b=2048): 255.750

Attention duration (in seconds): 0.0475
Attention throughput (in TFLOP/s): 232.936
MLP duration (in seconds): 0.0834
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25600x19200, b=2048): 0.0331
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25600x19200, b=2048): 243.227
Elapsed time for attention_key_query_prob (64x2048x400x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x400x2048): 192.297
Elapsed time for attention_prob_times_values (64x2048x2048x400): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x400): 191.114
Elapsed time for attention_linear_projection (4x6400x25600, b=2048): 0.0107
Throughput (in TFLOP/s) for attention_linear_projection (4x6400x25600, b=2048): 251.817
Elapsed time for mlp_h_to_4h (4x25600x25600, b=2048): 0.0424
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25600x25600, b=2048): 253.237
Elapsed time for mlp_4h_to_h (4x25600x25600, b=2048): 0.0425
Throughput (in TFLOP/s) for mlp_4h_to_h (4x25600x25600, b=2048): 252.501

Attention duration (in seconds): 0.0460
Attention throughput (in TFLOP/s): 242.709
MLP duration (in seconds): 0.0849
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25728x19296, b=2048): 0.0343
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25728x19296, b=2048): 237.288
Elapsed time for attention_key_query_prob (64x2048x402x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x402x2048): 110.205
Elapsed time for attention_prob_times_values (64x2048x2048x402): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x402): 124.790
Elapsed time for attention_linear_projection (4x6432x25728, b=2048): 0.0112
Throughput (in TFLOP/s) for attention_linear_projection (4x6432x25728, b=2048): 242.678
Elapsed time for mlp_h_to_4h (4x25728x25728, b=2048): 0.0425
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25728x25728, b=2048): 255.335
Elapsed time for mlp_4h_to_h (4x25728x25728, b=2048): 0.0429
Throughput (in TFLOP/s) for mlp_4h_to_h (4x25728x25728, b=2048): 253.063

Attention duration (in seconds): 0.0491
Attention throughput (in TFLOP/s): 229.489
MLP duration (in seconds): 0.0853
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1345
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25856x19392, b=2048): 0.0338
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25856x19392, b=2048): 243.350
Elapsed time for attention_key_query_prob (64x2048x404x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x404x2048): 110.875
Elapsed time for attention_prob_times_values (64x2048x2048x404): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x404): 125.307
Elapsed time for attention_linear_projection (4x6464x25856, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x6464x25856, b=2048): 247.918
Elapsed time for mlp_h_to_4h (4x25856x25856, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25856x25856, b=2048): 253.649
Elapsed time for mlp_4h_to_h (4x25856x25856, b=2048): 0.0429
Throughput (in TFLOP/s) for mlp_4h_to_h (4x25856x25856, b=2048): 255.247

Attention duration (in seconds): 0.0485
Attention throughput (in TFLOP/s): 234.832
MLP duration (in seconds): 0.0861
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1346
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25984x19488, b=2048): 0.0349
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25984x19488, b=2048): 237.780
Elapsed time for attention_key_query_prob (64x2048x406x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x406x2048): 102.012
Elapsed time for attention_prob_times_values (64x2048x2048x406): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x406): 125.702
Elapsed time for attention_linear_projection (4x6496x25984, b=2048): 0.0113
Throughput (in TFLOP/s) for attention_linear_projection (4x6496x25984, b=2048): 244.294
Elapsed time for mlp_h_to_4h (4x25984x25984, b=2048): 0.0438
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25984x25984, b=2048): 252.277
Elapsed time for mlp_4h_to_h (4x25984x25984, b=2048): 0.0438
Throughput (in TFLOP/s) for mlp_4h_to_h (4x25984x25984, b=2048): 252.428

Attention duration (in seconds): 0.0501
Attention throughput (in TFLOP/s): 229.579
MLP duration (in seconds): 0.0877
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1378
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26112x19584, b=2048): 0.0344
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26112x19584, b=2048): 243.562
Elapsed time for attention_key_query_prob (64x2048x408x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x408x2048): 185.903
Elapsed time for attention_prob_times_values (64x2048x2048x408): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x408): 189.743
Elapsed time for attention_linear_projection (4x6528x26112, b=2048): 0.0112
Throughput (in TFLOP/s) for attention_linear_projection (4x6528x26112, b=2048): 249.629
Elapsed time for mlp_h_to_4h (4x26112x26112, b=2048): 0.0441
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26112x26112, b=2048): 253.036
Elapsed time for mlp_4h_to_h (4x26112x26112, b=2048): 0.0445
Throughput (in TFLOP/s) for mlp_4h_to_h (4x26112x26112, b=2048): 250.980

Attention duration (in seconds): 0.0479
Attention throughput (in TFLOP/s): 242.264
MLP duration (in seconds): 0.0887
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1366
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26240x19680, b=2048): 0.0349
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26240x19680, b=2048): 242.505
Elapsed time for attention_key_query_prob (64x2048x410x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x410x2048): 112.139
Elapsed time for attention_prob_times_values (64x2048x2048x410): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x410): 126.057
Elapsed time for attention_linear_projection (4x6560x26240, b=2048): 0.0116
Throughput (in TFLOP/s) for attention_linear_projection (4x6560x26240, b=2048): 243.340
Elapsed time for mlp_h_to_4h (4x26240x26240, b=2048): 0.0445
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26240x26240, b=2048): 253.761
Elapsed time for mlp_4h_to_h (4x26240x26240, b=2048): 0.0443
Throughput (in TFLOP/s) for mlp_4h_to_h (4x26240x26240, b=2048): 254.641

Attention duration (in seconds): 0.0502
Attention throughput (in TFLOP/s): 233.547
MLP duration (in seconds): 0.0888
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1389
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26368x19776, b=2048): 0.0351
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26368x19776, b=2048): 243.102
Elapsed time for attention_key_query_prob (64x2048x412x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x412x2048): 112.645
Elapsed time for attention_prob_times_values (64x2048x2048x412): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x412): 126.654
Elapsed time for attention_linear_projection (4x6592x26368, b=2048): 0.0112
Throughput (in TFLOP/s) for attention_linear_projection (4x6592x26368, b=2048): 253.258
Elapsed time for mlp_h_to_4h (4x26368x26368, b=2048): 0.0449
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26368x26368, b=2048): 253.624
Elapsed time for mlp_4h_to_h (4x26368x26368, b=2048): 0.0452
Throughput (in TFLOP/s) for mlp_4h_to_h (4x26368x26368, b=2048): 252.278

Attention duration (in seconds): 0.0501
Attention throughput (in TFLOP/s): 236.209
MLP duration (in seconds): 0.0901
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1402
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26496x19872, b=2048): 0.0357
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26496x19872, b=2048): 241.779
Elapsed time for attention_key_query_prob (64x2048x414x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x414x2048): 112.672
Elapsed time for attention_prob_times_values (64x2048x2048x414): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x414): 127.408
Elapsed time for attention_linear_projection (4x6624x26496, b=2048): 0.0177
Throughput (in TFLOP/s) for attention_linear_projection (4x6624x26496, b=2048): 162.223
Elapsed time for mlp_h_to_4h (4x26496x26496, b=2048): 0.0452
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26496x26496, b=2048): 254.692
Elapsed time for mlp_4h_to_h (4x26496x26496, b=2048): 0.0451
Throughput (in TFLOP/s) for mlp_4h_to_h (4x26496x26496, b=2048): 254.826

Attention duration (in seconds): 0.0571
Attention throughput (in TFLOP/s): 209.140
MLP duration (in seconds): 0.0903
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1474
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26624x19968, b=2048): 0.0358
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26624x19968, b=2048): 243.334
Elapsed time for attention_key_query_prob (64x2048x416x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x416x2048): 200.675
Elapsed time for attention_prob_times_values (64x2048x2048x416): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x416): 198.506
Elapsed time for attention_linear_projection (4x6656x26624, b=2048): 0.0116
Throughput (in TFLOP/s) for attention_linear_projection (4x6656x26624, b=2048): 250.612
Elapsed time for mlp_h_to_4h (4x26624x26624, b=2048): 0.0457
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26624x26624, b=2048): 254.125
Elapsed time for mlp_4h_to_h (4x26624x26624, b=2048): 0.0456
Throughput (in TFLOP/s) for mlp_4h_to_h (4x26624x26624, b=2048): 254.518

Attention duration (in seconds): 0.0496
Attention throughput (in TFLOP/s): 243.060
MLP duration (in seconds): 0.0913
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1409
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26752x20064, b=2048): 0.0362
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26752x20064, b=2048): 243.267
Elapsed time for attention_key_query_prob (64x2048x418x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x418x2048): 114.327
Elapsed time for attention_prob_times_values (64x2048x2048x418): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x418): 127.888
Elapsed time for attention_linear_projection (4x6688x26752, b=2048): 0.0120
Throughput (in TFLOP/s) for attention_linear_projection (4x6688x26752, b=2048): 244.110
Elapsed time for mlp_h_to_4h (4x26752x26752, b=2048): 0.0467
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26752x26752, b=2048): 251.237
Elapsed time for mlp_4h_to_h (4x26752x26752, b=2048): 0.0462
Throughput (in TFLOP/s) for mlp_4h_to_h (4x26752x26752, b=2048): 253.989

Attention duration (in seconds): 0.0519
Attention throughput (in TFLOP/s): 234.680
MLP duration (in seconds): 0.0928
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1447
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26880x20160, b=2048): 0.0364
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26880x20160, b=2048): 244.047
Elapsed time for attention_key_query_prob (64x2048x420x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x420x2048): 115.350
Elapsed time for attention_prob_times_values (64x2048x2048x420): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x420): 128.151
Elapsed time for attention_linear_projection (4x6720x26880, b=2048): 0.0119
Throughput (in TFLOP/s) for attention_linear_projection (4x6720x26880, b=2048): 248.973
Elapsed time for mlp_h_to_4h (4x26880x26880, b=2048): 0.0469
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26880x26880, b=2048): 252.592
Elapsed time for mlp_4h_to_h (4x26880x26880, b=2048): 0.0469
Throughput (in TFLOP/s) for mlp_4h_to_h (4x26880x26880, b=2048): 252.593

Attention duration (in seconds): 0.0520
Attention throughput (in TFLOP/s): 236.411
MLP duration (in seconds): 0.0937
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1457
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27008x20256, b=2048): 0.0370
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27008x20256, b=2048): 242.443
Elapsed time for attention_key_query_prob (64x2048x422x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x422x2048): 115.197
Elapsed time for attention_prob_times_values (64x2048x2048x422): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x422): 128.570
Elapsed time for attention_linear_projection (4x6752x27008, b=2048): 0.0124
Throughput (in TFLOP/s) for attention_linear_projection (4x6752x27008, b=2048): 240.520
Elapsed time for mlp_h_to_4h (4x27008x27008, b=2048): 0.0478
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27008x27008, b=2048): 249.831
Elapsed time for mlp_4h_to_h (4x27008x27008, b=2048): 0.0472
Throughput (in TFLOP/s) for mlp_4h_to_h (4x27008x27008, b=2048): 253.234

Attention duration (in seconds): 0.0531
Attention throughput (in TFLOP/s): 233.505
MLP duration (in seconds): 0.0950
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1482
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27136x20352, b=2048): 0.0373
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27136x20352, b=2048): 242.688
Elapsed time for attention_key_query_prob (64x2048x424x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x424x2048): 181.410
Elapsed time for attention_prob_times_values (64x2048x2048x424): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x424): 196.615
Elapsed time for attention_linear_projection (4x6784x27136, b=2048): 0.0123
Throughput (in TFLOP/s) for attention_linear_projection (4x6784x27136, b=2048): 245.590
Elapsed time for mlp_h_to_4h (4x27136x27136, b=2048): 0.0478
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27136x27136, b=2048): 252.308
Elapsed time for mlp_4h_to_h (4x27136x27136, b=2048): 0.0479
Throughput (in TFLOP/s) for mlp_4h_to_h (4x27136x27136, b=2048): 252.116

Attention duration (in seconds): 0.0520
Attention throughput (in TFLOP/s): 240.868
MLP duration (in seconds): 0.0957
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1476
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27264x20448, b=2048): 0.0376
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27264x20448, b=2048): 243.054
Elapsed time for attention_key_query_prob (64x2048x426x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x426x2048): 115.812
Elapsed time for attention_prob_times_values (64x2048x2048x426): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x426): 128.795
Elapsed time for attention_linear_projection (4x6816x27264, b=2048): 0.0126
Throughput (in TFLOP/s) for attention_linear_projection (4x6816x27264, b=2048): 241.888
Elapsed time for mlp_h_to_4h (4x27264x27264, b=2048): 0.0483
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27264x27264, b=2048): 251.958
Elapsed time for mlp_4h_to_h (4x27264x27264, b=2048): 0.0491
Throughput (in TFLOP/s) for mlp_4h_to_h (4x27264x27264, b=2048): 248.146

Attention duration (in seconds): 0.0539
Attention throughput (in TFLOP/s): 234.359
MLP duration (in seconds): 0.0974
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1513
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27392x20544, b=2048): 0.0379
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27392x20544, b=2048): 243.136
Elapsed time for attention_key_query_prob (64x2048x428x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x428x2048): 116.440
Elapsed time for attention_prob_times_values (64x2048x2048x428): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x428): 130.894
Elapsed time for attention_linear_projection (4x6848x27392, b=2048): 0.0123
Throughput (in TFLOP/s) for attention_linear_projection (4x6848x27392, b=2048): 250.173
Elapsed time for mlp_h_to_4h (4x27392x27392, b=2048): 0.0486
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27392x27392, b=2048): 253.069
Elapsed time for mlp_4h_to_h (4x27392x27392, b=2048): 0.0492
Throughput (in TFLOP/s) for mlp_4h_to_h (4x27392x27392, b=2048): 249.932

Attention duration (in seconds): 0.0539
Attention throughput (in TFLOP/s): 236.450
MLP duration (in seconds): 0.0978
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1517
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27520x20640, b=2048): 0.0380
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27520x20640, b=2048): 244.841
Elapsed time for attention_key_query_prob (64x2048x430x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x430x2048): 117.652
Elapsed time for attention_prob_times_values (64x2048x2048x430): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x430): 131.363
Elapsed time for attention_linear_projection (4x6880x27520, b=2048): 0.0127
Throughput (in TFLOP/s) for attention_linear_projection (4x6880x27520, b=2048): 243.404
Elapsed time for mlp_h_to_4h (4x27520x27520, b=2048): 0.0493
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27520x27520, b=2048): 251.848
Elapsed time for mlp_4h_to_h (4x27520x27520, b=2048): 0.0489
Throughput (in TFLOP/s) for mlp_4h_to_h (4x27520x27520, b=2048): 253.909

Attention duration (in seconds): 0.0545
Attention throughput (in TFLOP/s): 236.263
MLP duration (in seconds): 0.0981
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1526
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27648x20736, b=2048): 0.0383
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27648x20736, b=2048): 245.116
Elapsed time for attention_key_query_prob (64x2048x432x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x432x2048): 193.742
Elapsed time for attention_prob_times_values (64x2048x2048x432): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x432): 205.792
Elapsed time for attention_linear_projection (4x6912x27648, b=2048): 0.0125
Throughput (in TFLOP/s) for attention_linear_projection (4x6912x27648, b=2048): 251.056
Elapsed time for mlp_h_to_4h (4x27648x27648, b=2048): 0.0492
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27648x27648, b=2048): 254.386
Elapsed time for mlp_4h_to_h (4x27648x27648, b=2048): 0.0497
Throughput (in TFLOP/s) for mlp_4h_to_h (4x27648x27648, b=2048): 251.826

Attention duration (in seconds): 0.0531
Attention throughput (in TFLOP/s): 244.518
MLP duration (in seconds): 0.0990
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1521
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27776x20832, b=2048): 0.0388
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27776x20832, b=2048): 244.268
Elapsed time for attention_key_query_prob (64x2048x434x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x434x2048): 118.963
Elapsed time for attention_prob_times_values (64x2048x2048x434): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x434): 131.994
Elapsed time for attention_linear_projection (4x6944x27776, b=2048): 0.0128
Throughput (in TFLOP/s) for attention_linear_projection (4x6944x27776, b=2048): 245.952
Elapsed time for mlp_h_to_4h (4x27776x27776, b=2048): 0.0505
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27776x27776, b=2048): 250.083
Elapsed time for mlp_4h_to_h (4x27776x27776, b=2048): 0.0504
Throughput (in TFLOP/s) for mlp_4h_to_h (4x27776x27776, b=2048): 250.632

Attention duration (in seconds): 0.0554
Attention throughput (in TFLOP/s): 236.649
MLP duration (in seconds): 0.1010
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1564
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27904x20928, b=2048): 0.0394
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27904x20928, b=2048): 242.837
Elapsed time for attention_key_query_prob (64x2048x436x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x436x2048): 118.990
Elapsed time for attention_prob_times_values (64x2048x2048x436): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x436): 132.352
Elapsed time for attention_linear_projection (4x6976x27904, b=2048): 0.0130
Throughput (in TFLOP/s) for attention_linear_projection (4x6976x27904, b=2048): 244.705
Elapsed time for mlp_h_to_4h (4x27904x27904, b=2048): 0.0509
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27904x27904, b=2048): 250.554
Elapsed time for mlp_4h_to_h (4x27904x27904, b=2048): 0.0502
Throughput (in TFLOP/s) for mlp_4h_to_h (4x27904x27904, b=2048): 253.951

Attention duration (in seconds): 0.0562
Attention throughput (in TFLOP/s): 235.454
MLP duration (in seconds): 0.1012
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1573
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28032x21024, b=2048): 0.0396
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28032x21024, b=2048): 244.081
Elapsed time for attention_key_query_prob (64x2048x438x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x438x2048): 119.478
Elapsed time for attention_prob_times_values (64x2048x2048x438): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x438): 132.388
Elapsed time for attention_linear_projection (4x7008x28032, b=2048): 0.0134
Throughput (in TFLOP/s) for attention_linear_projection (4x7008x28032, b=2048): 240.553
Elapsed time for mlp_h_to_4h (4x28032x28032, b=2048): 0.0521
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28032x28032, b=2048): 247.091
Elapsed time for mlp_4h_to_h (4x28032x28032, b=2048): 0.0513
Throughput (in TFLOP/s) for mlp_4h_to_h (4x28032x28032, b=2048): 251.207

Attention duration (in seconds): 0.0567
Attention throughput (in TFLOP/s): 235.422
MLP duration (in seconds): 0.1034
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1600
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28160x21120, b=2048): 0.0398
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28160x21120, b=2048): 245.094
Elapsed time for attention_key_query_prob (64x2048x440x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x440x2048): 184.643
Elapsed time for attention_prob_times_values (64x2048x2048x440): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x440): 202.326
Elapsed time for attention_linear_projection (4x7040x28160, b=2048): 0.0130
Throughput (in TFLOP/s) for attention_linear_projection (4x7040x28160, b=2048): 249.682
Elapsed time for mlp_h_to_4h (4x28160x28160, b=2048): 0.0515
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28160x28160, b=2048): 252.108
Elapsed time for mlp_4h_to_h (4x28160x28160, b=2048): 0.0514
Throughput (in TFLOP/s) for mlp_4h_to_h (4x28160x28160, b=2048): 252.671

Attention duration (in seconds): 0.0552
Attention throughput (in TFLOP/s): 243.870
MLP duration (in seconds): 0.1030
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1582
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28288x21216, b=2048): 0.0401
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28288x21216, b=2048): 245.087
Elapsed time for attention_key_query_prob (64x2048x442x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x442x2048): 121.274
Elapsed time for attention_prob_times_values (64x2048x2048x442): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x442): 134.536
Elapsed time for attention_linear_projection (4x7072x28288, b=2048): 0.0134
Throughput (in TFLOP/s) for attention_linear_projection (4x7072x28288, b=2048): 243.711
Elapsed time for mlp_h_to_4h (4x28288x28288, b=2048): 0.0562
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28288x28288, b=2048): 233.448
Elapsed time for mlp_4h_to_h (4x28288x28288, b=2048): 0.0522
Throughput (in TFLOP/s) for mlp_4h_to_h (4x28288x28288, b=2048): 251.191

Attention duration (in seconds): 0.0573
Attention throughput (in TFLOP/s): 237.132
MLP duration (in seconds): 0.1084
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1656
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28416x21312, b=2048): 0.0409
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28416x21312, b=2048): 242.769
Elapsed time for attention_key_query_prob (64x2048x444x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x444x2048): 121.100
Elapsed time for attention_prob_times_values (64x2048x2048x444): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x444): 135.364
Elapsed time for attention_linear_projection (4x7104x28416, b=2048): 0.0133
Throughput (in TFLOP/s) for attention_linear_projection (4x7104x28416, b=2048): 248.490
Elapsed time for mlp_h_to_4h (4x28416x28416, b=2048): 0.0525
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28416x28416, b=2048): 251.957
Elapsed time for mlp_4h_to_h (4x28416x28416, b=2048): 0.0522
Throughput (in TFLOP/s) for mlp_4h_to_h (4x28416x28416, b=2048): 253.373

Attention duration (in seconds): 0.0579
Attention throughput (in TFLOP/s): 236.682
MLP duration (in seconds): 0.1047
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1626
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28544x21408, b=2048): 0.0413
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28544x21408, b=2048): 242.670
Elapsed time for attention_key_query_prob (64x2048x446x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x446x2048): 121.307
Elapsed time for attention_prob_times_values (64x2048x2048x446): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x446): 137.219
Elapsed time for attention_linear_projection (4x7136x28544, b=2048): 0.0138
Throughput (in TFLOP/s) for attention_linear_projection (4x7136x28544, b=2048): 242.376
Elapsed time for mlp_h_to_4h (4x28544x28544, b=2048): 0.0532
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28544x28544, b=2048): 251.076
Elapsed time for mlp_4h_to_h (4x28544x28544, b=2048): 0.0941
Throughput (in TFLOP/s) for mlp_4h_to_h (4x28544x28544, b=2048): 141.818

Attention duration (in seconds): 0.0587
Attention throughput (in TFLOP/s): 235.391
MLP duration (in seconds): 0.1473
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28672x21504, b=2048): 0.0418
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28672x21504, b=2048): 241.955
Elapsed time for attention_key_query_prob (64x2048x448x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x448x2048): 201.038
Elapsed time for attention_prob_times_values (64x2048x2048x448): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x448): 207.573
Elapsed time for attention_linear_projection (4x7168x28672, b=2048): 0.0136
Throughput (in TFLOP/s) for attention_linear_projection (4x7168x28672, b=2048): 247.291
Elapsed time for mlp_h_to_4h (4x28672x28672, b=2048): 0.0537
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28672x28672, b=2048): 250.864
Elapsed time for mlp_4h_to_h (4x28672x28672, b=2048): 0.0537
Throughput (in TFLOP/s) for mlp_4h_to_h (4x28672x28672, b=2048): 250.632

Attention duration (in seconds): 0.0577
Attention throughput (in TFLOP/s): 241.675
MLP duration (in seconds): 0.1074
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1652
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28800x21600, b=2048): 0.0419
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28800x21600, b=2048): 243.523
Elapsed time for attention_key_query_prob (64x2048x450x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x450x2048): 114.550
Elapsed time for attention_prob_times_values (64x2048x2048x450): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x450): 138.279
Elapsed time for attention_linear_projection (4x7200x28800, b=2048): 0.0143
Throughput (in TFLOP/s) for attention_linear_projection (4x7200x28800, b=2048): 238.293
Elapsed time for mlp_h_to_4h (4x28800x28800, b=2048): 0.0537
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28800x28800, b=2048): 252.961
Elapsed time for mlp_4h_to_h (4x28800x28800, b=2048): 0.0536
Throughput (in TFLOP/s) for mlp_4h_to_h (4x28800x28800, b=2048): 253.377

Attention duration (in seconds): 0.0600
Attention throughput (in TFLOP/s): 234.677
MLP duration (in seconds): 0.1074
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1673
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28928x21696, b=2048): 0.0422
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28928x21696, b=2048): 243.607
Elapsed time for attention_key_query_prob (64x2048x452x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x452x2048): 114.425
Elapsed time for attention_prob_times_values (64x2048x2048x452): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x452): 137.375
Elapsed time for attention_linear_projection (4x7232x28928, b=2048): 0.0140
Throughput (in TFLOP/s) for attention_linear_projection (4x7232x28928, b=2048): 245.393
Elapsed time for mlp_h_to_4h (4x28928x28928, b=2048): 0.0546
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28928x28928, b=2048): 251.066
Elapsed time for mlp_4h_to_h (4x28928x28928, b=2048): 0.0545
Throughput (in TFLOP/s) for mlp_4h_to_h (4x28928x28928, b=2048): 251.535

Attention duration (in seconds): 0.0601
Attention throughput (in TFLOP/s): 236.337
MLP duration (in seconds): 0.1091
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1692
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29056x21792, b=2048): 0.0427
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29056x21792, b=2048): 243.118
Elapsed time for attention_key_query_prob (64x2048x454x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x454x2048): 114.932
Elapsed time for attention_prob_times_values (64x2048x2048x454): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x454): 132.631
Elapsed time for attention_linear_projection (4x7264x29056, b=2048): 0.0142
Throughput (in TFLOP/s) for attention_linear_projection (4x7264x29056, b=2048): 243.386
Elapsed time for mlp_h_to_4h (4x29056x29056, b=2048): 0.0547
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29056x29056, b=2048): 253.096
Elapsed time for mlp_4h_to_h (4x29056x29056, b=2048): 0.0554
Throughput (in TFLOP/s) for mlp_4h_to_h (4x29056x29056, b=2048): 249.746

Attention duration (in seconds): 0.0608
Attention throughput (in TFLOP/s): 235.375
MLP duration (in seconds): 0.1100
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1709
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29184x21888, b=2048): 0.0430
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29184x21888, b=2048): 243.432
Elapsed time for attention_key_query_prob (64x2048x456x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x456x2048): 180.556
Elapsed time for attention_prob_times_values (64x2048x2048x456): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x456): 206.438
Elapsed time for attention_linear_projection (4x7296x29184, b=2048): 0.0151
Throughput (in TFLOP/s) for attention_linear_projection (4x7296x29184, b=2048): 231.102
Elapsed time for mlp_h_to_4h (4x29184x29184, b=2048): 0.0549
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29184x29184, b=2048): 254.354
Elapsed time for mlp_4h_to_h (4x29184x29184, b=2048): 0.0549
Throughput (in TFLOP/s) for mlp_4h_to_h (4x29184x29184, b=2048): 254.178

Attention duration (in seconds): 0.0606
Attention throughput (in TFLOP/s): 238.232
MLP duration (in seconds): 0.1098
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1704
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29312x21984, b=2048): 0.0436
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29312x21984, b=2048): 242.304
Elapsed time for attention_key_query_prob (64x2048x458x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x458x2048): 115.723
Elapsed time for attention_prob_times_values (64x2048x2048x458): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x458): 137.071
Elapsed time for attention_linear_projection (4x7328x29312, b=2048): 0.0150
Throughput (in TFLOP/s) for attention_linear_projection (4x7328x29312, b=2048): 234.753
Elapsed time for mlp_h_to_4h (4x29312x29312, b=2048): 0.0559
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29312x29312, b=2048): 251.929
Elapsed time for mlp_4h_to_h (4x29312x29312, b=2048): 0.0553
Throughput (in TFLOP/s) for mlp_4h_to_h (4x29312x29312, b=2048): 254.459

Attention duration (in seconds): 0.0625
Attention throughput (in TFLOP/s): 233.167
MLP duration (in seconds): 0.1112
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1737
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29440x22080, b=2048): 0.0440
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29440x22080, b=2048): 241.863
Elapsed time for attention_key_query_prob (64x2048x460x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x460x2048): 116.320
Elapsed time for attention_prob_times_values (64x2048x2048x460): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x460): 137.926
Elapsed time for attention_linear_projection (4x7360x29440, b=2048): 0.0149
Throughput (in TFLOP/s) for attention_linear_projection (4x7360x29440, b=2048): 238.366
Elapsed time for mlp_h_to_4h (4x29440x29440, b=2048): 0.0562
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29440x29440, b=2048): 252.708
Elapsed time for mlp_4h_to_h (4x29440x29440, b=2048): 0.0603
Throughput (in TFLOP/s) for mlp_4h_to_h (4x29440x29440, b=2048): 235.626

Attention duration (in seconds): 0.0628
Attention throughput (in TFLOP/s): 233.831
MLP duration (in seconds): 0.1165
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1793
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29568x22176, b=2048): 0.0442
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29568x22176, b=2048): 242.902
Elapsed time for attention_key_query_prob (64x2048x462x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x462x2048): 117.102
Elapsed time for attention_prob_times_values (64x2048x2048x462): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x462): 138.287
Elapsed time for attention_linear_projection (4x7392x29568, b=2048): 0.0153
Throughput (in TFLOP/s) for attention_linear_projection (4x7392x29568, b=2048): 233.560
Elapsed time for mlp_h_to_4h (4x29568x29568, b=2048): 0.0567
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29568x29568, b=2048): 252.700
Elapsed time for mlp_4h_to_h (4x29568x29568, b=2048): 0.0571
Throughput (in TFLOP/s) for mlp_4h_to_h (4x29568x29568, b=2048): 250.889

Attention duration (in seconds): 0.0635
Attention throughput (in TFLOP/s): 233.491
MLP duration (in seconds): 0.1138
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1772
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29696x22272, b=2048): 0.0446
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29696x22272, b=2048): 243.078
Elapsed time for attention_key_query_prob (64x2048x464x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x464x2048): 196.619
Elapsed time for attention_prob_times_values (64x2048x2048x464): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x464): 216.636
Elapsed time for attention_linear_projection (4x7424x29696, b=2048): 0.0152
Throughput (in TFLOP/s) for attention_linear_projection (4x7424x29696, b=2048): 237.477
Elapsed time for mlp_h_to_4h (4x29696x29696, b=2048): 0.0639
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29696x29696, b=2048): 226.057
Elapsed time for mlp_4h_to_h (4x29696x29696, b=2048): 0.0582
Throughput (in TFLOP/s) for mlp_4h_to_h (4x29696x29696, b=2048): 248.385

Attention duration (in seconds): 0.0622
Attention throughput (in TFLOP/s): 240.274
MLP duration (in seconds): 0.1221
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1843
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29824x22368, b=2048): 0.0451
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29824x22368, b=2048): 242.357
Elapsed time for attention_key_query_prob (64x2048x466x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x466x2048): 117.205
Elapsed time for attention_prob_times_values (64x2048x2048x466): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x466): 139.336
Elapsed time for attention_linear_projection (4x7456x29824, b=2048): 0.0195
Throughput (in TFLOP/s) for attention_linear_projection (4x7456x29824, b=2048): 187.111
Elapsed time for mlp_h_to_4h (4x29824x29824, b=2048): 0.0578
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29824x29824, b=2048): 251.923
Elapsed time for mlp_4h_to_h (4x29824x29824, b=2048): 0.0582
Throughput (in TFLOP/s) for mlp_4h_to_h (4x29824x29824, b=2048): 250.343

Attention duration (in seconds): 0.0685
Attention throughput (in TFLOP/s): 220.053
MLP duration (in seconds): 0.1161
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1846
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29952x22464, b=2048): 0.0455
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29952x22464, b=2048): 242.197
Elapsed time for attention_key_query_prob (64x2048x468x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x468x2048): 115.921
Elapsed time for attention_prob_times_values (64x2048x2048x468): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x468): 140.251
Elapsed time for attention_linear_projection (4x7488x29952, b=2048): 0.0154
Throughput (in TFLOP/s) for attention_linear_projection (4x7488x29952, b=2048): 239.264
Elapsed time for mlp_h_to_4h (4x29952x29952, b=2048): 0.0587
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29952x29952, b=2048): 250.358
Elapsed time for mlp_4h_to_h (4x29952x29952, b=2048): 0.0587
Throughput (in TFLOP/s) for mlp_4h_to_h (4x29952x29952, b=2048): 250.523

Attention duration (in seconds): 0.0648
Attention throughput (in TFLOP/s): 234.464
MLP duration (in seconds): 0.1174
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1822
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30080x22560, b=2048): 0.0458
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30080x22560, b=2048): 242.950
Elapsed time for attention_key_query_prob (64x2048x470x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x470x2048): 118.264
Elapsed time for attention_prob_times_values (64x2048x2048x470): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x470): 140.794
Elapsed time for attention_linear_projection (4x7520x30080, b=2048): 0.0160
Throughput (in TFLOP/s) for attention_linear_projection (4x7520x30080, b=2048): 231.100
Elapsed time for mlp_h_to_4h (4x30080x30080, b=2048): 0.0583
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30080x30080, b=2048): 254.163
Elapsed time for mlp_4h_to_h (4x30080x30080, b=2048): 0.0588
Throughput (in TFLOP/s) for mlp_4h_to_h (4x30080x30080, b=2048): 252.230

Attention duration (in seconds): 0.0657
Attention throughput (in TFLOP/s): 233.226
MLP duration (in seconds): 0.1171
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1828
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30208x22656, b=2048): 0.0461
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30208x22656, b=2048): 243.420
Elapsed time for attention_key_query_prob (64x2048x472x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x472x2048): 189.490
Elapsed time for attention_prob_times_values (64x2048x2048x472): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x472): 213.081
Elapsed time for attention_linear_projection (4x7552x30208, b=2048): 0.0156
Throughput (in TFLOP/s) for attention_linear_projection (4x7552x30208, b=2048): 240.073
Elapsed time for mlp_h_to_4h (4x30208x30208, b=2048): 0.0588
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30208x30208, b=2048): 254.332
Elapsed time for mlp_4h_to_h (4x30208x30208, b=2048): 0.0588
Throughput (in TFLOP/s) for mlp_4h_to_h (4x30208x30208, b=2048): 254.161

Attention duration (in seconds): 0.0642
Attention throughput (in TFLOP/s): 240.921
MLP duration (in seconds): 0.1176
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1818
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30336x22752, b=2048): 0.0470
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30336x22752, b=2048): 240.438
Elapsed time for attention_key_query_prob (64x2048x474x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x474x2048): 119.377
Elapsed time for attention_prob_times_values (64x2048x2048x474): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x474): 142.086
Elapsed time for attention_linear_projection (4x7584x30336, b=2048): 0.0155
Throughput (in TFLOP/s) for attention_linear_projection (4x7584x30336, b=2048): 243.623
Elapsed time for mlp_h_to_4h (4x30336x30336, b=2048): 0.0596
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30336x30336, b=2048): 253.100
Elapsed time for mlp_4h_to_h (4x30336x30336, b=2048): 0.0597
Throughput (in TFLOP/s) for mlp_4h_to_h (4x30336x30336, b=2048): 252.614

Attention duration (in seconds): 0.0664
Attention throughput (in TFLOP/s): 234.643
MLP duration (in seconds): 0.1193
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1857
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30464x22848, b=2048): 0.0471
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30464x22848, b=2048): 242.276
Elapsed time for attention_key_query_prob (64x2048x476x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x476x2048): 119.654
Elapsed time for attention_prob_times_values (64x2048x2048x476): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x476): 142.629
Elapsed time for attention_linear_projection (4x7616x30464, b=2048): 0.0149
Throughput (in TFLOP/s) for attention_linear_projection (4x7616x30464, b=2048): 254.788
Elapsed time for mlp_h_to_4h (4x30464x30464, b=2048): 0.0611
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30464x30464, b=2048): 248.895
Elapsed time for mlp_4h_to_h (4x30464x30464, b=2048): 0.0606
Throughput (in TFLOP/s) for mlp_4h_to_h (4x30464x30464, b=2048): 250.896

Attention duration (in seconds): 0.0659
Attention throughput (in TFLOP/s): 238.426
MLP duration (in seconds): 0.1217
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1876
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30592x22944, b=2048): 0.0472
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30592x22944, b=2048): 243.635
Elapsed time for attention_key_query_prob (64x2048x478x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x478x2048): 119.795
Elapsed time for attention_prob_times_values (64x2048x2048x478): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x478): 124.651
Elapsed time for attention_linear_projection (4x7648x30592, b=2048): 0.0160
Throughput (in TFLOP/s) for attention_linear_projection (4x7648x30592, b=2048): 239.404
Elapsed time for mlp_h_to_4h (4x30592x30592, b=2048): 0.0612
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30592x30592, b=2048): 250.543
Elapsed time for mlp_4h_to_h (4x30592x30592, b=2048): 0.0610
Throughput (in TFLOP/s) for mlp_4h_to_h (4x30592x30592, b=2048): 251.530

Attention duration (in seconds): 0.0674
Attention throughput (in TFLOP/s): 235.061
MLP duration (in seconds): 0.1222
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1896
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30720x23040, b=2048): 0.0476
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30720x23040, b=2048): 243.532
Elapsed time for attention_key_query_prob (64x2048x480x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x480x2048): 203.285
Elapsed time for attention_prob_times_values (64x2048x2048x480): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x480): 217.172
Elapsed time for attention_linear_projection (4x7680x30720, b=2048): 0.0154
Throughput (in TFLOP/s) for attention_linear_projection (4x7680x30720, b=2048): 250.471
Elapsed time for mlp_h_to_4h (4x30720x30720, b=2048): 0.0612
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30720x30720, b=2048): 252.641
Elapsed time for mlp_4h_to_h (4x30720x30720, b=2048): 0.0611
Throughput (in TFLOP/s) for mlp_4h_to_h (4x30720x30720, b=2048): 252.946

Attention duration (in seconds): 0.0655
Attention throughput (in TFLOP/s): 243.910
MLP duration (in seconds): 0.1223
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1878
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30848x23136, b=2048): 0.0518
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30848x23136, b=2048): 225.587
Elapsed time for attention_key_query_prob (64x2048x482x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x482x2048): 119.983
Elapsed time for attention_prob_times_values (64x2048x2048x482): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x482): 141.767
Elapsed time for attention_linear_projection (4x7712x30848, b=2048): 0.0164
Throughput (in TFLOP/s) for attention_linear_projection (4x7712x30848, b=2048): 238.307
Elapsed time for mlp_h_to_4h (4x30848x30848, b=2048): 0.0614
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30848x30848, b=2048): 253.820
Elapsed time for mlp_4h_to_h (4x30848x30848, b=2048): 0.0621
Throughput (in TFLOP/s) for mlp_4h_to_h (4x30848x30848, b=2048): 251.025

Attention duration (in seconds): 0.0722
Attention throughput (in TFLOP/s): 223.194
MLP duration (in seconds): 0.1235
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1957
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30976x23232, b=2048): 0.0683
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30976x23232, b=2048): 172.725
Elapsed time for attention_key_query_prob (64x2048x484x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x484x2048): 122.019
Elapsed time for attention_prob_times_values (64x2048x2048x484): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x484): 145.510
Elapsed time for attention_linear_projection (4x7744x30976, b=2048): 0.0160
Throughput (in TFLOP/s) for attention_linear_projection (4x7744x30976, b=2048): 246.082
Elapsed time for mlp_h_to_4h (4x30976x30976, b=2048): 0.0622
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30976x30976, b=2048): 252.840
Elapsed time for mlp_4h_to_h (4x30976x30976, b=2048): 0.0625
Throughput (in TFLOP/s) for mlp_4h_to_h (4x30976x30976, b=2048): 251.561

Attention duration (in seconds): 0.0881
Attention throughput (in TFLOP/s): 184.240
MLP duration (in seconds): 0.1247
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31104x23328, b=2048): 0.0576
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31104x23328, b=2048): 206.449
Elapsed time for attention_key_query_prob (64x2048x486x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x486x2048): 122.482
Elapsed time for attention_prob_times_values (64x2048x2048x486): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x486): 145.008
Elapsed time for attention_linear_projection (4x7776x31104, b=2048): 0.0166
Throughput (in TFLOP/s) for attention_linear_projection (4x7776x31104, b=2048): 239.124
Elapsed time for mlp_h_to_4h (4x31104x31104, b=2048): 0.0629
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31104x31104, b=2048): 251.968
Elapsed time for mlp_4h_to_h (4x31104x31104, b=2048): 0.0622
Throughput (in TFLOP/s) for mlp_4h_to_h (4x31104x31104, b=2048): 254.743

Attention duration (in seconds): 0.0781
Attention throughput (in TFLOP/s): 209.677
MLP duration (in seconds): 0.1251
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31232x23424, b=2048): 0.0494
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31232x23424, b=2048): 242.870
Elapsed time for attention_key_query_prob (64x2048x488x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x488x2048): 187.907
Elapsed time for attention_prob_times_values (64x2048x2048x488): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x488): 218.639
Elapsed time for attention_linear_projection (4x7808x31232, b=2048): 0.0159
Throughput (in TFLOP/s) for attention_linear_projection (4x7808x31232, b=2048): 250.998
Elapsed time for mlp_h_to_4h (4x31232x31232, b=2048): 0.0639
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31232x31232, b=2048): 250.258
Elapsed time for mlp_4h_to_h (4x31232x31232, b=2048): 0.0633
Throughput (in TFLOP/s) for mlp_4h_to_h (4x31232x31232, b=2048): 252.320

Attention duration (in seconds): 0.0679
Attention throughput (in TFLOP/s): 243.220
MLP duration (in seconds): 0.1272
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1951
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31360x23520, b=2048): 0.0501
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31360x23520, b=2048): 241.294
Elapsed time for attention_key_query_prob (64x2048x490x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x490x2048): 121.237
Elapsed time for attention_prob_times_values (64x2048x2048x490): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x490): 142.925
Elapsed time for attention_linear_projection (4x7840x31360, b=2048): 0.0167
Throughput (in TFLOP/s) for attention_linear_projection (4x7840x31360, b=2048): 241.717
Elapsed time for mlp_h_to_4h (4x31360x31360, b=2048): 0.0634
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31360x31360, b=2048): 254.153
Elapsed time for mlp_4h_to_h (4x31360x31360, b=2048): 0.0638
Throughput (in TFLOP/s) for mlp_4h_to_h (4x31360x31360, b=2048): 252.460

Attention duration (in seconds): 0.0708
Attention throughput (in TFLOP/s): 235.153
MLP duration (in seconds): 0.1272
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1980
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31488x23616, b=2048): 0.0502
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31488x23616, b=2048): 242.614
Elapsed time for attention_key_query_prob (64x2048x492x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x492x2048): 122.757
Elapsed time for attention_prob_times_values (64x2048x2048x492): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x492): 147.012
Elapsed time for attention_linear_projection (4x7872x31488, b=2048): 0.0163
Throughput (in TFLOP/s) for attention_linear_projection (4x7872x31488, b=2048): 249.352
Elapsed time for mlp_h_to_4h (4x31488x31488, b=2048): 0.0644
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31488x31488, b=2048): 252.079
Elapsed time for mlp_4h_to_h (4x31488x31488, b=2048): 0.0642
Throughput (in TFLOP/s) for mlp_4h_to_h (4x31488x31488, b=2048): 252.953

Attention duration (in seconds): 0.0705
Attention throughput (in TFLOP/s): 238.073
MLP duration (in seconds): 0.1287
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1991
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31616x23712, b=2048): 0.0508
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31616x23712, b=2048): 241.877
Elapsed time for attention_key_query_prob (64x2048x494x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x494x2048): 124.875
Elapsed time for attention_prob_times_values (64x2048x2048x494): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x494): 148.200
Elapsed time for attention_linear_projection (4x7904x31616, b=2048): 0.0168
Throughput (in TFLOP/s) for attention_linear_projection (4x7904x31616, b=2048): 243.962
Elapsed time for mlp_h_to_4h (4x31616x31616, b=2048): 0.0653
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31616x31616, b=2048): 250.961
Elapsed time for mlp_4h_to_h (4x31616x31616, b=2048): 0.0660
Throughput (in TFLOP/s) for mlp_4h_to_h (4x31616x31616, b=2048): 247.951

Attention duration (in seconds): 0.0715
Attention throughput (in TFLOP/s): 236.544
MLP duration (in seconds): 0.1313
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31744x23808, b=2048): 0.0506
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31744x23808, b=2048): 244.766
Elapsed time for attention_key_query_prob (64x2048x496x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x496x2048): 198.101
Elapsed time for attention_prob_times_values (64x2048x2048x496): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x496): 226.412
Elapsed time for attention_linear_projection (4x7936x31744, b=2048): 0.0168
Throughput (in TFLOP/s) for attention_linear_projection (4x7936x31744, b=2048): 245.610
Elapsed time for mlp_h_to_4h (4x31744x31744, b=2048): 0.0664
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31744x31744, b=2048): 248.649
Elapsed time for mlp_4h_to_h (4x31744x31744, b=2048): 0.0650
Throughput (in TFLOP/s) for mlp_4h_to_h (4x31744x31744, b=2048): 253.913

Attention duration (in seconds): 0.0699
Attention throughput (in TFLOP/s): 243.763
MLP duration (in seconds): 0.1314
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2013
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31872x23904, b=2048): 0.0515
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31872x23904, b=2048): 242.403
Elapsed time for attention_key_query_prob (64x2048x498x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x498x2048): 125.929
Elapsed time for attention_prob_times_values (64x2048x2048x498): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x498): 149.839
Elapsed time for attention_linear_projection (4x7968x31872, b=2048): 0.0172
Throughput (in TFLOP/s) for attention_linear_projection (4x7968x31872, b=2048): 242.436
Elapsed time for mlp_h_to_4h (4x31872x31872, b=2048): 0.0661
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31872x31872, b=2048): 251.781
Elapsed time for mlp_4h_to_h (4x31872x31872, b=2048): 0.0663
Throughput (in TFLOP/s) for mlp_4h_to_h (4x31872x31872, b=2048): 251.219

Attention duration (in seconds): 0.0726
Attention throughput (in TFLOP/s): 236.727
MLP duration (in seconds): 0.1324
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32000x24000, b=2048): 0.0524
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32000x24000, b=2048): 240.005
Elapsed time for attention_key_query_prob (64x2048x500x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x500x2048): 126.805
Elapsed time for attention_prob_times_values (64x2048x2048x500): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x500): 151.964
Elapsed time for attention_linear_projection (4x8000x32000, b=2048): 0.0167
Throughput (in TFLOP/s) for attention_linear_projection (4x8000x32000, b=2048): 250.697
Elapsed time for mlp_h_to_4h (4x32000x32000, b=2048): 0.1152
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32000x32000, b=2048): 145.662
Elapsed time for mlp_4h_to_h (4x32000x32000, b=2048): 0.0666
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32000x32000, b=2048): 251.870

Attention duration (in seconds): 0.0730
Attention throughput (in TFLOP/s): 237.044
MLP duration (in seconds): 0.1818
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2548
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32128x24096, b=2048): 0.0521
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32128x24096, b=2048): 243.601
Elapsed time for attention_key_query_prob (64x2048x502x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x502x2048): 126.969
Elapsed time for attention_prob_times_values (64x2048x2048x502): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x502): 152.038
Elapsed time for attention_linear_projection (4x8032x32128, b=2048): 0.0173
Throughput (in TFLOP/s) for attention_linear_projection (4x8032x32128, b=2048): 244.653
Elapsed time for mlp_h_to_4h (4x32128x32128, b=2048): 0.0676
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32128x32128, b=2048): 250.102
Elapsed time for mlp_4h_to_h (4x32128x32128, b=2048): 0.0674
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32128x32128, b=2048): 251.030

Attention duration (in seconds): 0.0732
Attention throughput (in TFLOP/s): 238.254
MLP duration (in seconds): 0.1350
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32256x24192, b=2048): 0.0524
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32256x24192, b=2048): 243.934
Elapsed time for attention_key_query_prob (64x2048x504x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x504x2048): 191.094
Elapsed time for attention_prob_times_values (64x2048x2048x504): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x504): 225.807
Elapsed time for attention_linear_projection (4x8064x32256, b=2048): 0.0171
Throughput (in TFLOP/s) for attention_linear_projection (4x8064x32256, b=2048): 249.376
Elapsed time for mlp_h_to_4h (4x32256x32256, b=2048): 0.0719
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32256x32256, b=2048): 236.955
Elapsed time for mlp_4h_to_h (4x32256x32256, b=2048): 0.0673
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32256x32256, b=2048): 253.271

Attention duration (in seconds): 0.0721
Attention throughput (in TFLOP/s): 243.885
MLP duration (in seconds): 0.1392
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32384x24288, b=2048): 0.0528
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32384x24288, b=2048): 244.137
Elapsed time for attention_key_query_prob (64x2048x506x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x506x2048): 128.776
Elapsed time for attention_prob_times_values (64x2048x2048x506): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x506): 154.517
Elapsed time for attention_linear_projection (4x8096x32384, b=2048): 0.0179
Throughput (in TFLOP/s) for attention_linear_projection (4x8096x32384, b=2048): 240.569
Elapsed time for mlp_h_to_4h (4x32384x32384, b=2048): 0.0675
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32384x32384, b=2048): 254.370
Elapsed time for mlp_4h_to_h (4x32384x32384, b=2048): 0.0675
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32384x32384, b=2048): 254.544

Attention duration (in seconds): 0.0745
Attention throughput (in TFLOP/s): 237.901
MLP duration (in seconds): 0.1351
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32512x24384, b=2048): 0.0536
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32512x24384, b=2048): 242.273
Elapsed time for attention_key_query_prob (64x2048x508x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x508x2048): 129.681
Elapsed time for attention_prob_times_values (64x2048x2048x508): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x508): 156.508
Elapsed time for attention_linear_projection (4x8128x32512, b=2048): 0.0176
Throughput (in TFLOP/s) for attention_linear_projection (4x8128x32512, b=2048): 245.646
Elapsed time for mlp_h_to_4h (4x32512x32512, b=2048): 0.0692
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32512x32512, b=2048): 250.210
Elapsed time for mlp_4h_to_h (4x32512x32512, b=2048): 0.0683
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32512x32512, b=2048): 253.453

Attention duration (in seconds): 0.0751
Attention throughput (in TFLOP/s): 237.920
MLP duration (in seconds): 0.1375
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32640x24480, b=2048): 0.0624
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32640x24480, b=2048): 209.954
Elapsed time for attention_key_query_prob (64x2048x510x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x510x2048): 129.838
Elapsed time for attention_prob_times_values (64x2048x2048x510): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x510): 155.823
Elapsed time for attention_linear_projection (4x8160x32640, b=2048): 0.0178
Throughput (in TFLOP/s) for attention_linear_projection (4x8160x32640, b=2048): 244.665
Elapsed time for mlp_h_to_4h (4x32640x32640, b=2048): 0.0692
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32640x32640, b=2048): 252.323
Elapsed time for mlp_4h_to_h (4x32640x32640, b=2048): 0.0693
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32640x32640, b=2048): 251.831

Attention duration (in seconds): 0.0841
Attention throughput (in TFLOP/s): 214.178
MLP duration (in seconds): 0.1385
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
