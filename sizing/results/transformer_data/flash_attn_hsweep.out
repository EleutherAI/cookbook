num_attention_heads: 128, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 2.284
MLP duration (in seconds): 0.0002
MLP throughput (in TFLOP/s): 10.486
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 2.911
========================================================================================================================
num_attention_heads: 128, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 4.475
MLP duration (in seconds): 0.0002
MLP throughput (in TFLOP/s): 41.845
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 6.787
========================================================================================================================
num_attention_heads: 128, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 7.554
MLP duration (in seconds): 0.0002
MLP throughput (in TFLOP/s): 91.599
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 8.661
========================================================================================================================
num_attention_heads: 128, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 7.772
MLP duration (in seconds): 0.0003
MLP throughput (in TFLOP/s): 117.072
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 10.919
========================================================================================================================
num_attention_heads: 128, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 9.062
MLP duration (in seconds): 0.0004
MLP throughput (in TFLOP/s): 143.793
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 12.381
========================================================================================================================
num_attention_heads: 128, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 12.580
MLP duration (in seconds): 0.0005
MLP throughput (in TFLOP/s): 152.306
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 16.088
========================================================================================================================
num_attention_heads: 128, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 14.659
MLP duration (in seconds): 0.0006
MLP throughput (in TFLOP/s): 164.011
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 22.077
========================================================================================================================
num_attention_heads: 128, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 21.929
MLP duration (in seconds): 0.0008
MLP throughput (in TFLOP/s): 173.685
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 28.519
========================================================================================================================
num_attention_heads: 128, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 17.581
MLP duration (in seconds): 0.0010
MLP throughput (in TFLOP/s): 174.876
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 25.265
========================================================================================================================
num_attention_heads: 128, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 18.698
MLP duration (in seconds): 0.0012
MLP throughput (in TFLOP/s): 180.108
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 28.162
========================================================================================================================
num_attention_heads: 128, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 21.111
MLP duration (in seconds): 0.0014
MLP throughput (in TFLOP/s): 182.314
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 41.629
========================================================================================================================
num_attention_heads: 128, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 24.935
MLP duration (in seconds): 0.0017
MLP throughput (in TFLOP/s): 184.789
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 39.029
========================================================================================================================
num_attention_heads: 128, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 29.131
MLP duration (in seconds): 0.0019
MLP throughput (in TFLOP/s): 192.296
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 47.706
========================================================================================================================
num_attention_heads: 128, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 32.456
MLP duration (in seconds): 0.0022
MLP throughput (in TFLOP/s): 192.815
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 51.322
========================================================================================================================
num_attention_heads: 128, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 29.996
MLP duration (in seconds): 0.0025
MLP throughput (in TFLOP/s): 197.084
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 53.776
========================================================================================================================
num_attention_heads: 128, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 48.764
MLP duration (in seconds): 0.0027
MLP throughput (in TFLOP/s): 204.636
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 84.025
========================================================================================================================
num_attention_heads: 128, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 35.981
MLP duration (in seconds): 0.0031
MLP throughput (in TFLOP/s): 198.799
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 62.876
========================================================================================================================
num_attention_heads: 128, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 43.233
MLP duration (in seconds): 0.0034
MLP throughput (in TFLOP/s): 205.386
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 70.144
========================================================================================================================
num_attention_heads: 128, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 43.421
MLP duration (in seconds): 0.0038
MLP throughput (in TFLOP/s): 206.346
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 89.658
========================================================================================================================
num_attention_heads: 128, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 47.375
MLP duration (in seconds): 0.0042
MLP throughput (in TFLOP/s): 206.268
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 78.098
========================================================================================================================
num_attention_heads: 128, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 51.562
MLP duration (in seconds): 0.0046
MLP throughput (in TFLOP/s): 206.325
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 100.957
========================================================================================================================
num_attention_heads: 128, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 55.668
MLP duration (in seconds): 0.0050
MLP throughput (in TFLOP/s): 206.915
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 109.621
========================================================================================================================
num_attention_heads: 128, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 65.485
MLP duration (in seconds): 0.0053
MLP throughput (in TFLOP/s): 212.572
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 99.847
========================================================================================================================
num_attention_heads: 128, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 86.536
MLP duration (in seconds): 0.0060
MLP throughput (in TFLOP/s): 207.742
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 123.297
========================================================================================================================
num_attention_heads: 128, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 55.906
MLP duration (in seconds): 0.0063
MLP throughput (in TFLOP/s): 212.908
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 101.052
========================================================================================================================
num_attention_heads: 128, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 57.401
MLP duration (in seconds): 0.0067
MLP throughput (in TFLOP/s): 215.246
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 114.383
========================================================================================================================
num_attention_heads: 128, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 59.915
MLP duration (in seconds): 0.0072
MLP throughput (in TFLOP/s): 217.267
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 127.672
========================================================================================================================
num_attention_heads: 128, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 60.671
MLP duration (in seconds): 0.0079
MLP throughput (in TFLOP/s): 214.399
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 130.618
========================================================================================================================
num_attention_heads: 128, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 57.644
MLP duration (in seconds): 0.0083
MLP throughput (in TFLOP/s): 216.703
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 118.715
========================================================================================================================
num_attention_heads: 128, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 57.867
MLP duration (in seconds): 0.0088
MLP throughput (in TFLOP/s): 220.159
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 128.237
========================================================================================================================
num_attention_heads: 128, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 58.891
MLP duration (in seconds): 0.0095
MLP throughput (in TFLOP/s): 216.756
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 132.414
========================================================================================================================
num_attention_heads: 128, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 87.027
MLP duration (in seconds): 0.0100
MLP throughput (in TFLOP/s): 219.239
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 177.869
========================================================================================================================
num_attention_heads: 128, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 53.132
MLP duration (in seconds): 0.0103
MLP throughput (in TFLOP/s): 227.636
Transformer duration (in seconds): 0.0359
Transformer throughput (in TFLOP/s): 105.750
========================================================================================================================
num_attention_heads: 128, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 55.946
MLP duration (in seconds): 0.0111
MLP throughput (in TFLOP/s): 223.767
Transformer duration (in seconds): 0.0349
Transformer throughput (in TFLOP/s): 115.117
========================================================================================================================
num_attention_heads: 128, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 58.880
MLP duration (in seconds): 0.0116
MLP throughput (in TFLOP/s): 227.619
Transformer duration (in seconds): 0.0286
Transformer throughput (in TFLOP/s): 148.321
========================================================================================================================
num_attention_heads: 128, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 62.003
MLP duration (in seconds): 0.0120
MLP throughput (in TFLOP/s): 231.715
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 169.967
========================================================================================================================
num_attention_heads: 128, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 65.391
MLP duration (in seconds): 0.0126
MLP throughput (in TFLOP/s): 232.526
Transformer duration (in seconds): 0.0293
Transformer throughput (in TFLOP/s): 161.507
========================================================================================================================
num_attention_heads: 128, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 68.679
MLP duration (in seconds): 0.0136
MLP throughput (in TFLOP/s): 228.262
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 162.646
========================================================================================================================
num_attention_heads: 128, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 74.183
MLP duration (in seconds): 0.0141
MLP throughput (in TFLOP/s): 231.716
Transformer duration (in seconds): 0.0308
Transformer throughput (in TFLOP/s): 170.119
========================================================================================================================
num_attention_heads: 128, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 108.431
MLP duration (in seconds): 0.0148
MLP throughput (in TFLOP/s): 232.553
Transformer duration (in seconds): 0.0294
Transformer throughput (in TFLOP/s): 187.280
========================================================================================================================
num_attention_heads: 128, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 76.258
MLP duration (in seconds): 0.0157
MLP throughput (in TFLOP/s): 230.258
Transformer duration (in seconds): 0.0332
Transformer throughput (in TFLOP/s): 173.944
========================================================================================================================
num_attention_heads: 128, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 73.790
MLP duration (in seconds): 0.0164
MLP throughput (in TFLOP/s): 231.048
Transformer duration (in seconds): 0.0331
Transformer throughput (in TFLOP/s): 182.447
========================================================================================================================
num_attention_heads: 128, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 82.802
MLP duration (in seconds): 0.0170
MLP throughput (in TFLOP/s): 234.109
Transformer duration (in seconds): 0.0352
Transformer throughput (in TFLOP/s): 179.646
========================================================================================================================
num_attention_heads: 128, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0317
Attention throughput (in TFLOP/s): 77.509
MLP duration (in seconds): 0.0180
MLP throughput (in TFLOP/s): 230.658
Transformer duration (in seconds): 0.0470
Transformer throughput (in TFLOP/s): 140.859
========================================================================================================================
num_attention_heads: 128, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0317
Attention throughput (in TFLOP/s): 80.713
MLP duration (in seconds): 0.0185
MLP throughput (in TFLOP/s): 234.584
Transformer duration (in seconds): 0.0369
Transformer throughput (in TFLOP/s): 187.347
========================================================================================================================
num_attention_heads: 128, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0322
Attention throughput (in TFLOP/s): 82.901
MLP duration (in seconds): 0.0193
MLP throughput (in TFLOP/s): 235.413
Transformer duration (in seconds): 0.0402
Transformer throughput (in TFLOP/s): 179.393
========================================================================================================================
num_attention_heads: 128, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0316
Attention throughput (in TFLOP/s): 87.873
MLP duration (in seconds): 0.0204
MLP throughput (in TFLOP/s): 233.053
Transformer duration (in seconds): 0.0419
Transformer throughput (in TFLOP/s): 179.560
========================================================================================================================
num_attention_heads: 128, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 102.800
MLP duration (in seconds): 0.0209
MLP throughput (in TFLOP/s): 236.548
Transformer duration (in seconds): 0.0401
Transformer throughput (in TFLOP/s): 195.464
========================================================================================================================
num_attention_heads: 128, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0347
Attention throughput (in TFLOP/s): 86.496
MLP duration (in seconds): 0.0224
MLP throughput (in TFLOP/s): 230.663
Transformer duration (in seconds): 0.0446
Transformer throughput (in TFLOP/s): 182.785
========================================================================================================================
num_attention_heads: 128, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0369
Attention throughput (in TFLOP/s): 84.454
MLP duration (in seconds): 0.0227
MLP throughput (in TFLOP/s): 236.246
Transformer duration (in seconds): 0.0441
Transformer throughput (in TFLOP/s): 192.241
========================================================================================================================
num_attention_heads: 128, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0380
Attention throughput (in TFLOP/s): 85.036
MLP duration (in seconds): 0.0240
MLP throughput (in TFLOP/s): 232.942
Transformer duration (in seconds): 0.0571
Transformer throughput (in TFLOP/s): 154.415
========================================================================================================================
num_attention_heads: 128, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0373
Attention throughput (in TFLOP/s): 89.886
MLP duration (in seconds): 0.0245
MLP throughput (in TFLOP/s): 237.221
Transformer duration (in seconds): 0.0535
Transformer throughput (in TFLOP/s): 171.170
========================================================================================================================
num_attention_heads: 128, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0399
Attention throughput (in TFLOP/s): 86.959
MLP duration (in seconds): 0.0255
MLP throughput (in TFLOP/s): 236.699
Transformer duration (in seconds): 0.0502
Transformer throughput (in TFLOP/s): 189.150
========================================================================================================================
num_attention_heads: 128, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0399
Attention throughput (in TFLOP/s): 90.081
MLP duration (in seconds): 0.0262
MLP throughput (in TFLOP/s): 239.051
Transformer duration (in seconds): 0.0570
Transformer throughput (in TFLOP/s): 172.962
========================================================================================================================
num_attention_heads: 128, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0400
Attention throughput (in TFLOP/s): 93.014
MLP duration (in seconds): 0.0279
MLP throughput (in TFLOP/s): 232.598
Transformer duration (in seconds): 0.0544
Transformer throughput (in TFLOP/s): 187.762
========================================================================================================================
num_attention_heads: 128, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0298
Attention throughput (in TFLOP/s): 129.173
MLP duration (in seconds): 0.0285
MLP throughput (in TFLOP/s): 236.484
Transformer duration (in seconds): 0.0522
Transformer throughput (in TFLOP/s): 202.755
========================================================================================================================
num_attention_heads: 128, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 135.925
MLP duration (in seconds): 0.0302
MLP throughput (in TFLOP/s): 231.118
Transformer duration (in seconds): 0.0567
Transformer throughput (in TFLOP/s): 193.048
========================================================================================================================
num_attention_heads: 128, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0442
Attention throughput (in TFLOP/s): 93.060
MLP duration (in seconds): 0.0307
MLP throughput (in TFLOP/s): 234.949
Transformer duration (in seconds): 0.0572
Transformer throughput (in TFLOP/s): 198.126
========================================================================================================================
num_attention_heads: 128, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0450
Attention throughput (in TFLOP/s): 94.230
MLP duration (in seconds): 0.0323
MLP throughput (in TFLOP/s): 231.332
Transformer duration (in seconds): 0.0604
Transformer throughput (in TFLOP/s): 194.152
========================================================================================================================
num_attention_heads: 128, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0433
Attention throughput (in TFLOP/s): 101.104
MLP duration (in seconds): 0.0331
MLP throughput (in TFLOP/s): 233.293
Transformer duration (in seconds): 0.0611
Transformer throughput (in TFLOP/s): 198.242
========================================================================================================================
num_attention_heads: 128, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0462
Attention throughput (in TFLOP/s): 97.724
MLP duration (in seconds): 0.0344
MLP throughput (in TFLOP/s): 232.278
Transformer duration (in seconds): 0.0660
Transformer throughput (in TFLOP/s): 189.582
========================================================================================================================
num_attention_heads: 128, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0469
Attention throughput (in TFLOP/s): 99.262
MLP duration (in seconds): 0.0355
MLP throughput (in TFLOP/s): 232.570
Transformer duration (in seconds): 0.0656
Transformer throughput (in TFLOP/s): 196.869
========================================================================================================================
num_attention_heads: 128, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0463
Attention throughput (in TFLOP/s): 103.814
MLP duration (in seconds): 0.0365
MLP throughput (in TFLOP/s): 233.694
Transformer duration (in seconds): 0.0668
Transformer throughput (in TFLOP/s): 199.557
========================================================================================================================
num_attention_heads: 128, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0364
Attention throughput (in TFLOP/s): 136.071
MLP duration (in seconds): 0.0378
MLP throughput (in TFLOP/s): 232.479
Transformer duration (in seconds): 0.0665
Transformer throughput (in TFLOP/s): 206.700
========================================================================================================================
num_attention_heads: 128, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0290
Attention throughput (in TFLOP/s): 175.766
MLP duration (in seconds): 0.0393
MLP throughput (in TFLOP/s): 230.922
Transformer duration (in seconds): 0.0719
Transformer throughput (in TFLOP/s): 197.165
========================================================================================================================
num_attention_heads: 128, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0486
Attention throughput (in TFLOP/s): 107.996
MLP duration (in seconds): 0.0404
MLP throughput (in TFLOP/s): 231.514
Transformer duration (in seconds): 0.0722
Transformer throughput (in TFLOP/s): 202.265
========================================================================================================================
num_attention_heads: 128, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0484
Attention throughput (in TFLOP/s): 111.385
MLP duration (in seconds): 0.0416
MLP throughput (in TFLOP/s): 231.493
Transformer duration (in seconds): 0.0742
Transformer throughput (in TFLOP/s): 202.657
========================================================================================================================
num_attention_heads: 128, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0410
Attention throughput (in TFLOP/s): 135.258
MLP duration (in seconds): 0.0426
MLP throughput (in TFLOP/s): 233.299
Transformer duration (in seconds): 0.0764
Transformer throughput (in TFLOP/s): 202.694
========================================================================================================================
num_attention_heads: 128, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0515
Attention throughput (in TFLOP/s): 110.745
MLP duration (in seconds): 0.0439
MLP throughput (in TFLOP/s): 232.809
Transformer duration (in seconds): 0.0796
Transformer throughput (in TFLOP/s): 200.042
========================================================================================================================
num_attention_heads: 128, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0519
Attention throughput (in TFLOP/s): 113.044
MLP duration (in seconds): 0.0452
MLP throughput (in TFLOP/s): 232.916
Transformer duration (in seconds): 0.0813
Transformer throughput (in TFLOP/s): 201.607
========================================================================================================================
num_attention_heads: 128, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0519
Attention throughput (in TFLOP/s): 116.095
MLP duration (in seconds): 0.0455
MLP throughput (in TFLOP/s): 238.108
Transformer duration (in seconds): 0.0816
Transformer throughput (in TFLOP/s): 206.415
========================================================================================================================
num_attention_heads: 128, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0399
Attention throughput (in TFLOP/s): 154.864
MLP duration (in seconds): 0.0477
MLP throughput (in TFLOP/s): 233.301
Transformer duration (in seconds): 0.0818
Transformer throughput (in TFLOP/s): 211.794
========================================================================================================================
num_attention_heads: 128, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0348
Attention throughput (in TFLOP/s): 182.538
MLP duration (in seconds): 0.0490
MLP throughput (in TFLOP/s): 233.628
Transformer duration (in seconds): 0.0860
Transformer throughput (in TFLOP/s): 206.846
========================================================================================================================
num_attention_heads: 128, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0558
Attention throughput (in TFLOP/s): 116.813
MLP duration (in seconds): 0.0511
MLP throughput (in TFLOP/s): 230.062
Transformer duration (in seconds): 0.0902
Transformer throughput (in TFLOP/s): 202.605
========================================================================================================================
num_attention_heads: 128, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0556
Attention throughput (in TFLOP/s): 120.274
MLP duration (in seconds): 0.0527
MLP throughput (in TFLOP/s): 229.383
Transformer duration (in seconds): 0.0923
Transformer throughput (in TFLOP/s): 203.385
========================================================================================================================
num_attention_heads: 128, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0543
Attention throughput (in TFLOP/s): 126.350
MLP duration (in seconds): 0.0539
MLP throughput (in TFLOP/s): 230.255
Transformer duration (in seconds): 0.0941
Transformer throughput (in TFLOP/s): 204.709
========================================================================================================================
num_attention_heads: 128, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0564
Attention throughput (in TFLOP/s): 124.578
MLP duration (in seconds): 0.0555
MLP throughput (in TFLOP/s): 229.574
Transformer duration (in seconds): 0.0969
Transformer throughput (in TFLOP/s): 203.988
========================================================================================================================
num_attention_heads: 128, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0578
Attention throughput (in TFLOP/s): 124.605
MLP duration (in seconds): 0.0569
MLP throughput (in TFLOP/s): 229.656
Transformer duration (in seconds): 0.0994
Transformer throughput (in TFLOP/s): 203.983
========================================================================================================================
num_attention_heads: 128, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0587
Attention throughput (in TFLOP/s): 125.715
MLP duration (in seconds): 0.0585
MLP throughput (in TFLOP/s): 228.941
Transformer duration (in seconds): 0.1013
Transformer throughput (in TFLOP/s): 205.133
========================================================================================================================
num_attention_heads: 128, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0364
Attention throughput (in TFLOP/s): 207.697
MLP duration (in seconds): 0.0600
MLP throughput (in TFLOP/s): 229.028
Transformer duration (in seconds): 0.1011
Transformer throughput (in TFLOP/s): 210.662
========================================================================================================================
num_attention_heads: 128, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0559
Attention throughput (in TFLOP/s): 138.583
MLP duration (in seconds): 0.0613
MLP throughput (in TFLOP/s): 230.005
Transformer duration (in seconds): 0.1062
Transformer throughput (in TFLOP/s): 205.560
========================================================================================================================
num_attention_heads: 128, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0603
Attention throughput (in TFLOP/s): 131.393
MLP duration (in seconds): 0.0628
MLP throughput (in TFLOP/s): 229.922
Transformer duration (in seconds): 0.1085
Transformer throughput (in TFLOP/s): 206.036
========================================================================================================================
num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0603
Attention throughput (in TFLOP/s): 134.546
MLP duration (in seconds): 0.0643
MLP throughput (in TFLOP/s): 229.983
Transformer duration (in seconds): 0.1110
Transformer throughput (in TFLOP/s): 206.384
========================================================================================================================
num_attention_heads: 128, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0405
Attention throughput (in TFLOP/s): 204.912
MLP duration (in seconds): 0.0660
MLP throughput (in TFLOP/s): 229.417
Transformer duration (in seconds): 0.1115
Transformer throughput (in TFLOP/s): 210.316
========================================================================================================================
num_attention_heads: 128, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0409
Attention throughput (in TFLOP/s): 207.377
MLP duration (in seconds): 0.0676
MLP throughput (in TFLOP/s): 229.409
Transformer duration (in seconds): 0.1138
Transformer throughput (in TFLOP/s): 210.851
========================================================================================================================
num_attention_heads: 128, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0421
Attention throughput (in TFLOP/s): 206.038
MLP duration (in seconds): 0.0692
MLP throughput (in TFLOP/s): 229.567
Transformer duration (in seconds): 0.1166
Transformer throughput (in TFLOP/s): 210.595
========================================================================================================================
num_attention_heads: 128, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0443
Attention throughput (in TFLOP/s): 200.530
MLP duration (in seconds): 0.0708
MLP throughput (in TFLOP/s): 229.581
Transformer duration (in seconds): 0.1201
Transformer throughput (in TFLOP/s): 209.218
========================================================================================================================
num_attention_heads: 128, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0424
Attention throughput (in TFLOP/s): 213.969
MLP duration (in seconds): 0.0727
MLP throughput (in TFLOP/s): 228.784
Transformer duration (in seconds): 0.1204
Transformer throughput (in TFLOP/s): 213.400
========================================================================================================================
num_attention_heads: 128, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0471
Attention throughput (in TFLOP/s): 196.649
MLP duration (in seconds): 0.0721
MLP throughput (in TFLOP/s): 235.855
Transformer duration (in seconds): 0.1231
Transformer throughput (in TFLOP/s): 213.458
========================================================================================================================
num_attention_heads: 128, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0475
Attention throughput (in TFLOP/s): 199.276
MLP duration (in seconds): 0.0755
MLP throughput (in TFLOP/s): 230.330
Transformer duration (in seconds): 0.1274
Transformer throughput (in TFLOP/s): 210.860
========================================================================================================================
num_attention_heads: 128, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0488
Attention throughput (in TFLOP/s): 198.256
MLP duration (in seconds): 0.0773
MLP throughput (in TFLOP/s): 230.190
Transformer duration (in seconds): 0.1301
Transformer throughput (in TFLOP/s): 211.044
========================================================================================================================
num_attention_heads: 128, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0485
Attention throughput (in TFLOP/s): 203.657
MLP duration (in seconds): 0.0791
MLP throughput (in TFLOP/s): 229.784
Transformer duration (in seconds): 0.1327
Transformer throughput (in TFLOP/s): 211.368
========================================================================================================================
num_attention_heads: 128, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0495
Attention throughput (in TFLOP/s): 203.877
MLP duration (in seconds): 0.0812
MLP throughput (in TFLOP/s): 228.747
Transformer duration (in seconds): 0.1355
Transformer throughput (in TFLOP/s): 211.482
========================================================================================================================
num_attention_heads: 128, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0500
Attention throughput (in TFLOP/s): 205.758
MLP duration (in seconds): 0.0815
MLP throughput (in TFLOP/s): 232.683
Transformer duration (in seconds): 0.1368
Transformer throughput (in TFLOP/s): 213.957
========================================================================================================================
num_attention_heads: 128, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0518
Attention throughput (in TFLOP/s): 202.770
MLP duration (in seconds): 0.0831
MLP throughput (in TFLOP/s): 233.142
Transformer duration (in seconds): 0.1400
Transformer throughput (in TFLOP/s): 213.509
========================================================================================================================
num_attention_heads: 128, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0495
Attention throughput (in TFLOP/s): 216.498
MLP duration (in seconds): 0.0851
MLP throughput (in TFLOP/s): 232.529
Transformer duration (in seconds): 0.1400
Transformer throughput (in TFLOP/s): 217.868
========================================================================================================================
num_attention_heads: 128, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0550
Attention throughput (in TFLOP/s): 198.674
MLP duration (in seconds): 0.0870
MLP throughput (in TFLOP/s): 232.226
Transformer duration (in seconds): 0.1465
Transformer throughput (in TFLOP/s): 212.555
========================================================================================================================
num_attention_heads: 128, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0554
Attention throughput (in TFLOP/s): 201.374
MLP duration (in seconds): 0.0868
MLP throughput (in TFLOP/s): 237.645
Transformer duration (in seconds): 0.1461
Transformer throughput (in TFLOP/s): 217.520
========================================================================================================================
num_attention_heads: 128, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0559
Attention throughput (in TFLOP/s): 203.491
MLP duration (in seconds): 0.0885
MLP throughput (in TFLOP/s): 237.902
Transformer duration (in seconds): 0.1500
Transformer throughput (in TFLOP/s): 216.133
========================================================================================================================
num_attention_heads: 128, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0565
Attention throughput (in TFLOP/s): 205.090
MLP duration (in seconds): 0.0898
MLP throughput (in TFLOP/s): 239.105
Transformer duration (in seconds): 0.1527
Transformer throughput (in TFLOP/s): 216.643
========================================================================================================================
num_attention_heads: 128, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0580
Attention throughput (in TFLOP/s): 203.962
MLP duration (in seconds): 0.0920
MLP throughput (in TFLOP/s): 238.086
Transformer duration (in seconds): 0.1555
Transformer throughput (in TFLOP/s): 216.855
========================================================================================================================
num_attention_heads: 128, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0600
Attention throughput (in TFLOP/s): 200.739
MLP duration (in seconds): 0.0936
MLP throughput (in TFLOP/s): 238.666
Transformer duration (in seconds): 0.1588
Transformer throughput (in TFLOP/s): 216.535
========================================================================================================================
num_attention_heads: 128, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0611
Attention throughput (in TFLOP/s): 200.797
MLP duration (in seconds): 0.0964
MLP throughput (in TFLOP/s): 236.420
Transformer duration (in seconds): 0.1616
Transformer throughput (in TFLOP/s): 216.992
========================================================================================================================
num_attention_heads: 128, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0581
Attention throughput (in TFLOP/s): 215.284
MLP duration (in seconds): 0.0974
MLP throughput (in TFLOP/s): 238.379
Transformer duration (in seconds): 0.1613
Transformer throughput (in TFLOP/s): 221.590
========================================================================================================================
num_attention_heads: 128, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0619
Attention throughput (in TFLOP/s): 205.807
MLP duration (in seconds): 0.0993
MLP throughput (in TFLOP/s): 238.523
Transformer duration (in seconds): 0.1667
Transformer throughput (in TFLOP/s): 218.504
========================================================================================================================
num_attention_heads: 128, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0643
Attention throughput (in TFLOP/s): 201.874
MLP duration (in seconds): 0.0999
MLP throughput (in TFLOP/s): 241.612
Transformer duration (in seconds): 0.1707
Transformer throughput (in TFLOP/s): 217.382
========================================================================================================================
num_attention_heads: 128, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0649
Attention throughput (in TFLOP/s): 203.645
MLP duration (in seconds): 0.1031
MLP throughput (in TFLOP/s): 238.560
Transformer duration (in seconds): 0.1737
Transformer throughput (in TFLOP/s): 217.585
========================================================================================================================
num_attention_heads: 128, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0730
Attention throughput (in TFLOP/s): 184.167
MLP duration (in seconds): 0.1048
MLP throughput (in TFLOP/s): 239.069
Transformer duration (in seconds): 0.1766
Transformer throughput (in TFLOP/s): 218.054
========================================================================================================================
num_attention_heads: 128, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0759
Attention throughput (in TFLOP/s): 180.428
MLP duration (in seconds): 0.1072
MLP throughput (in TFLOP/s): 238.056
Transformer duration (in seconds): 0.1803
Transformer throughput (in TFLOP/s): 217.466
========================================================================================================================
num_attention_heads: 128, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0746
Attention throughput (in TFLOP/s): 186.840
MLP duration (in seconds): 0.1091
MLP throughput (in TFLOP/s): 238.280
Transformer duration (in seconds): 0.1832
Transformer throughput (in TFLOP/s): 217.896
========================================================================================================================
num_attention_heads: 128, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0679
Attention throughput (in TFLOP/s): 209.021
MLP duration (in seconds): 0.1110
MLP throughput (in TFLOP/s): 238.361
Transformer duration (in seconds): 0.1848
Transformer throughput (in TFLOP/s): 219.886
========================================================================================================================
num_attention_heads: 128, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0660
Attention throughput (in TFLOP/s): 218.766
MLP duration (in seconds): 0.1138
MLP throughput (in TFLOP/s): 236.690
Transformer duration (in seconds): 0.1852
Transformer throughput (in TFLOP/s): 223.373
========================================================================================================================
num_attention_heads: 128, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0802
Attention throughput (in TFLOP/s): 183.120
MLP duration (in seconds): 0.1150
MLP throughput (in TFLOP/s): 238.376
Transformer duration (in seconds): 0.1926
Transformer throughput (in TFLOP/s): 218.635
========================================================================================================================
num_attention_heads: 128, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0714
Attention throughput (in TFLOP/s): 209.065
MLP duration (in seconds): 0.1170
MLP throughput (in TFLOP/s): 238.575
Transformer duration (in seconds): 0.1949
Transformer throughput (in TFLOP/s): 219.849
========================================================================================================================
num_attention_heads: 128, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0721
Attention throughput (in TFLOP/s): 210.548
MLP duration (in seconds): 0.1191
MLP throughput (in TFLOP/s): 238.428
Transformer duration (in seconds): 0.1974
Transformer throughput (in TFLOP/s): 220.795
========================================================================================================================
num_attention_heads: 128, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0734
Attention throughput (in TFLOP/s): 210.552
MLP duration (in seconds): 0.1212
MLP throughput (in TFLOP/s): 238.510
Transformer duration (in seconds): 0.2005
Transformer throughput (in TFLOP/s): 221.107
========================================================================================================================
num_attention_heads: 128, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0752
Attention throughput (in TFLOP/s): 208.833
MLP duration (in seconds): 0.1234
MLP throughput (in TFLOP/s): 238.159
Transformer duration (in seconds): 0.2039
Transformer throughput (in TFLOP/s): 221.210
========================================================================================================================
num_attention_heads: 128, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0737
Attention throughput (in TFLOP/s): 216.612
MLP duration (in seconds): 0.1258
MLP throughput (in TFLOP/s): 237.700
Transformer duration (in seconds): 0.2050
Transformer throughput (in TFLOP/s): 223.698
========================================================================================================================
num_attention_heads: 128, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0751
Attention throughput (in TFLOP/s): 215.950
MLP duration (in seconds): 0.1283
MLP throughput (in TFLOP/s): 237.084
Transformer duration (in seconds): 0.2091
Transformer throughput (in TFLOP/s): 223.045
========================================================================================================================
num_attention_heads: 128, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0725
Attention throughput (in TFLOP/s): 227.633
MLP duration (in seconds): 0.1299
MLP throughput (in TFLOP/s): 238.035
Transformer duration (in seconds): 0.2073
Transformer throughput (in TFLOP/s): 228.776
========================================================================================================================
num_attention_heads: 128, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0789
Attention throughput (in TFLOP/s): 212.347
MLP duration (in seconds): 0.1325
MLP throughput (in TFLOP/s): 237.340
Transformer duration (in seconds): 0.2172
Transformer throughput (in TFLOP/s): 221.909
========================================================================================================================
num_attention_heads: 128, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0811
Attention throughput (in TFLOP/s): 210.060
MLP duration (in seconds): 0.1350
MLP throughput (in TFLOP/s): 236.721
Transformer duration (in seconds): 0.2228
Transformer throughput (in TFLOP/s): 219.915
========================================================================================================================
num_attention_heads: 128, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0816
Attention throughput (in TFLOP/s): 212.096
MLP duration (in seconds): 0.1366
MLP throughput (in TFLOP/s): 237.912
Transformer duration (in seconds): 0.2249
Transformer throughput (in TFLOP/s): 221.411
========================================================================================================================
num_attention_heads: 128, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0833
Attention throughput (in TFLOP/s): 210.949
MLP duration (in seconds): 0.1387
MLP throughput (in TFLOP/s): 238.041
Transformer duration (in seconds): 0.2293
Transformer throughput (in TFLOP/s): 220.621
========================================================================================================================
num_attention_heads: 128, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0852
Attention throughput (in TFLOP/s): 209.620
MLP duration (in seconds): 0.1418
MLP throughput (in TFLOP/s): 236.583
Transformer duration (in seconds): 0.2332
Transformer throughput (in TFLOP/s): 220.419
========================================================================================================================
num_attention_heads: 128, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0860
Attention throughput (in TFLOP/s): 210.866
MLP duration (in seconds): 0.1433
MLP throughput (in TFLOP/s): 237.884
Transformer duration (in seconds): 0.2362
Transformer throughput (in TFLOP/s): 221.105
========================================================================================================================
num_attention_heads: 128, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0870
Attention throughput (in TFLOP/s): 211.550
MLP duration (in seconds): 0.1455
MLP throughput (in TFLOP/s): 238.092
Transformer duration (in seconds): 0.2392
Transformer throughput (in TFLOP/s): 221.735
========================================================================================================================
num_attention_heads: 128, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0840
Attention throughput (in TFLOP/s): 222.462
MLP duration (in seconds): 0.1481
MLP throughput (in TFLOP/s): 237.526
Transformer duration (in seconds): 0.2377
Transformer throughput (in TFLOP/s): 226.655
========================================================================================================================
num_attention_heads: 128, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0914
Attention throughput (in TFLOP/s): 207.721
MLP duration (in seconds): 0.1508
MLP throughput (in TFLOP/s): 237.024
Transformer duration (in seconds): 0.2486
Transformer throughput (in TFLOP/s): 220.101
========================================================================================================================
num_attention_heads: 128, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0887
Attention throughput (in TFLOP/s): 217.216
MLP duration (in seconds): 0.1528
MLP throughput (in TFLOP/s): 237.532
Transformer duration (in seconds): 0.2481
Transformer throughput (in TFLOP/s): 223.956
========================================================================================================================
num_attention_heads: 128, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0897
Attention throughput (in TFLOP/s): 217.869
MLP duration (in seconds): 0.1557
MLP throughput (in TFLOP/s): 236.653
Transformer duration (in seconds): 0.2514
Transformer throughput (in TFLOP/s): 224.358
========================================================================================================================
num_attention_heads: 128, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0908
Attention throughput (in TFLOP/s): 218.516
MLP duration (in seconds): 0.1573
MLP throughput (in TFLOP/s): 237.922
Transformer duration (in seconds): 0.2556
Transformer throughput (in TFLOP/s): 224.062
========================================================================================================================
num_attention_heads: 128, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0926
Attention throughput (in TFLOP/s): 217.433
MLP duration (in seconds): 0.1606
MLP throughput (in TFLOP/s): 236.553
Transformer duration (in seconds): 0.2598
Transformer throughput (in TFLOP/s): 223.743
========================================================================================================================
num_attention_heads: 128, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0940
Attention throughput (in TFLOP/s): 217.377
MLP duration (in seconds): 0.1630
MLP throughput (in TFLOP/s): 236.566
Transformer duration (in seconds): 0.2642
Transformer throughput (in TFLOP/s): 223.257
========================================================================================================================
num_attention_heads: 128, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0951
Attention throughput (in TFLOP/s): 218.061
MLP duration (in seconds): 0.1644
MLP throughput (in TFLOP/s): 238.035
Transformer duration (in seconds): 0.2673
Transformer throughput (in TFLOP/s): 223.991
========================================================================================================================
num_attention_heads: 128, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0922
Attention throughput (in TFLOP/s): 228.009
MLP duration (in seconds): 0.1674
MLP throughput (in TFLOP/s): 237.272
Transformer duration (in seconds): 0.2686
Transformer throughput (in TFLOP/s): 226.161
========================================================================================================================
num_attention_heads: 128, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1003
Attention throughput (in TFLOP/s): 212.624
MLP duration (in seconds): 0.1698
MLP throughput (in TFLOP/s): 237.318
Transformer duration (in seconds): 0.2789
Transformer throughput (in TFLOP/s): 221.036
========================================================================================================================
num_attention_heads: 128, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1019
Attention throughput (in TFLOP/s): 212.259
MLP duration (in seconds): 0.1726
MLP throughput (in TFLOP/s): 236.949
Transformer duration (in seconds): 0.2809
Transformer throughput (in TFLOP/s): 222.582
========================================================================================================================
num_attention_heads: 128, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1033
Attention throughput (in TFLOP/s): 212.389
MLP duration (in seconds): 0.1753
MLP throughput (in TFLOP/s): 236.710
Transformer duration (in seconds): 0.2863
Transformer throughput (in TFLOP/s): 221.576
========================================================================================================================
num_attention_heads: 128, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1044
Attention throughput (in TFLOP/s): 213.060
MLP duration (in seconds): 0.1776
MLP throughput (in TFLOP/s): 237.026
Transformer duration (in seconds): 0.2906
Transformer throughput (in TFLOP/s): 221.364
========================================================================================================================
num_attention_heads: 128, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1059
Attention throughput (in TFLOP/s): 212.920
MLP duration (in seconds): 0.1800
MLP throughput (in TFLOP/s): 237.217
Transformer duration (in seconds): 0.2941
Transformer throughput (in TFLOP/s): 221.891
========================================================================================================================
num_attention_heads: 128, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1073
Attention throughput (in TFLOP/s): 213.217
MLP duration (in seconds): 0.1835
MLP throughput (in TFLOP/s): 235.992
Transformer duration (in seconds): 0.2991
Transformer throughput (in TFLOP/s): 221.261
========================================================================================================================
num_attention_heads: 128, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1085
Attention throughput (in TFLOP/s): 213.613
MLP duration (in seconds): 0.1851
MLP throughput (in TFLOP/s): 237.254
Transformer duration (in seconds): 0.3035
Transformer throughput (in TFLOP/s): 221.068
========================================================================================================================
num_attention_heads: 128, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1056
Attention throughput (in TFLOP/s): 222.458
MLP duration (in seconds): 0.1878
MLP throughput (in TFLOP/s): 237.065
Transformer duration (in seconds): 0.3019
Transformer throughput (in TFLOP/s): 225.313
========================================================================================================================
num_attention_heads: 128, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1112
Attention throughput (in TFLOP/s): 214.294
MLP duration (in seconds): 0.1910
MLP throughput (in TFLOP/s): 236.404
Transformer duration (in seconds): 0.3100
Transformer throughput (in TFLOP/s): 222.454
========================================================================================================================
num_attention_heads: 128, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1128
Attention throughput (in TFLOP/s): 213.993
MLP duration (in seconds): 0.1951
MLP throughput (in TFLOP/s): 234.602
Transformer duration (in seconds): 0.3153
Transformer throughput (in TFLOP/s): 221.756
========================================================================================================================
num_attention_heads: 128, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1139
Attention throughput (in TFLOP/s): 214.820
MLP duration (in seconds): 0.1975
MLP throughput (in TFLOP/s): 234.975
Transformer duration (in seconds): 0.3179
Transformer throughput (in TFLOP/s): 222.903
========================================================================================================================
num_attention_heads: 128, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1151
Attention throughput (in TFLOP/s): 215.346
MLP duration (in seconds): 0.1968
MLP throughput (in TFLOP/s): 239.015
Transformer duration (in seconds): 0.3189
Transformer throughput (in TFLOP/s): 225.206
========================================================================================================================
num_attention_heads: 128, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1169
Attention throughput (in TFLOP/s): 214.936
MLP duration (in seconds): 0.2004
MLP throughput (in TFLOP/s): 237.946
Transformer duration (in seconds): 0.3246
Transformer throughput (in TFLOP/s): 224.292
========================================================================================================================
num_attention_heads: 128, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1172
Attention throughput (in TFLOP/s): 217.214
MLP duration (in seconds): 0.2024
MLP throughput (in TFLOP/s): 238.716
Transformer duration (in seconds): 0.3269
Transformer throughput (in TFLOP/s): 225.627
========================================================================================================================
num_attention_heads: 128, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1185
Attention throughput (in TFLOP/s): 217.484
MLP duration (in seconds): 0.2042
MLP throughput (in TFLOP/s): 239.810
Transformer duration (in seconds): 0.3309
Transformer throughput (in TFLOP/s): 225.885
========================================================================================================================
num_attention_heads: 128, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1131
Attention throughput (in TFLOP/s): 230.845
MLP duration (in seconds): 0.2082
MLP throughput (in TFLOP/s): 238.250
Transformer duration (in seconds): 0.3284
Transformer throughput (in TFLOP/s): 230.566
========================================================================================================================
num_attention_heads: 128, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1224
Attention throughput (in TFLOP/s): 216.032
MLP duration (in seconds): 0.2114
MLP throughput (in TFLOP/s): 237.817
Transformer duration (in seconds): 0.3399
Transformer throughput (in TFLOP/s): 225.691
========================================================================================================================
num_attention_heads: 128, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1221
Attention throughput (in TFLOP/s): 219.397
MLP duration (in seconds): 0.2136
MLP throughput (in TFLOP/s): 238.430
Transformer duration (in seconds): 0.3451
Transformer throughput (in TFLOP/s): 225.218
========================================================================================================================
num_attention_heads: 128, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1242
Attention throughput (in TFLOP/s): 218.424
MLP duration (in seconds): 0.2164
MLP throughput (in TFLOP/s): 238.362
Transformer duration (in seconds): 0.3481
Transformer throughput (in TFLOP/s): 226.145
========================================================================================================================
num_attention_heads: 128, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1259
Attention throughput (in TFLOP/s): 218.240
MLP duration (in seconds): 0.2197
MLP throughput (in TFLOP/s): 237.925
Transformer duration (in seconds): 0.3528
Transformer throughput (in TFLOP/s): 225.993
========================================================================================================================
num_attention_heads: 128, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1273
Attention throughput (in TFLOP/s): 218.547
MLP duration (in seconds): 0.2223
MLP throughput (in TFLOP/s): 238.129
Transformer duration (in seconds): 0.3584
Transformer throughput (in TFLOP/s): 225.304
========================================================================================================================
num_attention_heads: 128, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1286
Attention throughput (in TFLOP/s): 218.922
MLP duration (in seconds): 0.2242
MLP throughput (in TFLOP/s): 239.071
Transformer duration (in seconds): 0.3608
Transformer throughput (in TFLOP/s): 226.670
========================================================================================================================
num_attention_heads: 128, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1306
Attention throughput (in TFLOP/s): 218.383
MLP duration (in seconds): 0.2287
MLP throughput (in TFLOP/s): 237.437
Transformer duration (in seconds): 0.3679
Transformer throughput (in TFLOP/s): 225.095
========================================================================================================================
num_attention_heads: 128, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1275
Attention throughput (in TFLOP/s): 226.341
MLP duration (in seconds): 0.2310
MLP throughput (in TFLOP/s): 237.959
Transformer duration (in seconds): 0.3662
Transformer throughput (in TFLOP/s): 228.910
========================================================================================================================
num_attention_heads: 128, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1356
Attention throughput (in TFLOP/s): 215.450
MLP duration (in seconds): 0.2530
MLP throughput (in TFLOP/s): 219.982
Transformer duration (in seconds): 0.3976
Transformer throughput (in TFLOP/s): 213.480
========================================================================================================================
num_attention_heads: 128, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1327
Attention throughput (in TFLOP/s): 222.877
MLP duration (in seconds): 0.2543
MLP throughput (in TFLOP/s): 221.653
Transformer duration (in seconds): 0.3928
Transformer throughput (in TFLOP/s): 218.787
========================================================================================================================
num_attention_heads: 128, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1386
Attention throughput (in TFLOP/s): 215.903
MLP duration (in seconds): 0.2600
MLP throughput (in TFLOP/s): 219.479
Transformer duration (in seconds): 0.4061
Transformer throughput (in TFLOP/s): 214.196
========================================================================================================================
num_attention_heads: 128, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1402
Attention throughput (in TFLOP/s): 216.088
MLP duration (in seconds): 0.2628
MLP throughput (in TFLOP/s): 219.786
Transformer duration (in seconds): 0.4086
Transformer throughput (in TFLOP/s): 215.462
========================================================================================================================
num_attention_heads: 128, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1414
Attention throughput (in TFLOP/s): 216.824
MLP duration (in seconds): 0.2679
MLP throughput (in TFLOP/s): 218.232
Transformer duration (in seconds): 0.4185
Transformer throughput (in TFLOP/s): 212.924
========================================================================================================================
num_attention_heads: 128, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1425
Attention throughput (in TFLOP/s): 217.622
MLP duration (in seconds): 0.2696
MLP throughput (in TFLOP/s): 219.492
Transformer duration (in seconds): 0.4197
Transformer throughput (in TFLOP/s): 214.873
========================================================================================================================
num_attention_heads: 128, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1441
Attention throughput (in TFLOP/s): 217.767
MLP duration (in seconds): 0.2730
MLP throughput (in TFLOP/s): 219.415
Transformer duration (in seconds): 0.4255
Transformer throughput (in TFLOP/s): 214.519
========================================================================================================================
num_attention_heads: 128, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1420
Attention throughput (in TFLOP/s): 223.582
MLP duration (in seconds): 0.2752
MLP throughput (in TFLOP/s): 220.271
Transformer duration (in seconds): 0.4231
Transformer throughput (in TFLOP/s): 218.293
========================================================================================================================
num_attention_heads: 128, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1477
Attention throughput (in TFLOP/s): 217.531
MLP duration (in seconds): 0.2805
MLP throughput (in TFLOP/s): 218.647
Transformer duration (in seconds): 0.4380
Transformer throughput (in TFLOP/s): 213.346
========================================================================================================================
num_attention_heads: 128, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1486
Attention throughput (in TFLOP/s): 218.626
MLP duration (in seconds): 0.2815
MLP throughput (in TFLOP/s): 220.449
Transformer duration (in seconds): 0.4384
Transformer throughput (in TFLOP/s): 215.667
========================================================================================================================
num_attention_heads: 128, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1502
Attention throughput (in TFLOP/s): 218.779
MLP duration (in seconds): 0.2856
MLP throughput (in TFLOP/s): 219.886
Transformer duration (in seconds): 0.4454
Transformer throughput (in TFLOP/s): 214.784
========================================================================================================================
num_attention_heads: 128, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1519
Attention throughput (in TFLOP/s): 218.879
MLP duration (in seconds): 0.2873
MLP throughput (in TFLOP/s): 221.165
Transformer duration (in seconds): 0.4475
Transformer throughput (in TFLOP/s): 216.264
========================================================================================================================
num_attention_heads: 128, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1533
Attention throughput (in TFLOP/s): 219.381
MLP duration (in seconds): 0.2946
MLP throughput (in TFLOP/s): 218.167
Transformer duration (in seconds): 0.4583
Transformer throughput (in TFLOP/s): 213.608
========================================================================================================================
num_attention_heads: 128, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1554
Attention throughput (in TFLOP/s): 218.768
MLP duration (in seconds): 0.3012
MLP throughput (in TFLOP/s): 215.833
Transformer duration (in seconds): 0.4628
Transformer throughput (in TFLOP/s): 213.943
========================================================================================================================
num_attention_heads: 128, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1564
Attention throughput (in TFLOP/s): 219.902
MLP duration (in seconds): 0.2825
MLP throughput (in TFLOP/s): 232.802
Transformer duration (in seconds): 0.4471
Transformer throughput (in TFLOP/s): 224.013
========================================================================================================================
num_attention_heads: 128, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1500
Attention throughput (in TFLOP/s): 231.861
MLP duration (in seconds): 0.2858
MLP throughput (in TFLOP/s): 232.760
Transformer duration (in seconds): 0.4431
Transformer throughput (in TFLOP/s): 228.616
========================================================================================================================
num_attention_heads: 128, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1611
Attention throughput (in TFLOP/s): 218.297
MLP duration (in seconds): 0.2890
MLP throughput (in TFLOP/s): 232.759
Transformer duration (in seconds): 0.4600
Transformer throughput (in TFLOP/s): 222.702
========================================================================================================================
num_attention_heads: 128, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1626
Attention throughput (in TFLOP/s): 218.610
MLP duration (in seconds): 0.2922
MLP throughput (in TFLOP/s): 232.852
Transformer duration (in seconds): 0.4620
Transformer throughput (in TFLOP/s): 224.225
========================================================================================================================
num_attention_heads: 128, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1644
Attention throughput (in TFLOP/s): 218.643
MLP duration (in seconds): 0.2959
MLP throughput (in TFLOP/s): 232.542
Transformer duration (in seconds): 0.4694
Transformer throughput (in TFLOP/s): 223.142
========================================================================================================================
num_attention_heads: 128, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1623
Attention throughput (in TFLOP/s): 223.923
MLP duration (in seconds): 0.2987
MLP throughput (in TFLOP/s): 232.933
Transformer duration (in seconds): 0.4672
Transformer throughput (in TFLOP/s): 226.706
========================================================================================================================
num_attention_heads: 128, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1688
Attention throughput (in TFLOP/s): 217.571
MLP duration (in seconds): 0.3018
MLP throughput (in TFLOP/s): 233.152
Transformer duration (in seconds): 0.4811
Transformer throughput (in TFLOP/s): 222.580
========================================================================================================================
num_attention_heads: 128, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1691
Attention throughput (in TFLOP/s): 219.522
MLP duration (in seconds): 0.3046
MLP throughput (in TFLOP/s): 233.561
Transformer duration (in seconds): 0.4851
Transformer throughput (in TFLOP/s): 223.189
========================================================================================================================
num_attention_heads: 128, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1721
Attention throughput (in TFLOP/s): 218.098
MLP duration (in seconds): 0.3084
MLP throughput (in TFLOP/s): 233.199
Transformer duration (in seconds): 0.4919
Transformer throughput (in TFLOP/s): 222.509
========================================================================================================================
num_attention_heads: 128, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1628
Attention throughput (in TFLOP/s): 232.985
MLP duration (in seconds): 0.3115
MLP throughput (in TFLOP/s): 233.386
Transformer duration (in seconds): 0.4820
Transformer throughput (in TFLOP/s): 229.548
========================================================================================================================
num_attention_heads: 128, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1737
Attention throughput (in TFLOP/s): 220.770
MLP duration (in seconds): 0.3163
MLP throughput (in TFLOP/s): 232.341
Transformer duration (in seconds): 0.5007
Transformer throughput (in TFLOP/s): 223.338
========================================================================================================================
num_attention_heads: 128, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1717
Attention throughput (in TFLOP/s): 225.645
MLP duration (in seconds): 0.3188
MLP throughput (in TFLOP/s): 233.031
Transformer duration (in seconds): 0.4981
Transformer throughput (in TFLOP/s): 226.934
========================================================================================================================
num_attention_heads: 128, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1789
Attention throughput (in TFLOP/s): 218.894
MLP duration (in seconds): 0.3233
MLP throughput (in TFLOP/s): 232.262
Transformer duration (in seconds): 0.5120
Transformer throughput (in TFLOP/s): 223.150
========================================================================================================================
num_attention_heads: 128, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1806
Attention throughput (in TFLOP/s): 219.035
MLP duration (in seconds): 0.3264
MLP throughput (in TFLOP/s): 232.568
Transformer duration (in seconds): 0.5177
Transformer throughput (in TFLOP/s): 223.032
========================================================================================================================
num_attention_heads: 128, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1771
Attention throughput (in TFLOP/s): 225.776
MLP duration (in seconds): 0.3295
MLP throughput (in TFLOP/s): 232.840
Transformer duration (in seconds): 0.5148
Transformer throughput (in TFLOP/s): 226.658
========================================================================================================================
num_attention_heads: 128, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1807
Attention throughput (in TFLOP/s): 223.484
MLP duration (in seconds): 0.3340
MLP throughput (in TFLOP/s): 232.102
Transformer duration (in seconds): 0.5222
Transformer throughput (in TFLOP/s): 225.813
========================================================================================================================
num_attention_heads: 128, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1809
Attention throughput (in TFLOP/s): 225.622
MLP duration (in seconds): 0.3376
MLP throughput (in TFLOP/s): 232.052
Transformer duration (in seconds): 0.5270
Transformer throughput (in TFLOP/s): 226.100
========================================================================================================================
num_attention_heads: 128, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1761
Attention throughput (in TFLOP/s): 234.074
MLP duration (in seconds): 0.3406
MLP throughput (in TFLOP/s): 232.436
Transformer duration (in seconds): 0.5255
Transformer throughput (in TFLOP/s): 229.104
========================================================================================================================
num_attention_heads: 128, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1889
Attention throughput (in TFLOP/s): 220.461
MLP duration (in seconds): 0.3446
MLP throughput (in TFLOP/s): 232.120
Transformer duration (in seconds): 0.5406
Transformer throughput (in TFLOP/s): 225.033
========================================================================================================================
num_attention_heads: 128, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1936
Attention throughput (in TFLOP/s): 217.365
MLP duration (in seconds): 0.3481
MLP throughput (in TFLOP/s): 232.195
Transformer duration (in seconds): 0.5518
Transformer throughput (in TFLOP/s): 222.730
========================================================================================================================
num_attention_heads: 128, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1950
Attention throughput (in TFLOP/s): 218.018
MLP duration (in seconds): 0.3515
MLP throughput (in TFLOP/s): 232.344
Transformer duration (in seconds): 0.5549
Transformer throughput (in TFLOP/s): 223.736
========================================================================================================================
num_attention_heads: 128, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1985
Attention throughput (in TFLOP/s): 216.320
MLP duration (in seconds): 0.3554
MLP throughput (in TFLOP/s): 232.117
Transformer duration (in seconds): 0.5642
Transformer throughput (in TFLOP/s): 222.320
========================================================================================================================
num_attention_heads: 128, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1989
Attention throughput (in TFLOP/s): 218.004
MLP duration (in seconds): 0.3591
MLP throughput (in TFLOP/s): 232.112
Transformer duration (in seconds): 0.5690
Transformer throughput (in TFLOP/s): 222.688
========================================================================================================================
num_attention_heads: 128, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2012
Attention throughput (in TFLOP/s): 217.674
MLP duration (in seconds): 0.3636
MLP throughput (in TFLOP/s): 231.572
Transformer duration (in seconds): 0.5747
Transformer throughput (in TFLOP/s): 222.707
========================================================================================================================
num_attention_heads: 128, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2022
Attention throughput (in TFLOP/s): 218.777
MLP duration (in seconds): 0.3663
MLP throughput (in TFLOP/s): 232.170
Transformer duration (in seconds): 0.5789
Transformer throughput (in TFLOP/s): 223.293
========================================================================================================================
num_attention_heads: 128, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1984
Attention throughput (in TFLOP/s): 225.150
MLP duration (in seconds): 0.3699
MLP throughput (in TFLOP/s): 232.194
Transformer duration (in seconds): 0.5808
Transformer throughput (in TFLOP/s): 224.810
========================================================================================================================
num_attention_heads: 128, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2049
Attention throughput (in TFLOP/s): 220.173
MLP duration (in seconds): 0.3747
MLP throughput (in TFLOP/s): 231.546
Transformer duration (in seconds): 0.5914
Transformer throughput (in TFLOP/s): 222.983
========================================================================================================================
num_attention_heads: 128, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2079
Attention throughput (in TFLOP/s): 219.109
MLP duration (in seconds): 0.3786
MLP throughput (in TFLOP/s): 231.442
Transformer duration (in seconds): 0.5972
Transformer throughput (in TFLOP/s): 222.982
========================================================================================================================
num_attention_heads: 128, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2094
Attention throughput (in TFLOP/s): 219.637
MLP duration (in seconds): 0.3819
MLP throughput (in TFLOP/s): 231.740
Transformer duration (in seconds): 0.6030
Transformer throughput (in TFLOP/s): 223.033
========================================================================================================================
num_attention_heads: 128, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2118
Attention throughput (in TFLOP/s): 219.234
MLP duration (in seconds): 0.3869
MLP throughput (in TFLOP/s): 230.963
Transformer duration (in seconds): 0.6078
Transformer throughput (in TFLOP/s): 223.431
========================================================================================================================
num_attention_heads: 128, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2130
Attention throughput (in TFLOP/s): 220.078
MLP duration (in seconds): 0.3901
MLP throughput (in TFLOP/s): 231.322
Transformer duration (in seconds): 0.6143
Transformer throughput (in TFLOP/s): 223.225
========================================================================================================================
num_attention_heads: 128, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2145
Attention throughput (in TFLOP/s): 220.635
MLP duration (in seconds): 0.3938
MLP throughput (in TFLOP/s): 231.418
Transformer duration (in seconds): 0.6186
Transformer throughput (in TFLOP/s): 223.822
========================================================================================================================
num_attention_heads: 128, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2170
Attention throughput (in TFLOP/s): 220.175
MLP duration (in seconds): 0.3970
MLP throughput (in TFLOP/s): 231.759
Transformer duration (in seconds): 0.6257
Transformer throughput (in TFLOP/s): 223.426
========================================================================================================================
num_attention_heads: 128, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2136
Attention throughput (in TFLOP/s): 225.879
MLP duration (in seconds): 0.4012
MLP throughput (in TFLOP/s): 231.550
Transformer duration (in seconds): 0.6242
Transformer throughput (in TFLOP/s): 226.116
========================================================================================================================
num_attention_heads: 128, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2225
Attention throughput (in TFLOP/s): 218.825
MLP duration (in seconds): 0.4054
MLP throughput (in TFLOP/s): 231.366
Transformer duration (in seconds): 0.6401
Transformer throughput (in TFLOP/s): 222.628
========================================================================================================================
num_attention_heads: 128, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2247
Attention throughput (in TFLOP/s): 218.754
MLP duration (in seconds): 0.4094
MLP throughput (in TFLOP/s): 231.309
Transformer duration (in seconds): 0.6452
Transformer throughput (in TFLOP/s): 222.975
========================================================================================================================
num_attention_heads: 128, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2268
Attention throughput (in TFLOP/s): 218.805
MLP duration (in seconds): 0.4144
MLP throughput (in TFLOP/s): 230.739
Transformer duration (in seconds): 0.6510
Transformer throughput (in TFLOP/s): 223.087
========================================================================================================================
num_attention_heads: 128, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2285
Attention throughput (in TFLOP/s): 219.119
MLP duration (in seconds): 0.4192
MLP throughput (in TFLOP/s): 230.243
Transformer duration (in seconds): 0.6560
Transformer throughput (in TFLOP/s): 223.461
========================================================================================================================
num_attention_heads: 128, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2305
Attention throughput (in TFLOP/s): 219.297
MLP duration (in seconds): 0.4211
MLP throughput (in TFLOP/s): 231.385
Transformer duration (in seconds): 0.6629
Transformer throughput (in TFLOP/s): 223.216
========================================================================================================================
num_attention_heads: 128, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2326
Attention throughput (in TFLOP/s): 219.287
MLP duration (in seconds): 0.4250
MLP throughput (in TFLOP/s): 231.382
Transformer duration (in seconds): 0.6687
Transformer throughput (in TFLOP/s): 223.344
========================================================================================================================
num_attention_heads: 128, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2346
Attention throughput (in TFLOP/s): 219.401
MLP duration (in seconds): 0.4292
MLP throughput (in TFLOP/s): 231.277
Transformer duration (in seconds): 0.6744
Transformer throughput (in TFLOP/s): 223.542
========================================================================================================================
num_attention_heads: 128, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2303
Attention throughput (in TFLOP/s): 225.608
MLP duration (in seconds): 0.4332
MLP throughput (in TFLOP/s): 231.269
Transformer duration (in seconds): 0.6724
Transformer throughput (in TFLOP/s): 226.272
========================================================================================================================
num_attention_heads: 128, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2370
Attention throughput (in TFLOP/s): 221.159
MLP duration (in seconds): 0.4374
MLP throughput (in TFLOP/s): 231.178
Transformer duration (in seconds): 0.6845
Transformer throughput (in TFLOP/s): 224.310
========================================================================================================================
num_attention_heads: 128, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2361
Attention throughput (in TFLOP/s): 224.075
MLP duration (in seconds): 0.4413
MLP throughput (in TFLOP/s): 231.274
Transformer duration (in seconds): 0.6904
Transformer throughput (in TFLOP/s): 224.441
========================================================================================================================
num_attention_heads: 128, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2383
Attention throughput (in TFLOP/s): 223.979
MLP duration (in seconds): 0.4463
MLP throughput (in TFLOP/s): 230.799
Transformer duration (in seconds): 0.6959
Transformer throughput (in TFLOP/s): 224.701
========================================================================================================================
num_attention_heads: 128, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2400
Attention throughput (in TFLOP/s): 224.457
MLP duration (in seconds): 0.4493
MLP throughput (in TFLOP/s): 231.308
Transformer duration (in seconds): 0.7034
Transformer throughput (in TFLOP/s): 224.324
========================================================================================================================
num_attention_heads: 128, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2446
Attention throughput (in TFLOP/s): 222.142
MLP duration (in seconds): 0.4549
MLP throughput (in TFLOP/s): 230.590
Transformer duration (in seconds): 0.7089
Transformer throughput (in TFLOP/s): 224.618
========================================================================================================================
num_attention_heads: 128, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2468
Attention throughput (in TFLOP/s): 222.164
MLP duration (in seconds): 0.4573
MLP throughput (in TFLOP/s): 231.436
Transformer duration (in seconds): 0.7155
Transformer throughput (in TFLOP/s): 224.560
========================================================================================================================
num_attention_heads: 128, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2413
Attention throughput (in TFLOP/s): 229.241
MLP duration (in seconds): 0.4660
MLP throughput (in TFLOP/s): 229.183
Transformer duration (in seconds): 0.7139
Transformer throughput (in TFLOP/s): 227.060
========================================================================================================================
num_attention_heads: 128, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2379
Attention throughput (in TFLOP/s): 234.570
MLP duration (in seconds): 0.4650
MLP throughput (in TFLOP/s): 231.702
Transformer duration (in seconds): 0.7130
Transformer throughput (in TFLOP/s): 229.387
========================================================================================================================
num_attention_heads: 128, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2546
Attention throughput (in TFLOP/s): 221.105
MLP duration (in seconds): 0.4753
MLP throughput (in TFLOP/s): 228.713
Transformer duration (in seconds): 0.7382
Transformer throughput (in TFLOP/s): 223.538
========================================================================================================================
num_attention_heads: 128, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2578
Attention throughput (in TFLOP/s): 220.275
MLP duration (in seconds): 0.4787
MLP throughput (in TFLOP/s): 229.118
Transformer duration (in seconds): 0.7448
Transformer throughput (in TFLOP/s): 223.500
========================================================================================================================
num_attention_heads: 128, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2598
Attention throughput (in TFLOP/s): 220.433
MLP duration (in seconds): 0.4833
MLP throughput (in TFLOP/s): 228.957
Transformer duration (in seconds): 0.7524
Transformer throughput (in TFLOP/s): 223.210
========================================================================================================================
num_attention_heads: 128, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2616
Attention throughput (in TFLOP/s): 220.860
MLP duration (in seconds): 0.4897
MLP throughput (in TFLOP/s): 227.988
Transformer duration (in seconds): 0.7572
Transformer throughput (in TFLOP/s): 223.747
========================================================================================================================
num_attention_heads: 128, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2596
Attention throughput (in TFLOP/s): 224.510
MLP duration (in seconds): 0.4941
MLP throughput (in TFLOP/s): 227.918
Transformer duration (in seconds): 0.7608
Transformer throughput (in TFLOP/s): 224.625
========================================================================================================================
num_attention_heads: 128, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2618
Attention throughput (in TFLOP/s): 224.507
MLP duration (in seconds): 0.4967
MLP throughput (in TFLOP/s): 228.703
Transformer duration (in seconds): 0.7661
Transformer throughput (in TFLOP/s): 225.015
========================================================================================================================
num_attention_heads: 128, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2643
Attention throughput (in TFLOP/s): 224.266
MLP duration (in seconds): 0.5048
MLP throughput (in TFLOP/s): 226.999
Transformer duration (in seconds): 0.7783
Transformer throughput (in TFLOP/s): 223.413
========================================================================================================================
num_attention_heads: 128, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2603
Attention throughput (in TFLOP/s): 229.684
MLP duration (in seconds): 0.5063
MLP throughput (in TFLOP/s): 228.315
Transformer duration (in seconds): 0.7749
Transformer throughput (in TFLOP/s): 226.327
========================================================================================================================
num_attention_heads: 128, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2686
Attention throughput (in TFLOP/s): 224.483
MLP duration (in seconds): 0.5126
MLP throughput (in TFLOP/s): 227.458
Transformer duration (in seconds): 0.7878
Transformer throughput (in TFLOP/s): 224.525
========================================================================================================================
num_attention_heads: 128, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2730
Attention throughput (in TFLOP/s): 222.721
MLP duration (in seconds): 0.5157
MLP throughput (in TFLOP/s): 228.004
Transformer duration (in seconds): 0.7967
Transformer throughput (in TFLOP/s): 223.915
========================================================================================================================
num_attention_heads: 128, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2739
Attention throughput (in TFLOP/s): 223.866
MLP duration (in seconds): 0.5185
MLP throughput (in TFLOP/s): 228.710
Transformer duration (in seconds): 0.8032
Transformer throughput (in TFLOP/s): 223.999
========================================================================================================================
num_attention_heads: 128, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2697
Attention throughput (in TFLOP/s): 229.266
MLP duration (in seconds): 0.5235
MLP throughput (in TFLOP/s): 228.472
Transformer duration (in seconds): 0.7999
Transformer throughput (in TFLOP/s): 226.833
========================================================================================================================
num_attention_heads: 128, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2712
Attention throughput (in TFLOP/s): 229.882
MLP duration (in seconds): 0.5285
MLP throughput (in TFLOP/s): 228.242
Transformer duration (in seconds): 0.8064
Transformer throughput (in TFLOP/s): 226.886
========================================================================================================================
num_attention_heads: 128, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2735
Attention throughput (in TFLOP/s): 229.853
MLP duration (in seconds): 0.5329
MLP throughput (in TFLOP/s): 228.278
Transformer duration (in seconds): 0.8136
Transformer throughput (in TFLOP/s): 226.775
========================================================================================================================
num_attention_heads: 128, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2756
Attention throughput (in TFLOP/s): 230.009
MLP duration (in seconds): 0.5366
MLP throughput (in TFLOP/s): 228.581
Transformer duration (in seconds): 0.8186
Transformer throughput (in TFLOP/s): 227.282
========================================================================================================================
num_attention_heads: 128, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2729
Attention throughput (in TFLOP/s): 234.178
MLP duration (in seconds): 0.5392
MLP throughput (in TFLOP/s): 229.396
Transformer duration (in seconds): 0.8209
Transformer throughput (in TFLOP/s): 228.527
========================================================================================================================
num_attention_heads: 128, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2848
Attention throughput (in TFLOP/s): 226.236
MLP duration (in seconds): 0.5451
MLP throughput (in TFLOP/s): 228.836
Transformer duration (in seconds): 0.8396
Transformer throughput (in TFLOP/s): 225.314
========================================================================================================================
num_attention_heads: 128, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2872
Attention throughput (in TFLOP/s): 226.212
MLP duration (in seconds): 0.5509
MLP throughput (in TFLOP/s): 228.299
Transformer duration (in seconds): 0.8464
Transformer throughput (in TFLOP/s): 225.330
========================================================================================================================
num_attention_heads: 128, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2886
Attention throughput (in TFLOP/s): 226.949
MLP duration (in seconds): 0.5546
MLP throughput (in TFLOP/s): 228.644
Transformer duration (in seconds): 0.8544
Transformer throughput (in TFLOP/s): 225.071
========================================================================================================================
num_attention_heads: 128, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2913
Attention throughput (in TFLOP/s): 226.659
MLP duration (in seconds): 0.5601
MLP throughput (in TFLOP/s): 228.266
Transformer duration (in seconds): 0.8591
Transformer throughput (in TFLOP/s): 225.669
========================================================================================================================
num_attention_heads: 128, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2940
Attention throughput (in TFLOP/s): 226.409
MLP duration (in seconds): 0.5621
MLP throughput (in TFLOP/s): 229.328
Transformer duration (in seconds): 0.8659
Transformer throughput (in TFLOP/s): 225.727
========================================================================================================================
num_attention_heads: 128, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2957
Attention throughput (in TFLOP/s): 226.861
MLP duration (in seconds): 0.5659
MLP throughput (in TFLOP/s): 229.643
Transformer duration (in seconds): 0.8745
Transformer throughput (in TFLOP/s): 225.337
========================================================================================================================
num_attention_heads: 128, hidden_size: 31616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2984
Attention throughput (in TFLOP/s): 226.616
MLP duration (in seconds): 0.5728
MLP throughput (in TFLOP/s): 228.719
Transformer duration (in seconds): 0.8794
Transformer throughput (in TFLOP/s): 225.896
========================================================================================================================
num_attention_heads: 128, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2948
Attention throughput (in TFLOP/s): 231.252
MLP duration (in seconds): 0.5759
MLP throughput (in TFLOP/s): 229.334
Transformer duration (in seconds): 0.8837
Transformer throughput (in TFLOP/s): 226.607
========================================================================================================================
num_attention_heads: 128, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.3026
Attention throughput (in TFLOP/s): 227.073
MLP duration (in seconds): 0.5845
MLP throughput (in TFLOP/s): 227.806
Transformer duration (in seconds): 0.8933
Transformer throughput (in TFLOP/s): 225.969
========================================================================================================================
num_attention_heads: 128, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.3045
Attention throughput (in TFLOP/s): 227.464
MLP duration (in seconds): 0.5845
MLP throughput (in TFLOP/s): 229.613
Transformer duration (in seconds): 0.9010
Transformer throughput (in TFLOP/s): 225.835
========================================================================================================================
num_attention_heads: 128, hidden_size: 32128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.3079
Attention throughput (in TFLOP/s): 226.717
MLP duration (in seconds): 0.5914
MLP throughput (in TFLOP/s): 228.767
Transformer duration (in seconds): 0.9116
Transformer throughput (in TFLOP/s): 224.990
========================================================================================================================
num_attention_heads: 128, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.3101
Attention throughput (in TFLOP/s): 226.862
MLP duration (in seconds): 0.5958
MLP throughput (in TFLOP/s): 228.904
Transformer duration (in seconds): 0.9150
Transformer throughput (in TFLOP/s): 225.939
========================================================================================================================
num_attention_heads: 128, hidden_size: 32384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.3134
Attention throughput (in TFLOP/s): 226.271
MLP duration (in seconds): 0.6030
MLP throughput (in TFLOP/s): 227.966
Transformer duration (in seconds): 0.9301
Transformer throughput (in TFLOP/s): 224.013
========================================================================================================================
num_attention_heads: 128, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.3151
Attention throughput (in TFLOP/s): 226.765
MLP duration (in seconds): 0.6071
MLP throughput (in TFLOP/s): 228.228
Transformer duration (in seconds): 0.9366
Transformer throughput (in TFLOP/s): 224.218
========================================================================================================================
num_attention_heads: 128, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.3178
Attention throughput (in TFLOP/s): 226.615
MLP duration (in seconds): 0.6116
MLP throughput (in TFLOP/s): 228.319
Transformer duration (in seconds): 0.9462
Transformer throughput (in TFLOP/s): 223.690
========================================================================================================================
