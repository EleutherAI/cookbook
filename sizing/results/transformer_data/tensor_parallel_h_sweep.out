num_attention_heads: 128, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0388
Attention throughput (in TFLOP/s): 127.357
MLP duration (in seconds): 0.0367
MLP throughput (in TFLOP/s): 239.830
Transformer duration (in seconds): 0.0786
Transformer throughput (in TFLOP/s): 174.765
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0533
Attention throughput (in TFLOP/s): 95.514
MLP duration (in seconds): 0.0382
MLP throughput (in TFLOP/s): 237.644
Transformer duration (in seconds): 0.0956
Transformer throughput (in TFLOP/s): 148.248
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0546
Attention throughput (in TFLOP/s): 96.132
MLP duration (in seconds): 0.0394
MLP throughput (in TFLOP/s): 237.158
Transformer duration (in seconds): 0.0980
Transformer throughput (in TFLOP/s): 148.901
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0551
Attention throughput (in TFLOP/s): 97.836
MLP duration (in seconds): 0.0409
MLP throughput (in TFLOP/s): 235.913
Transformer duration (in seconds): 0.1007
Transformer throughput (in TFLOP/s): 149.357
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0570
Attention throughput (in TFLOP/s): 97.406
MLP duration (in seconds): 0.0418
MLP throughput (in TFLOP/s): 237.724
Transformer duration (in seconds): 0.1027
Transformer throughput (in TFLOP/s): 150.684
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0576
Attention throughput (in TFLOP/s): 99.071
MLP duration (in seconds): 0.0431
MLP throughput (in TFLOP/s): 237.349
Transformer duration (in seconds): 0.1047
Transformer throughput (in TFLOP/s): 152.090
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0596
Attention throughput (in TFLOP/s): 98.347
MLP duration (in seconds): 0.0432
MLP throughput (in TFLOP/s): 243.422
Transformer duration (in seconds): 0.1057
Transformer throughput (in TFLOP/s): 155.036
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0608
Attention throughput (in TFLOP/s): 99.073
MLP duration (in seconds): 0.0446
MLP throughput (in TFLOP/s): 242.653
Transformer duration (in seconds): 0.1075
Transformer throughput (in TFLOP/s): 156.691
Transformer - MLP - Attention (in seconds): 0.0021
========================================================================================================================
num_attention_heads: 128, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0454
Attention throughput (in TFLOP/s): 136.295
MLP duration (in seconds): 0.0468
MLP throughput (in TFLOP/s): 237.722
Transformer duration (in seconds): 0.0955
Transformer throughput (in TFLOP/s): 181.365
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0607
Attention throughput (in TFLOP/s): 104.572
MLP duration (in seconds): 0.0479
MLP throughput (in TFLOP/s): 239.074
Transformer duration (in seconds): 0.1128
Transformer throughput (in TFLOP/s): 157.748
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0490
Attention throughput (in TFLOP/s): 132.995
MLP duration (in seconds): 0.0496
MLP throughput (in TFLOP/s): 237.087
Transformer duration (in seconds): 0.1020
Transformer throughput (in TFLOP/s): 179.088
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0622
Attention throughput (in TFLOP/s): 107.531
MLP duration (in seconds): 0.0510
MLP throughput (in TFLOP/s): 237.037
Transformer duration (in seconds): 0.1179
Transformer throughput (in TFLOP/s): 159.144
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0502
Attention throughput (in TFLOP/s): 136.652
MLP duration (in seconds): 0.0510
MLP throughput (in TFLOP/s): 243.043
Transformer duration (in seconds): 0.1040
Transformer throughput (in TFLOP/s): 185.134
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0638
Attention throughput (in TFLOP/s): 110.152
MLP duration (in seconds): 0.0540
MLP throughput (in TFLOP/s): 235.675
Transformer duration (in seconds): 0.1218
Transformer throughput (in TFLOP/s): 162.273
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0518
Attention throughput (in TFLOP/s): 139.017
MLP duration (in seconds): 0.0540
MLP throughput (in TFLOP/s): 241.832
Transformer duration (in seconds): 0.1087
Transformer throughput (in TFLOP/s): 186.479
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0657
Attention throughput (in TFLOP/s): 112.254
MLP duration (in seconds): 0.0572
MLP throughput (in TFLOP/s): 234.205
Transformer duration (in seconds): 0.1274
Transformer throughput (in TFLOP/s): 163.074
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0512
Attention throughput (in TFLOP/s): 147.607
MLP duration (in seconds): 0.0570
MLP throughput (in TFLOP/s): 241.095
Transformer duration (in seconds): 0.1107
Transformer throughput (in TFLOP/s): 192.473
Transformer - MLP - Attention (in seconds): 0.0025
========================================================================================================================
num_attention_heads: 128, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0672
Attention throughput (in TFLOP/s): 115.255
MLP duration (in seconds): 0.0600
MLP throughput (in TFLOP/s): 234.702
Transformer duration (in seconds): 0.1317
Transformer throughput (in TFLOP/s): 165.816
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0551
Attention throughput (in TFLOP/s): 143.942
MLP duration (in seconds): 0.0616
MLP throughput (in TFLOP/s): 234.476
Transformer duration (in seconds): 0.1204
Transformer throughput (in TFLOP/s): 185.774
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0687
Attention throughput (in TFLOP/s): 117.979
MLP duration (in seconds): 0.0626
MLP throughput (in TFLOP/s): 236.146
Transformer duration (in seconds): 0.1352
Transformer throughput (in TFLOP/s): 169.412
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0561
Attention throughput (in TFLOP/s): 147.970
MLP duration (in seconds): 0.0621
MLP throughput (in TFLOP/s): 243.954
Transformer duration (in seconds): 0.1211
Transformer throughput (in TFLOP/s): 193.589
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 128, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0704
Attention throughput (in TFLOP/s): 120.629
MLP duration (in seconds): 0.0656
MLP throughput (in TFLOP/s): 236.520
Transformer duration (in seconds): 0.1403
Transformer throughput (in TFLOP/s): 171.062
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0575
Attention throughput (in TFLOP/s): 150.828
MLP duration (in seconds): 0.0654
MLP throughput (in TFLOP/s): 242.785
Transformer duration (in seconds): 0.1262
Transformer throughput (in TFLOP/s): 194.678
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0717
Attention throughput (in TFLOP/s): 123.848
MLP duration (in seconds): 0.0686
MLP throughput (in TFLOP/s): 236.772
Transformer duration (in seconds): 0.1443
Transformer throughput (in TFLOP/s): 174.166
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0572
Attention throughput (in TFLOP/s): 158.612
MLP duration (in seconds): 0.0683
MLP throughput (in TFLOP/s): 243.462
Transformer duration (in seconds): 0.1282
Transformer throughput (in TFLOP/s): 200.524
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0734
Attention throughput (in TFLOP/s): 126.223
MLP duration (in seconds): 0.0702
MLP throughput (in TFLOP/s): 242.374
Transformer duration (in seconds): 0.1467
Transformer throughput (in TFLOP/s): 179.173
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0608
Attention throughput (in TFLOP/s): 155.829
MLP duration (in seconds): 0.0735
MLP throughput (in TFLOP/s): 236.613
Transformer duration (in seconds): 0.1387
Transformer throughput (in TFLOP/s): 193.708
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0750
Attention throughput (in TFLOP/s): 129.048
MLP duration (in seconds): 0.0753
MLP throughput (in TFLOP/s): 236.284
Transformer duration (in seconds): 0.1549
Transformer throughput (in TFLOP/s): 177.239
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0626
Attention throughput (in TFLOP/s): 157.708
MLP duration (in seconds): 0.0756
MLP throughput (in TFLOP/s): 240.541
Transformer duration (in seconds): 0.1418
Transformer throughput (in TFLOP/s): 197.871
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0773
Attention throughput (in TFLOP/s): 130.544
MLP duration (in seconds): 0.0791
MLP throughput (in TFLOP/s): 234.716
Transformer duration (in seconds): 0.1609
Transformer throughput (in TFLOP/s): 178.148
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0641
Attention throughput (in TFLOP/s): 160.654
MLP duration (in seconds): 0.0776
MLP throughput (in TFLOP/s): 244.472
Transformer duration (in seconds): 0.1452
Transformer throughput (in TFLOP/s): 201.538
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0790
Attention throughput (in TFLOP/s): 132.946
MLP duration (in seconds): 0.0810
MLP throughput (in TFLOP/s): 239.125
Transformer duration (in seconds): 0.1645
Transformer throughput (in TFLOP/s): 181.708
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0633
Attention throughput (in TFLOP/s): 169.467
MLP duration (in seconds): 0.0808
MLP throughput (in TFLOP/s): 244.825
Transformer duration (in seconds): 0.1481
Transformer throughput (in TFLOP/s): 206.023
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0824
Attention throughput (in TFLOP/s): 132.739
MLP duration (in seconds): 0.0847
MLP throughput (in TFLOP/s): 238.532
Transformer duration (in seconds): 0.1715
Transformer throughput (in TFLOP/s): 181.628
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0687
Attention throughput (in TFLOP/s): 162.283
MLP duration (in seconds): 0.0839
MLP throughput (in TFLOP/s): 245.734
Transformer duration (in seconds): 0.1564
Transformer throughput (in TFLOP/s): 203.142
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0848
Attention throughput (in TFLOP/s): 134.102
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 238.595
Transformer duration (in seconds): 0.1780
Transformer throughput (in TFLOP/s): 182.167
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0700
Attention throughput (in TFLOP/s): 165.655
MLP duration (in seconds): 0.0873
MLP throughput (in TFLOP/s): 245.929
Transformer duration (in seconds): 0.1613
Transformer throughput (in TFLOP/s): 205.024
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0870
Attention throughput (in TFLOP/s): 135.932
MLP duration (in seconds): 0.0891
MLP throughput (in TFLOP/s): 245.806
Transformer duration (in seconds): 0.1812
Transformer throughput (in TFLOP/s): 186.180
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 128, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0719
Attention throughput (in TFLOP/s): 167.568
MLP duration (in seconds): 0.0907
MLP throughput (in TFLOP/s): 246.287
Transformer duration (in seconds): 0.1667
Transformer throughput (in TFLOP/s): 206.316
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0896
Attention throughput (in TFLOP/s): 137.001
MLP duration (in seconds): 0.0931
MLP throughput (in TFLOP/s): 244.659
Transformer duration (in seconds): 0.1870
Transformer throughput (in TFLOP/s): 187.486
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0722
Attention throughput (in TFLOP/s): 173.249
MLP duration (in seconds): 0.0949
MLP throughput (in TFLOP/s): 244.724
Transformer duration (in seconds): 0.1712
Transformer throughput (in TFLOP/s): 208.674
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0927
Attention throughput (in TFLOP/s): 137.444
MLP duration (in seconds): 0.0968
MLP throughput (in TFLOP/s): 244.646
Transformer duration (in seconds): 0.1936
Transformer throughput (in TFLOP/s): 188.118
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0764
Attention throughput (in TFLOP/s): 169.916
MLP duration (in seconds): 0.0984
MLP throughput (in TFLOP/s): 245.188
Transformer duration (in seconds): 0.1792
Transformer throughput (in TFLOP/s): 207.048
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0944
Attention throughput (in TFLOP/s): 139.983
MLP duration (in seconds): 0.1002
MLP throughput (in TFLOP/s): 245.272
Transformer duration (in seconds): 0.1977
Transformer throughput (in TFLOP/s): 191.199
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0773
Attention throughput (in TFLOP/s): 174.113
MLP duration (in seconds): 0.1008
MLP throughput (in TFLOP/s): 248.455
Transformer duration (in seconds): 0.1830
Transformer throughput (in TFLOP/s): 210.333
Transformer - MLP - Attention (in seconds): 0.0050
========================================================================================================================
num_attention_heads: 128, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0956
Attention throughput (in TFLOP/s): 143.175
MLP duration (in seconds): 0.1037
MLP throughput (in TFLOP/s): 246.051
Transformer duration (in seconds): 0.2037
Transformer throughput (in TFLOP/s): 192.493
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0794
Attention throughput (in TFLOP/s): 175.533
MLP duration (in seconds): 0.1056
MLP throughput (in TFLOP/s): 246.141
Transformer duration (in seconds): 0.1900
Transformer throughput (in TFLOP/s): 210.076
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 128, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0982
Attention throughput (in TFLOP/s): 144.412
MLP duration (in seconds): 0.1078
MLP throughput (in TFLOP/s): 245.528
Transformer duration (in seconds): 0.2092
Transformer throughput (in TFLOP/s): 194.305
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0795
Attention throughput (in TFLOP/s): 181.512
MLP duration (in seconds): 0.1090
MLP throughput (in TFLOP/s): 247.227
Transformer duration (in seconds): 0.1931
Transformer throughput (in TFLOP/s): 214.213
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1005
Attention throughput (in TFLOP/s): 146.064
MLP duration (in seconds): 0.1115
MLP throughput (in TFLOP/s): 245.822
Transformer duration (in seconds): 0.2153
Transformer throughput (in TFLOP/s): 195.556
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0836
Attention throughput (in TFLOP/s): 178.662
MLP duration (in seconds): 0.1138
MLP throughput (in TFLOP/s): 245.279
Transformer duration (in seconds): 0.2021
Transformer throughput (in TFLOP/s): 211.950
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1024
Attention throughput (in TFLOP/s): 148.315
MLP duration (in seconds): 0.1164
MLP throughput (in TFLOP/s): 243.930
Transformer duration (in seconds): 0.2230
Transformer throughput (in TFLOP/s): 195.432
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0859
Attention throughput (in TFLOP/s): 179.725
MLP duration (in seconds): 0.1180
MLP throughput (in TFLOP/s): 244.886
Transformer duration (in seconds): 0.2080
Transformer throughput (in TFLOP/s): 213.201
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1041
Attention throughput (in TFLOP/s): 150.889
MLP duration (in seconds): 0.1191
MLP throughput (in TFLOP/s): 246.722
Transformer duration (in seconds): 0.2271
Transformer throughput (in TFLOP/s): 198.611
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0874
Attention throughput (in TFLOP/s): 182.682
MLP duration (in seconds): 0.1212
MLP throughput (in TFLOP/s): 246.748
Transformer duration (in seconds): 0.2129
Transformer throughput (in TFLOP/s): 215.456
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1061
Attention throughput (in TFLOP/s): 152.933
MLP duration (in seconds): 0.1231
MLP throughput (in TFLOP/s): 246.971
Transformer duration (in seconds): 0.2341
Transformer throughput (in TFLOP/s): 199.214
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0872
Attention throughput (in TFLOP/s): 189.179
MLP duration (in seconds): 0.1256
MLP throughput (in TFLOP/s): 246.166
Transformer duration (in seconds): 0.2180
Transformer throughput (in TFLOP/s): 217.504
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 128, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1082
Attention throughput (in TFLOP/s): 154.934
MLP duration (in seconds): 0.1285
MLP throughput (in TFLOP/s): 244.753
Transformer duration (in seconds): 0.2415
Transformer throughput (in TFLOP/s): 199.607
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0926
Attention throughput (in TFLOP/s): 183.976
MLP duration (in seconds): 0.1304
MLP throughput (in TFLOP/s): 245.157
Transformer duration (in seconds): 0.2276
Transformer throughput (in TFLOP/s): 215.218
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1106
Attention throughput (in TFLOP/s): 156.446
MLP duration (in seconds): 0.1326
MLP throughput (in TFLOP/s): 244.972
Transformer duration (in seconds): 0.2484
Transformer throughput (in TFLOP/s): 200.431
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 128, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0941
Attention throughput (in TFLOP/s): 186.791
MLP duration (in seconds): 0.1346
MLP throughput (in TFLOP/s): 245.404
Transformer duration (in seconds): 0.2331
Transformer throughput (in TFLOP/s): 217.014
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1137
Attention throughput (in TFLOP/s): 157.037
MLP duration (in seconds): 0.1359
MLP throughput (in TFLOP/s): 246.986
Transformer duration (in seconds): 0.2526
Transformer throughput (in TFLOP/s): 203.508
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0947
Attention throughput (in TFLOP/s): 191.522
MLP duration (in seconds): 0.1384
MLP throughput (in TFLOP/s): 246.295
Transformer duration (in seconds): 0.2386
Transformer throughput (in TFLOP/s): 218.837
Transformer - MLP - Attention (in seconds): 0.0056
========================================================================================================================
num_attention_heads: 128, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1155
Attention throughput (in TFLOP/s): 159.414
MLP duration (in seconds): 0.1409
MLP throughput (in TFLOP/s): 245.765
Transformer duration (in seconds): 0.2606
Transformer throughput (in TFLOP/s): 203.556
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0958
Attention throughput (in TFLOP/s): 195.086
MLP duration (in seconds): 0.1435
MLP throughput (in TFLOP/s): 245.133
Transformer duration (in seconds): 0.2459
Transformer throughput (in TFLOP/s): 219.072
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 128, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1229
Attention throughput (in TFLOP/s): 154.415
MLP duration (in seconds): 0.1455
MLP throughput (in TFLOP/s): 245.565
Transformer duration (in seconds): 0.2722
Transformer throughput (in TFLOP/s): 201.003
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1031
Attention throughput (in TFLOP/s): 186.821
MLP duration (in seconds): 0.1480
MLP throughput (in TFLOP/s): 245.232
Transformer duration (in seconds): 0.2564
Transformer throughput (in TFLOP/s): 216.699
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 128, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1259
Attention throughput (in TFLOP/s): 155.348
MLP duration (in seconds): 0.1503
MLP throughput (in TFLOP/s): 245.159
Transformer duration (in seconds): 0.2799
Transformer throughput (in TFLOP/s): 201.540
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1055
Attention throughput (in TFLOP/s): 188.149
MLP duration (in seconds): 0.1532
MLP throughput (in TFLOP/s): 244.254
Transformer duration (in seconds): 0.2632
Transformer throughput (in TFLOP/s): 217.592
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1272
Attention throughput (in TFLOP/s): 158.327
MLP duration (in seconds): 0.1539
MLP throughput (in TFLOP/s): 246.759
Transformer duration (in seconds): 0.2864
Transformer throughput (in TFLOP/s): 202.954
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 128, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1075
Attention throughput (in TFLOP/s): 190.007
MLP duration (in seconds): 0.1567
MLP throughput (in TFLOP/s): 246.135
Transformer duration (in seconds): 0.2690
Transformer throughput (in TFLOP/s): 219.299
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1312
Attention throughput (in TFLOP/s): 158.007
MLP duration (in seconds): 0.1596
MLP throughput (in TFLOP/s): 245.172
Transformer duration (in seconds): 0.2970
Transformer throughput (in TFLOP/s): 201.578
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 128, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1083
Attention throughput (in TFLOP/s): 194.115
MLP duration (in seconds): 0.1618
MLP throughput (in TFLOP/s): 245.473
Transformer duration (in seconds): 0.2744
Transformer throughput (in TFLOP/s): 221.380
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1346
Attention throughput (in TFLOP/s): 158.467
MLP duration (in seconds): 0.1647
MLP throughput (in TFLOP/s): 244.704
Transformer duration (in seconds): 0.3047
Transformer throughput (in TFLOP/s): 202.271
Transformer - MLP - Attention (in seconds): 0.0054
========================================================================================================================
num_attention_heads: 128, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1139
Attention throughput (in TFLOP/s): 189.962
MLP duration (in seconds): 0.1673
MLP throughput (in TFLOP/s): 244.392
Transformer duration (in seconds): 0.2860
Transformer throughput (in TFLOP/s): 218.657
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1379
Attention throughput (in TFLOP/s): 159.139
MLP duration (in seconds): 0.1691
MLP throughput (in TFLOP/s): 245.315
Transformer duration (in seconds): 0.3118
Transformer throughput (in TFLOP/s): 203.405
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1157
Attention throughput (in TFLOP/s): 192.285
MLP duration (in seconds): 0.1723
MLP throughput (in TFLOP/s): 244.292
Transformer duration (in seconds): 0.2923
Transformer throughput (in TFLOP/s): 220.143
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1400
Attention throughput (in TFLOP/s): 161.118
MLP duration (in seconds): 0.1742
MLP throughput (in TFLOP/s): 245.037
Transformer duration (in seconds): 0.3196
Transformer throughput (in TFLOP/s): 204.182
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 128, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1184
Attention throughput (in TFLOP/s): 193.169
MLP duration (in seconds): 0.1766
MLP throughput (in TFLOP/s): 245.139
Transformer duration (in seconds): 0.3003
Transformer throughput (in TFLOP/s): 220.378
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 128, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1420
Attention throughput (in TFLOP/s): 163.254
MLP duration (in seconds): 0.1799
MLP throughput (in TFLOP/s): 244.069
Transformer duration (in seconds): 0.3253
Transformer throughput (in TFLOP/s): 206.282
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1181
Attention throughput (in TFLOP/s): 198.990
MLP duration (in seconds): 0.1816
MLP throughput (in TFLOP/s): 245.224
Transformer duration (in seconds): 0.3030
Transformer throughput (in TFLOP/s): 224.538
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1317
Attention throughput (in TFLOP/s): 180.836
MLP duration (in seconds): 0.1834
MLP throughput (in TFLOP/s): 246.215
Transformer duration (in seconds): 0.3198
Transformer throughput (in TFLOP/s): 215.657
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1224
Attention throughput (in TFLOP/s): 197.243
MLP duration (in seconds): 0.1870
MLP throughput (in TFLOP/s): 244.775
Transformer duration (in seconds): 0.3124
Transformer throughput (in TFLOP/s): 223.796
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1343
Attention throughput (in TFLOP/s): 182.147
MLP duration (in seconds): 0.1892
MLP throughput (in TFLOP/s): 245.207
Transformer duration (in seconds): 0.3277
Transformer throughput (in TFLOP/s): 216.262
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1239
Attention throughput (in TFLOP/s): 200.075
MLP duration (in seconds): 0.1913
MLP throughput (in TFLOP/s): 245.838
Transformer duration (in seconds): 0.3190
Transformer throughput (in TFLOP/s): 225.153
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1366
Attention throughput (in TFLOP/s): 183.829
MLP duration (in seconds): 0.1936
MLP throughput (in TFLOP/s): 246.250
Transformer duration (in seconds): 0.3341
Transformer throughput (in TFLOP/s): 217.907
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1270
Attention throughput (in TFLOP/s): 200.435
MLP duration (in seconds): 0.1959
MLP throughput (in TFLOP/s): 246.613
Transformer duration (in seconds): 0.3278
Transformer throughput (in TFLOP/s): 225.033
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1405
Attention throughput (in TFLOP/s): 183.449
MLP duration (in seconds): 0.1995
MLP throughput (in TFLOP/s): 245.459
Transformer duration (in seconds): 0.3442
Transformer throughput (in TFLOP/s): 217.136
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1288
Attention throughput (in TFLOP/s): 202.790
MLP duration (in seconds): 0.2026
MLP throughput (in TFLOP/s): 244.929
Transformer duration (in seconds): 0.3348
Transformer throughput (in TFLOP/s): 226.191
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1419
Attention throughput (in TFLOP/s): 186.335
MLP duration (in seconds): 0.2045
MLP throughput (in TFLOP/s): 245.784
Transformer duration (in seconds): 0.3498
Transformer throughput (in TFLOP/s): 219.303
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1314
Attention throughput (in TFLOP/s): 203.822
MLP duration (in seconds): 0.2065
MLP throughput (in TFLOP/s): 246.623
Transformer duration (in seconds): 0.3421
Transformer throughput (in TFLOP/s): 227.181
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1433
Attention throughput (in TFLOP/s): 189.322
MLP duration (in seconds): 0.2092
MLP throughput (in TFLOP/s): 246.579
Transformer duration (in seconds): 0.3565
Transformer throughput (in TFLOP/s): 220.825
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1339
Attention throughput (in TFLOP/s): 205.178
MLP duration (in seconds): 0.2127
MLP throughput (in TFLOP/s): 245.761
Transformer duration (in seconds): 0.3514
Transformer throughput (in TFLOP/s): 226.908
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1464
Attention throughput (in TFLOP/s): 189.968
MLP duration (in seconds): 0.2159
MLP throughput (in TFLOP/s): 245.197
Transformer duration (in seconds): 0.3672
Transformer throughput (in TFLOP/s): 219.901
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1380
Attention throughput (in TFLOP/s): 204.014
MLP duration (in seconds): 0.2187
MLP throughput (in TFLOP/s): 245.133
Transformer duration (in seconds): 0.3613
Transformer throughput (in TFLOP/s): 226.308
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1487
Attention throughput (in TFLOP/s): 191.798
MLP duration (in seconds): 0.2206
MLP throughput (in TFLOP/s): 246.067
Transformer duration (in seconds): 0.3740
Transformer throughput (in TFLOP/s): 221.411
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1375
Attention throughput (in TFLOP/s): 209.973
MLP duration (in seconds): 0.2237
MLP throughput (in TFLOP/s): 245.802
Transformer duration (in seconds): 0.3667
Transformer throughput (in TFLOP/s): 228.612
Transformer - MLP - Attention (in seconds): 0.0056
========================================================================================================================
num_attention_heads: 128, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1537
Attention throughput (in TFLOP/s): 190.132
MLP duration (in seconds): 0.2879
MLP throughput (in TFLOP/s): 193.350
Transformer duration (in seconds): 0.4482
Transformer throughput (in TFLOP/s): 189.392
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 128, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1429
Attention throughput (in TFLOP/s): 206.978
MLP duration (in seconds): 0.2824
MLP throughput (in TFLOP/s): 199.582
Transformer duration (in seconds): 0.4349
Transformer throughput (in TFLOP/s): 197.584
Transformer - MLP - Attention (in seconds): 0.0096
========================================================================================================================
num_attention_heads: 128, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1555
Attention throughput (in TFLOP/s): 192.418
MLP duration (in seconds): 0.2948
MLP throughput (in TFLOP/s): 193.537
Transformer duration (in seconds): 0.4580
Transformer throughput (in TFLOP/s): 189.903
Transformer - MLP - Attention (in seconds): 0.0077
========================================================================================================================
num_attention_heads: 128, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1467
Attention throughput (in TFLOP/s): 206.463
MLP duration (in seconds): 0.2995
MLP throughput (in TFLOP/s): 192.878
Transformer duration (in seconds): 0.4559
Transformer throughput (in TFLOP/s): 193.132
Transformer - MLP - Attention (in seconds): 0.0097
========================================================================================================================
num_attention_heads: 128, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1599
Attention throughput (in TFLOP/s): 191.721
MLP duration (in seconds): 0.3106
MLP throughput (in TFLOP/s): 188.258
Transformer duration (in seconds): 0.4745
Transformer throughput (in TFLOP/s): 187.793
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1495
Attention throughput (in TFLOP/s): 207.388
MLP duration (in seconds): 0.3089
MLP throughput (in TFLOP/s): 191.548
Transformer duration (in seconds): 0.4625
Transformer throughput (in TFLOP/s): 195.013
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1619
Attention throughput (in TFLOP/s): 193.771
MLP duration (in seconds): 0.3092
MLP throughput (in TFLOP/s): 193.698
Transformer duration (in seconds): 0.4817
Transformer throughput (in TFLOP/s): 189.493
Transformer - MLP - Attention (in seconds): 0.0105
========================================================================================================================
num_attention_heads: 128, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1516
Attention throughput (in TFLOP/s): 209.400
MLP duration (in seconds): 0.3135
MLP throughput (in TFLOP/s): 193.322
Transformer duration (in seconds): 0.4717
Transformer throughput (in TFLOP/s): 195.800
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 128, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1654
Attention throughput (in TFLOP/s): 194.223
MLP duration (in seconds): 0.3189
MLP throughput (in TFLOP/s): 192.312
Transformer duration (in seconds): 0.4878
Transformer throughput (in TFLOP/s): 191.561
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1551
Attention throughput (in TFLOP/s): 209.437
MLP duration (in seconds): 0.3173
MLP throughput (in TFLOP/s): 195.597
Transformer duration (in seconds): 0.4772
Transformer throughput (in TFLOP/s): 198.143
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1680
Attention throughput (in TFLOP/s): 195.652
MLP duration (in seconds): 0.3265
MLP throughput (in TFLOP/s): 192.329
Transformer duration (in seconds): 0.5023
Transformer throughput (in TFLOP/s): 190.435
Transformer - MLP - Attention (in seconds): 0.0078
========================================================================================================================
num_attention_heads: 128, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1578
Attention throughput (in TFLOP/s): 210.696
MLP duration (in seconds): 0.3340
MLP throughput (in TFLOP/s): 190.210
Transformer duration (in seconds): 0.4954
Transformer throughput (in TFLOP/s): 195.351
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1706
Attention throughput (in TFLOP/s): 197.076
MLP duration (in seconds): 0.3341
MLP throughput (in TFLOP/s): 192.345
Transformer duration (in seconds): 0.5143
Transformer throughput (in TFLOP/s): 190.329
Transformer - MLP - Attention (in seconds): 0.0096
========================================================================================================================
num_attention_heads: 128, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1617
Attention throughput (in TFLOP/s): 210.233
MLP duration (in seconds): 0.3372
MLP throughput (in TFLOP/s): 192.796
Transformer duration (in seconds): 0.5053
Transformer throughput (in TFLOP/s): 195.960
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 128, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1747
Attention throughput (in TFLOP/s): 196.820
MLP duration (in seconds): 0.2740
MLP throughput (in TFLOP/s): 240.002
Transformer duration (in seconds): 0.4528
Transformer throughput (in TFLOP/s): 221.173
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1632
Attention throughput (in TFLOP/s): 213.074
MLP duration (in seconds): 0.2768
MLP throughput (in TFLOP/s): 240.297
Transformer duration (in seconds): 0.4441
Transformer throughput (in TFLOP/s): 228.060
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1778
Attention throughput (in TFLOP/s): 197.776
MLP duration (in seconds): 0.2791
MLP throughput (in TFLOP/s): 241.094
Transformer duration (in seconds): 0.4619
Transformer throughput (in TFLOP/s): 221.752
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 128, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1682
Attention throughput (in TFLOP/s): 211.377
MLP duration (in seconds): 0.2826
MLP throughput (in TFLOP/s): 240.763
Transformer duration (in seconds): 0.4544
Transformer throughput (in TFLOP/s): 227.972
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1802
Attention throughput (in TFLOP/s): 199.447
MLP duration (in seconds): 0.2828
MLP throughput (in TFLOP/s): 243.325
Transformer duration (in seconds): 0.4709
Transformer throughput (in TFLOP/s): 222.425
Transformer - MLP - Attention (in seconds): 0.0080
========================================================================================================================
num_attention_heads: 128, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1735
Attention throughput (in TFLOP/s): 209.441
MLP duration (in seconds): 0.2869
MLP throughput (in TFLOP/s): 242.511
Transformer duration (in seconds): 0.4648
Transformer throughput (in TFLOP/s): 227.849
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1859
Attention throughput (in TFLOP/s): 197.548
MLP duration (in seconds): 0.2900
MLP throughput (in TFLOP/s): 242.605
Transformer duration (in seconds): 0.4810
Transformer throughput (in TFLOP/s): 222.629
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================

1.13.1 

[2023-09-28 22:10:54,686] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-09-28 22:10:55,433] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.150.175, master_port=6000
[2023-09-28 22:10:55,434] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-09-28 22:10:58,543] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 128, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0392
Attention throughput (in TFLOP/s): 63.040
MLP duration (in seconds): 0.0371
MLP throughput (in TFLOP/s): 118.563
Transformer duration (in seconds): 0.0794
Transformer throughput (in TFLOP/s): 86.560
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0536
Attention throughput (in TFLOP/s): 47.539
MLP duration (in seconds): 0.0385
MLP throughput (in TFLOP/s): 117.884
Transformer duration (in seconds): 0.0961
Transformer throughput (in TFLOP/s): 73.691
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0548
Attention throughput (in TFLOP/s): 47.884
MLP duration (in seconds): 0.0398
MLP throughput (in TFLOP/s): 117.490
Transformer duration (in seconds): 0.0986
Transformer throughput (in TFLOP/s): 74.014
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0553
Attention throughput (in TFLOP/s): 48.791
MLP duration (in seconds): 0.0410
MLP throughput (in TFLOP/s): 117.641
Transformer duration (in seconds): 0.1008
Transformer throughput (in TFLOP/s): 74.613
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0571
Attention throughput (in TFLOP/s): 48.594
MLP duration (in seconds): 0.0419
MLP throughput (in TFLOP/s): 118.599
Transformer duration (in seconds): 0.1026
Transformer throughput (in TFLOP/s): 75.442
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0577
Attention throughput (in TFLOP/s): 49.462
MLP duration (in seconds): 0.0431
MLP throughput (in TFLOP/s): 118.561
Transformer duration (in seconds): 0.1045
Transformer throughput (in TFLOP/s): 76.200
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0597
Attention throughput (in TFLOP/s): 49.089
MLP duration (in seconds): 0.0429
MLP throughput (in TFLOP/s): 122.729
Transformer duration (in seconds): 0.1058
Transformer throughput (in TFLOP/s): 77.426
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0609
Attention throughput (in TFLOP/s): 49.470
MLP duration (in seconds): 0.0446
MLP throughput (in TFLOP/s): 121.366
Transformer duration (in seconds): 0.1079
Transformer throughput (in TFLOP/s): 78.077
Transformer - MLP - Attention (in seconds): 0.0024
========================================================================================================================
num_attention_heads: 128, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0453
Attention throughput (in TFLOP/s): 68.211
MLP duration (in seconds): 0.0469
MLP throughput (in TFLOP/s): 118.666
Transformer duration (in seconds): 0.0956
Transformer throughput (in TFLOP/s): 90.598
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0610
Attention throughput (in TFLOP/s): 52.044
MLP duration (in seconds): 0.0484
MLP throughput (in TFLOP/s): 118.217
Transformer duration (in seconds): 0.1138
Transformer throughput (in TFLOP/s): 78.176
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0494
Attention throughput (in TFLOP/s): 65.899
MLP duration (in seconds): 0.0504
MLP throughput (in TFLOP/s): 116.680
Transformer duration (in seconds): 0.1031
Transformer throughput (in TFLOP/s): 88.655
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0626
Attention throughput (in TFLOP/s): 53.370
MLP duration (in seconds): 0.0519
MLP throughput (in TFLOP/s): 116.454
Transformer duration (in seconds): 0.1190
Transformer throughput (in TFLOP/s): 78.846
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0504
Attention throughput (in TFLOP/s): 68.005
MLP duration (in seconds): 0.0516
MLP throughput (in TFLOP/s): 120.289
Transformer duration (in seconds): 0.1051
Transformer throughput (in TFLOP/s): 91.626
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0643
Attention throughput (in TFLOP/s): 54.673
MLP duration (in seconds): 0.0547
MLP throughput (in TFLOP/s): 116.477
Transformer duration (in seconds): 0.1228
Transformer throughput (in TFLOP/s): 80.442
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0521
Attention throughput (in TFLOP/s): 69.153
MLP duration (in seconds): 0.0544
MLP throughput (in TFLOP/s): 120.173
Transformer duration (in seconds): 0.1093
Transformer throughput (in TFLOP/s): 92.735
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0659
Attention throughput (in TFLOP/s): 55.978
MLP duration (in seconds): 0.0575
MLP throughput (in TFLOP/s): 116.511
Transformer duration (in seconds): 0.1275
Transformer throughput (in TFLOP/s): 81.481
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0512
Attention throughput (in TFLOP/s): 73.834
MLP duration (in seconds): 0.0569
MLP throughput (in TFLOP/s): 120.723
Transformer duration (in seconds): 0.1112
Transformer throughput (in TFLOP/s): 95.801
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0674
Attention throughput (in TFLOP/s): 57.425
MLP duration (in seconds): 0.0604
MLP throughput (in TFLOP/s): 116.542
Transformer duration (in seconds): 0.1320
Transformer throughput (in TFLOP/s): 82.699
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0552
Attention throughput (in TFLOP/s): 71.797
MLP duration (in seconds): 0.0617
MLP throughput (in TFLOP/s): 116.929
Transformer duration (in seconds): 0.1207
Transformer throughput (in TFLOP/s): 92.654
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0689
Attention throughput (in TFLOP/s): 58.843
MLP duration (in seconds): 0.0633
MLP throughput (in TFLOP/s): 116.829
Transformer duration (in seconds): 0.1366
Transformer throughput (in TFLOP/s): 83.837
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0565
Attention throughput (in TFLOP/s): 73.470
MLP duration (in seconds): 0.0628
MLP throughput (in TFLOP/s): 120.638
Transformer duration (in seconds): 0.1226
Transformer throughput (in TFLOP/s): 95.673
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0709
Attention throughput (in TFLOP/s): 59.883
MLP duration (in seconds): 0.0664
MLP throughput (in TFLOP/s): 116.790
Transformer duration (in seconds): 0.1415
Transformer throughput (in TFLOP/s): 84.810
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0580
Attention throughput (in TFLOP/s): 74.880
MLP duration (in seconds): 0.0658
MLP throughput (in TFLOP/s): 120.648
Transformer duration (in seconds): 0.1273
Transformer throughput (in TFLOP/s): 96.511
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0722
Attention throughput (in TFLOP/s): 61.470
MLP duration (in seconds): 0.0695
MLP throughput (in TFLOP/s): 116.911
Transformer duration (in seconds): 0.1460
Transformer throughput (in TFLOP/s): 86.061
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0577
Attention throughput (in TFLOP/s): 78.671
MLP duration (in seconds): 0.0689
MLP throughput (in TFLOP/s): 120.614
Transformer duration (in seconds): 0.1296
Transformer throughput (in TFLOP/s): 99.124
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0740
Attention throughput (in TFLOP/s): 62.620
MLP duration (in seconds): 0.0706
MLP throughput (in TFLOP/s): 120.388
Transformer duration (in seconds): 0.1480
Transformer throughput (in TFLOP/s): 88.758
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0613
Attention throughput (in TFLOP/s): 77.257
MLP duration (in seconds): 0.0742
MLP throughput (in TFLOP/s): 117.140
Transformer duration (in seconds): 0.1396
Transformer throughput (in TFLOP/s): 96.203
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0754
Attention throughput (in TFLOP/s): 64.109
MLP duration (in seconds): 0.0760
MLP throughput (in TFLOP/s): 117.068
Transformer duration (in seconds): 0.1557
Transformer throughput (in TFLOP/s): 88.156
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0627
Attention throughput (in TFLOP/s): 78.715
MLP duration (in seconds): 0.0756
MLP throughput (in TFLOP/s): 120.285
Transformer duration (in seconds): 0.1413
Transformer throughput (in TFLOP/s): 99.291
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0773
Attention throughput (in TFLOP/s): 65.255
MLP duration (in seconds): 0.0794
MLP throughput (in TFLOP/s): 117.008
Transformer duration (in seconds): 0.1608
Transformer throughput (in TFLOP/s): 89.131
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0639
Attention throughput (in TFLOP/s): 80.585
MLP duration (in seconds): 0.0775
MLP throughput (in TFLOP/s): 122.390
Transformer duration (in seconds): 0.1449
Transformer throughput (in TFLOP/s): 100.987
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0789
Attention throughput (in TFLOP/s): 66.555
MLP duration (in seconds): 0.0815
MLP throughput (in TFLOP/s): 118.889
Transformer duration (in seconds): 0.1645
Transformer throughput (in TFLOP/s): 90.825
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0632
Attention throughput (in TFLOP/s): 84.826
MLP duration (in seconds): 0.0810
MLP throughput (in TFLOP/s): 122.191
Transformer duration (in seconds): 0.1476
Transformer throughput (in TFLOP/s): 103.325
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0822
Attention throughput (in TFLOP/s): 66.494
MLP duration (in seconds): 0.0849
MLP throughput (in TFLOP/s): 118.987
Transformer duration (in seconds): 0.1717
Transformer throughput (in TFLOP/s): 90.675
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0687
Attention throughput (in TFLOP/s): 81.156
MLP duration (in seconds): 0.0843
MLP throughput (in TFLOP/s): 122.265
Transformer duration (in seconds): 0.1558
Transformer throughput (in TFLOP/s): 101.970
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0845
Attention throughput (in TFLOP/s): 67.271
MLP duration (in seconds): 0.0886
MLP throughput (in TFLOP/s): 118.831
Transformer duration (in seconds): 0.1778
Transformer throughput (in TFLOP/s): 91.198
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0699
Attention throughput (in TFLOP/s): 82.904
MLP duration (in seconds): 0.0875
MLP throughput (in TFLOP/s): 122.658
Transformer duration (in seconds): 0.1609
Transformer throughput (in TFLOP/s): 102.775
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0873
Attention throughput (in TFLOP/s): 67.683
MLP duration (in seconds): 0.0895
MLP throughput (in TFLOP/s): 122.403
Transformer duration (in seconds): 0.1803
Transformer throughput (in TFLOP/s): 93.537
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0719
Attention throughput (in TFLOP/s): 83.811
MLP duration (in seconds): 0.0911
MLP throughput (in TFLOP/s): 122.683
Transformer duration (in seconds): 0.1664
Transformer throughput (in TFLOP/s): 103.310
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0899
Attention throughput (in TFLOP/s): 68.295
MLP duration (in seconds): 0.0931
MLP throughput (in TFLOP/s): 122.327
Transformer duration (in seconds): 0.1866
Transformer throughput (in TFLOP/s): 93.959
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0722
Attention throughput (in TFLOP/s): 86.637
MLP duration (in seconds): 0.0950
MLP throughput (in TFLOP/s): 122.218
Transformer duration (in seconds): 0.1711
Transformer throughput (in TFLOP/s): 104.437
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0928
Attention throughput (in TFLOP/s): 68.620
MLP duration (in seconds): 0.0971
MLP throughput (in TFLOP/s): 121.936
Transformer duration (in seconds): 0.1932
Transformer throughput (in TFLOP/s): 94.243
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0764
Attention throughput (in TFLOP/s): 84.959
MLP duration (in seconds): 0.0986
MLP throughput (in TFLOP/s): 122.361
Transformer duration (in seconds): 0.1783
Transformer throughput (in TFLOP/s): 104.066
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0939
Attention throughput (in TFLOP/s): 70.324
MLP duration (in seconds): 0.1005
MLP throughput (in TFLOP/s): 122.270
Transformer duration (in seconds): 0.1985
Transformer throughput (in TFLOP/s): 95.217
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0776
Attention throughput (in TFLOP/s): 86.693
MLP duration (in seconds): 0.1021
MLP throughput (in TFLOP/s): 122.697
Transformer duration (in seconds): 0.1831
Transformer throughput (in TFLOP/s): 105.121
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0959
Attention throughput (in TFLOP/s): 71.414
MLP duration (in seconds): 0.1043
MLP throughput (in TFLOP/s): 122.358
Transformer duration (in seconds): 0.2036
Transformer throughput (in TFLOP/s): 96.276
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0796
Attention throughput (in TFLOP/s): 87.591
MLP duration (in seconds): 0.1062
MLP throughput (in TFLOP/s): 122.306
Transformer duration (in seconds): 0.1895
Transformer throughput (in TFLOP/s): 105.344
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0978
Attention throughput (in TFLOP/s): 72.522
MLP duration (in seconds): 0.1080
MLP throughput (in TFLOP/s): 122.492
Transformer duration (in seconds): 0.2102
Transformer throughput (in TFLOP/s): 96.690
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0799
Attention throughput (in TFLOP/s): 90.271
MLP duration (in seconds): 0.1100
MLP throughput (in TFLOP/s): 122.412
Transformer duration (in seconds): 0.1940
Transformer throughput (in TFLOP/s): 106.599
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0999
Attention throughput (in TFLOP/s): 73.507
MLP duration (in seconds): 0.1123
MLP throughput (in TFLOP/s): 122.118
Transformer duration (in seconds): 0.2155
Transformer throughput (in TFLOP/s): 97.698
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0835
Attention throughput (in TFLOP/s): 89.387
MLP duration (in seconds): 0.1139
MLP throughput (in TFLOP/s): 122.535
Transformer duration (in seconds): 0.2009
Transformer throughput (in TFLOP/s): 106.600
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1015
Attention throughput (in TFLOP/s): 74.837
MLP duration (in seconds): 0.1162
MLP throughput (in TFLOP/s): 122.204
Transformer duration (in seconds): 0.2218
Transformer throughput (in TFLOP/s): 98.254
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0854
Attention throughput (in TFLOP/s): 90.434
MLP duration (in seconds): 0.1179
MLP throughput (in TFLOP/s): 122.544
Transformer duration (in seconds): 0.2067
Transformer throughput (in TFLOP/s): 107.258
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1034
Attention throughput (in TFLOP/s): 75.933
MLP duration (in seconds): 0.1202
MLP throughput (in TFLOP/s): 122.317
Transformer duration (in seconds): 0.2279
Transformer throughput (in TFLOP/s): 98.935
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0870
Attention throughput (in TFLOP/s): 91.792
MLP duration (in seconds): 0.1223
MLP throughput (in TFLOP/s): 122.227
Transformer duration (in seconds): 0.2127
Transformer throughput (in TFLOP/s): 107.823
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1057
Attention throughput (in TFLOP/s): 76.762
MLP duration (in seconds): 0.1245
MLP throughput (in TFLOP/s): 122.143
Transformer duration (in seconds): 0.2338
Transformer throughput (in TFLOP/s): 99.730
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0878
Attention throughput (in TFLOP/s): 93.943
MLP duration (in seconds): 0.1263
MLP throughput (in TFLOP/s): 122.462
Transformer duration (in seconds): 0.2179
Transformer throughput (in TFLOP/s): 108.815
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1081
Attention throughput (in TFLOP/s): 77.513
MLP duration (in seconds): 0.1290
MLP throughput (in TFLOP/s): 121.870
Transformer duration (in seconds): 0.2411
Transformer throughput (in TFLOP/s): 99.971
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0921
Attention throughput (in TFLOP/s): 92.490
MLP duration (in seconds): 0.1310
MLP throughput (in TFLOP/s): 122.006
Transformer duration (in seconds): 0.2264
Transformer throughput (in TFLOP/s): 108.208
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1103
Attention throughput (in TFLOP/s): 78.443
MLP duration (in seconds): 0.1332
MLP throughput (in TFLOP/s): 121.944
Transformer duration (in seconds): 0.2472
Transformer throughput (in TFLOP/s): 100.703
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0938
Attention throughput (in TFLOP/s): 93.681
MLP duration (in seconds): 0.1352
MLP throughput (in TFLOP/s): 122.108
Transformer duration (in seconds): 0.2331
Transformer throughput (in TFLOP/s): 108.540
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1126
Attention throughput (in TFLOP/s): 79.279
MLP duration (in seconds): 0.1371
MLP throughput (in TFLOP/s): 122.338
Transformer duration (in seconds): 0.2538
Transformer throughput (in TFLOP/s): 101.278
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0957
Attention throughput (in TFLOP/s): 94.671
MLP duration (in seconds): 0.1395
MLP throughput (in TFLOP/s): 122.191
Transformer duration (in seconds): 0.2392
Transformer throughput (in TFLOP/s): 109.152
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1146
Attention throughput (in TFLOP/s): 80.307
MLP duration (in seconds): 0.1418
MLP throughput (in TFLOP/s): 122.097
Transformer duration (in seconds): 0.2613
Transformer throughput (in TFLOP/s): 101.488
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0965
Attention throughput (in TFLOP/s): 96.882
MLP duration (in seconds): 0.1446
MLP throughput (in TFLOP/s): 121.665
Transformer duration (in seconds): 0.2444
Transformer throughput (in TFLOP/s): 110.221
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1223
Attention throughput (in TFLOP/s): 77.554
MLP duration (in seconds): 0.1461
MLP throughput (in TFLOP/s): 122.292
Transformer duration (in seconds): 0.2723
Transformer throughput (in TFLOP/s): 100.469
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1031
Attention throughput (in TFLOP/s): 93.451
MLP duration (in seconds): 0.1488
MLP throughput (in TFLOP/s): 121.982
Transformer duration (in seconds): 0.2558
Transformer throughput (in TFLOP/s): 108.585
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1256
Attention throughput (in TFLOP/s): 77.804
MLP duration (in seconds): 0.1516
MLP throughput (in TFLOP/s): 121.576
Transformer duration (in seconds): 0.2804
Transformer throughput (in TFLOP/s): 100.595
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1057
Attention throughput (in TFLOP/s): 93.904
MLP duration (in seconds): 0.1540
MLP throughput (in TFLOP/s): 121.495
Transformer duration (in seconds): 0.2633
Transformer throughput (in TFLOP/s): 108.735
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1288
Attention throughput (in TFLOP/s): 78.156
MLP duration (in seconds): 0.1561
MLP throughput (in TFLOP/s): 121.659
Transformer duration (in seconds): 0.2891
Transformer throughput (in TFLOP/s): 100.524
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1086
Attention throughput (in TFLOP/s): 94.091
MLP duration (in seconds): 0.1584
MLP throughput (in TFLOP/s): 121.719
Transformer duration (in seconds): 0.2702
Transformer throughput (in TFLOP/s): 109.179
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1323
Attention throughput (in TFLOP/s): 78.351
MLP duration (in seconds): 0.1606
MLP throughput (in TFLOP/s): 121.871
Transformer duration (in seconds): 0.2963
Transformer throughput (in TFLOP/s): 101.007
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1089
Attention throughput (in TFLOP/s): 96.536
MLP duration (in seconds): 0.1631
MLP throughput (in TFLOP/s): 121.740
Transformer duration (in seconds): 0.2752
Transformer throughput (in TFLOP/s): 110.353
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1348
Attention throughput (in TFLOP/s): 79.090
MLP duration (in seconds): 0.1657
MLP throughput (in TFLOP/s): 121.614
Transformer duration (in seconds): 0.3040
Transformer throughput (in TFLOP/s): 101.380
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1141
Attention throughput (in TFLOP/s): 94.827
MLP duration (in seconds): 0.1684
MLP throughput (in TFLOP/s): 121.423
Transformer duration (in seconds): 0.2857
Transformer throughput (in TFLOP/s): 109.449
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1375
Attention throughput (in TFLOP/s): 79.804
MLP duration (in seconds): 0.1703
MLP throughput (in TFLOP/s): 121.840
Transformer duration (in seconds): 0.3120
Transformer throughput (in TFLOP/s): 101.657
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1152
Attention throughput (in TFLOP/s): 96.557
MLP duration (in seconds): 0.1724
MLP throughput (in TFLOP/s): 122.077
Transformer duration (in seconds): 0.2913
Transformer throughput (in TFLOP/s): 110.425
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1396
Attention throughput (in TFLOP/s): 80.768
MLP duration (in seconds): 0.1746
MLP throughput (in TFLOP/s): 122.236
Transformer duration (in seconds): 0.3170
Transformer throughput (in TFLOP/s): 102.908
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1180
Attention throughput (in TFLOP/s): 96.883
MLP duration (in seconds): 0.1779
MLP throughput (in TFLOP/s): 121.691
Transformer duration (in seconds): 0.2996
Transformer throughput (in TFLOP/s): 110.442
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1428
Attention throughput (in TFLOP/s): 81.189
MLP duration (in seconds): 0.1805
MLP throughput (in TFLOP/s): 121.622
Transformer duration (in seconds): 0.3262
Transformer throughput (in TFLOP/s): 102.863
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1177
Attention throughput (in TFLOP/s): 99.839
MLP duration (in seconds): 0.1834
MLP throughput (in TFLOP/s): 121.375
Transformer duration (in seconds): 0.3048
Transformer throughput (in TFLOP/s): 111.592
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1323
Attention throughput (in TFLOP/s): 90.025
MLP duration (in seconds): 0.1855
MLP throughput (in TFLOP/s): 121.669
Transformer duration (in seconds): 0.3210
Transformer throughput (in TFLOP/s): 107.419
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1233
Attention throughput (in TFLOP/s): 97.918
MLP duration (in seconds): 0.1875
MLP throughput (in TFLOP/s): 122.053
Transformer duration (in seconds): 0.3150
Transformer throughput (in TFLOP/s): 110.971
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1352
Attention throughput (in TFLOP/s): 90.482
MLP duration (in seconds): 0.1912
MLP throughput (in TFLOP/s): 121.371
Transformer duration (in seconds): 0.3288
Transformer throughput (in TFLOP/s): 107.774
Transformer - MLP - Attention (in seconds): 0.0024
========================================================================================================================
num_attention_heads: 128, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1260
Attention throughput (in TFLOP/s): 98.390
MLP duration (in seconds): 0.1927
MLP throughput (in TFLOP/s): 122.032
Transformer duration (in seconds): 0.3214
Transformer throughput (in TFLOP/s): 111.760
Transformer - MLP - Attention (in seconds): 0.0026
========================================================================================================================
num_attention_heads: 128, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1375
Attention throughput (in TFLOP/s): 91.365
MLP duration (in seconds): 0.1959
MLP throughput (in TFLOP/s): 121.659
Transformer duration (in seconds): 0.3365
Transformer throughput (in TFLOP/s): 108.161
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1287
Attention throughput (in TFLOP/s): 98.879
MLP duration (in seconds): 0.1977
MLP throughput (in TFLOP/s): 122.170
Transformer duration (in seconds): 0.3294
Transformer throughput (in TFLOP/s): 111.986
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 128, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1398
Attention throughput (in TFLOP/s): 92.227
MLP duration (in seconds): 0.2003
MLP throughput (in TFLOP/s): 122.240
Transformer duration (in seconds): 0.3444
Transformer throughput (in TFLOP/s): 108.529
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1289
Attention throughput (in TFLOP/s): 101.260
MLP duration (in seconds): 0.2029
MLP throughput (in TFLOP/s): 122.257
Transformer duration (in seconds): 0.3351
Transformer throughput (in TFLOP/s): 113.005
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1420
Attention throughput (in TFLOP/s): 93.135
MLP duration (in seconds): 0.2058
MLP throughput (in TFLOP/s): 122.158
Transformer duration (in seconds): 0.3509
Transformer throughput (in TFLOP/s): 109.317
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1332
Attention throughput (in TFLOP/s): 100.582
MLP duration (in seconds): 0.2079
MLP throughput (in TFLOP/s): 122.458
Transformer duration (in seconds): 0.3457
Transformer throughput (in TFLOP/s): 112.396
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1449
Attention throughput (in TFLOP/s): 93.622
MLP duration (in seconds): 0.2110
MLP throughput (in TFLOP/s): 122.262
Transformer duration (in seconds): 0.3594
Transformer throughput (in TFLOP/s): 109.531
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1359
Attention throughput (in TFLOP/s): 101.094
MLP duration (in seconds): 0.2143
MLP throughput (in TFLOP/s): 121.954
Transformer duration (in seconds): 0.3522
Transformer throughput (in TFLOP/s): 113.182
Transformer - MLP - Attention (in seconds): 0.0021
========================================================================================================================
num_attention_heads: 128, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1477
Attention throughput (in TFLOP/s): 94.185
MLP duration (in seconds): 0.2160
MLP throughput (in TFLOP/s): 122.530
Transformer duration (in seconds): 0.3676
Transformer throughput (in TFLOP/s): 109.846
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1383
Attention throughput (in TFLOP/s): 101.787
MLP duration (in seconds): 0.2186
MLP throughput (in TFLOP/s): 122.613
Transformer duration (in seconds): 0.3612
Transformer throughput (in TFLOP/s): 113.185
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1507
Attention throughput (in TFLOP/s): 94.614
MLP duration (in seconds): 0.2225
MLP throughput (in TFLOP/s): 122.004
Transformer duration (in seconds): 0.3755
Transformer throughput (in TFLOP/s): 110.268
Transformer - MLP - Attention (in seconds): 0.0023
========================================================================================================================
num_attention_heads: 128, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1387
Attention throughput (in TFLOP/s): 104.061
MLP duration (in seconds): 0.2255
MLP throughput (in TFLOP/s): 121.912
Transformer duration (in seconds): 0.3665
Transformer throughput (in TFLOP/s): 114.390
Transformer - MLP - Attention (in seconds): 0.0023
========================================================================================================================
num_attention_heads: 128, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1540
Attention throughput (in TFLOP/s): 94.865
MLP duration (in seconds): 0.2865
MLP throughput (in TFLOP/s): 97.143
Transformer duration (in seconds): 0.4473
Transformer throughput (in TFLOP/s): 94.881
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 128, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1444
Attention throughput (in TFLOP/s): 102.427
MLP duration (in seconds): 0.2824
MLP throughput (in TFLOP/s): 99.792
Transformer duration (in seconds): 0.4355
Transformer throughput (in TFLOP/s): 98.663
Transformer - MLP - Attention (in seconds): 0.0087
========================================================================================================================
num_attention_heads: 128, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1570
Attention throughput (in TFLOP/s): 95.300
MLP duration (in seconds): 0.2936
MLP throughput (in TFLOP/s): 97.162
Transformer duration (in seconds): 0.4568
Transformer throughput (in TFLOP/s): 95.205
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 128, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1470
Attention throughput (in TFLOP/s): 102.999
MLP duration (in seconds): 0.2994
MLP throughput (in TFLOP/s): 96.468
Transformer duration (in seconds): 0.4548
Transformer throughput (in TFLOP/s): 96.798
Transformer - MLP - Attention (in seconds): 0.0084
========================================================================================================================
num_attention_heads: 128, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1599
Attention throughput (in TFLOP/s): 95.856
MLP duration (in seconds): 0.3088
MLP throughput (in TFLOP/s): 94.662
Transformer duration (in seconds): 0.4729
Transformer throughput (in TFLOP/s): 94.221
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1503
Attention throughput (in TFLOP/s): 103.181
MLP duration (in seconds): 0.3061
MLP throughput (in TFLOP/s): 96.672
Transformer duration (in seconds): 0.4637
Transformer throughput (in TFLOP/s): 97.251
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 128, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1629
Attention throughput (in TFLOP/s): 96.333
MLP duration (in seconds): 0.3067
MLP throughput (in TFLOP/s): 97.641
Transformer duration (in seconds): 0.4809
Transformer throughput (in TFLOP/s): 94.902
Transformer - MLP - Attention (in seconds): 0.0113
========================================================================================================================
num_attention_heads: 128, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1518
Attention throughput (in TFLOP/s): 104.541
MLP duration (in seconds): 0.3132
MLP throughput (in TFLOP/s): 96.750
Transformer duration (in seconds): 0.4712
Transformer throughput (in TFLOP/s): 97.997
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 128, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1664
Attention throughput (in TFLOP/s): 96.498
MLP duration (in seconds): 0.3169
MLP throughput (in TFLOP/s): 96.759
Transformer duration (in seconds): 0.4882
Transformer throughput (in TFLOP/s): 95.710
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1567
Attention throughput (in TFLOP/s): 103.658
MLP duration (in seconds): 0.3156
MLP throughput (in TFLOP/s): 98.336
Transformer duration (in seconds): 0.4791
Transformer throughput (in TFLOP/s): 98.669
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 128, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1689
Attention throughput (in TFLOP/s): 97.314
MLP duration (in seconds): 0.3253
MLP throughput (in TFLOP/s): 96.531
Transformer duration (in seconds): 0.5043
Transformer throughput (in TFLOP/s): 94.844
Transformer - MLP - Attention (in seconds): 0.0102
========================================================================================================================
num_attention_heads: 128, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1588
Attention throughput (in TFLOP/s): 104.661
MLP duration (in seconds): 0.3337
MLP throughput (in TFLOP/s): 95.184
Transformer duration (in seconds): 0.4983
Transformer throughput (in TFLOP/s): 97.109
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 128, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1722
Attention throughput (in TFLOP/s): 97.615
MLP duration (in seconds): 0.3330
MLP throughput (in TFLOP/s): 96.492
Transformer duration (in seconds): 0.5149
Transformer throughput (in TFLOP/s): 95.061
Transformer - MLP - Attention (in seconds): 0.0096
========================================================================================================================
num_attention_heads: 128, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1623
Attention throughput (in TFLOP/s): 104.749
MLP duration (in seconds): 0.3358
MLP throughput (in TFLOP/s): 96.817
Transformer duration (in seconds): 0.5062
Transformer throughput (in TFLOP/s): 97.802
Transformer - MLP - Attention (in seconds): 0.0081
========================================================================================================================
num_attention_heads: 128, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1754
Attention throughput (in TFLOP/s): 98.024
MLP duration (in seconds): 0.2770
MLP throughput (in TFLOP/s): 118.722
Transformer duration (in seconds): 0.4557
Transformer throughput (in TFLOP/s): 109.891
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1637
Attention throughput (in TFLOP/s): 106.202
MLP duration (in seconds): 0.2800
MLP throughput (in TFLOP/s): 118.772
Transformer duration (in seconds): 0.4481
Transformer throughput (in TFLOP/s): 113.031
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1783
Attention throughput (in TFLOP/s): 98.597
MLP duration (in seconds): 0.2819
MLP throughput (in TFLOP/s): 119.351
Transformer duration (in seconds): 0.4650
Transformer throughput (in TFLOP/s): 110.159
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1680
Attention throughput (in TFLOP/s): 105.780
MLP duration (in seconds): 0.2851
MLP throughput (in TFLOP/s): 119.342
Transformer duration (in seconds): 0.4589
Transformer throughput (in TFLOP/s): 112.869
Transformer - MLP - Attention (in seconds): 0.0058
========================================================================================================================

1.13.1 

[2023-09-29 23:05:00,377] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-09-29 23:05:01,131] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.144.149, master_port=6000
[2023-09-29 23:05:01,132] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-09-29 23:05:04,085] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 128, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1775
Attention throughput (in TFLOP/s): 101.246
MLP duration (in seconds): 0.2809
MLP throughput (in TFLOP/s): 122.468
Transformer duration (in seconds): 0.4674
Transformer throughput (in TFLOP/s): 112.045
Transformer - MLP - Attention (in seconds): 0.0090
========================================================================================================================
num_attention_heads: 128, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1711
Attention throughput (in TFLOP/s): 106.164
MLP duration (in seconds): 0.2849
MLP throughput (in TFLOP/s): 122.131
Transformer duration (in seconds): 0.4624
Transformer throughput (in TFLOP/s): 114.535
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 128, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1845
Attention throughput (in TFLOP/s): 99.523
MLP duration (in seconds): 0.2881
MLP throughput (in TFLOP/s): 122.091
Transformer duration (in seconds): 0.4788
Transformer throughput (in TFLOP/s): 111.830
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
1.13.1 

[2023-10-22 20:12:51,462] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-10-22 20:12:52,480] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.157.218, master_port=6000
[2023-10-22 20:12:52,480] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-10-22 20:12:55,177] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 128, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 0.417
MLP duration (in seconds): 0.0001
MLP throughput (in TFLOP/s): 14.343
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 0.507
Transformer - MLP - Attention (in seconds): -0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 0.827
MLP duration (in seconds): 0.0001
MLP throughput (in TFLOP/s): 64.222
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.148
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 1.487
MLP duration (in seconds): 0.0002
MLP throughput (in TFLOP/s): 103.005
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 2.270
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 1.922
MLP duration (in seconds): 0.0003
MLP throughput (in TFLOP/s): 123.070
Transformer duration (in seconds): 0.0272
Transformer throughput (in TFLOP/s): 3.157
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2.811
MLP duration (in seconds): 0.0004
MLP throughput (in TFLOP/s): 148.536
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 4.875
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 3.250
MLP duration (in seconds): 0.0005
MLP throughput (in TFLOP/s): 153.823
Transformer duration (in seconds): 0.0284
Transformer throughput (in TFLOP/s): 5.892
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 4.410
MLP duration (in seconds): 0.0006
MLP throughput (in TFLOP/s): 167.433
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 8.255
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 6.660
MLP duration (in seconds): 0.0008
MLP throughput (in TFLOP/s): 178.140
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 12.704
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 5.952
MLP duration (in seconds): 0.0010
MLP throughput (in TFLOP/s): 177.991
Transformer duration (in seconds): 0.0288
Transformer throughput (in TFLOP/s): 11.734
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 6.333
MLP duration (in seconds): 0.0011
MLP throughput (in TFLOP/s): 196.107
Transformer duration (in seconds): 0.0319
Transformer throughput (in TFLOP/s): 12.783
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 8.032
MLP duration (in seconds): 0.0014
MLP throughput (in TFLOP/s): 184.912
Transformer duration (in seconds): 0.0296
Transformer throughput (in TFLOP/s): 16.339
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 8.601
MLP duration (in seconds): 0.0017
MLP throughput (in TFLOP/s): 186.329
Transformer duration (in seconds): 0.0319
Transformer throughput (in TFLOP/s): 17.775
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 10.374
MLP duration (in seconds): 0.0019
MLP throughput (in TFLOP/s): 195.181
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 21.563
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 10.787
MLP duration (in seconds): 0.0022
MLP throughput (in TFLOP/s): 195.657
Transformer duration (in seconds): 0.0331
Transformer throughput (in TFLOP/s): 22.689
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0287
Attention throughput (in TFLOP/s): 12.903
MLP duration (in seconds): 0.0024
MLP throughput (in TFLOP/s): 198.903
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 27.115
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 18.905
MLP duration (in seconds): 0.0027
MLP throughput (in TFLOP/s): 206.672
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 38.748
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 14.818
MLP duration (in seconds): 0.0030
MLP throughput (in TFLOP/s): 203.684
Transformer duration (in seconds): 0.0342
Transformer throughput (in TFLOP/s): 31.448
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0287
Attention throughput (in TFLOP/s): 17.510
MLP duration (in seconds): 0.0033
MLP throughput (in TFLOP/s): 208.780
Transformer duration (in seconds): 0.0324
Transformer throughput (in TFLOP/s): 36.955
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0310
Attention throughput (in TFLOP/s): 17.746
MLP duration (in seconds): 0.0037
MLP throughput (in TFLOP/s): 209.808
Transformer duration (in seconds): 0.0351
Transformer throughput (in TFLOP/s): 37.753
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0294
Attention throughput (in TFLOP/s): 20.432
MLP duration (in seconds): 0.0041
MLP throughput (in TFLOP/s): 210.375
Transformer duration (in seconds): 0.0339
Transformer throughput (in TFLOP/s): 43.044
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0319
Attention throughput (in TFLOP/s): 20.491
MLP duration (in seconds): 0.0045
MLP throughput (in TFLOP/s): 211.263
Transformer duration (in seconds): 0.0368
Transformer throughput (in TFLOP/s): 43.470
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 24.009
MLP duration (in seconds): 0.0049
MLP throughput (in TFLOP/s): 211.062
Transformer duration (in seconds): 0.0350
Transformer throughput (in TFLOP/s): 49.996
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0318
Attention throughput (in TFLOP/s): 24.062
MLP duration (in seconds): 0.0053
MLP throughput (in TFLOP/s): 215.602
Transformer duration (in seconds): 0.0376
Transformer throughput (in TFLOP/s): 50.606
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 35.163
MLP duration (in seconds): 0.0058
MLP throughput (in TFLOP/s): 211.623
Transformer duration (in seconds): 0.0298
Transformer throughput (in TFLOP/s): 69.080
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0328
Attention throughput (in TFLOP/s): 27.045
MLP duration (in seconds): 0.0062
MLP throughput (in TFLOP/s): 216.536
Transformer duration (in seconds): 0.0394
Transformer throughput (in TFLOP/s): 56.503
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 32.409
MLP duration (in seconds): 0.0066
MLP throughput (in TFLOP/s): 218.646
Transformer duration (in seconds): 0.0366
Transformer throughput (in TFLOP/s): 65.520
Transformer - MLP - Attention (in seconds): 0.0007
========================================================================================================================
num_attention_heads: 128, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0328
Attention throughput (in TFLOP/s): 30.919
MLP duration (in seconds): 0.0070
MLP throughput (in TFLOP/s): 223.304
Transformer duration (in seconds): 0.0404
Transformer throughput (in TFLOP/s): 63.908
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0298
Attention throughput (in TFLOP/s): 36.344
MLP duration (in seconds): 0.0077
MLP throughput (in TFLOP/s): 218.789
Transformer duration (in seconds): 0.0383
Transformer throughput (in TFLOP/s): 72.290
Transformer - MLP - Attention (in seconds): 0.0008
========================================================================================================================
num_attention_heads: 128, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0334
Attention throughput (in TFLOP/s): 34.478
MLP duration (in seconds): 0.0081
MLP throughput (in TFLOP/s): 221.726
Transformer duration (in seconds): 0.0423
Transformer throughput (in TFLOP/s): 69.907
Transformer - MLP - Attention (in seconds): 0.0008
========================================================================================================================
num_attention_heads: 128, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 40.299
MLP duration (in seconds): 0.0086
MLP throughput (in TFLOP/s): 225.663
Transformer duration (in seconds): 0.0400
Transformer throughput (in TFLOP/s): 78.870
Transformer - MLP - Attention (in seconds): 0.0011
========================================================================================================================
num_attention_heads: 128, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0343
Attention throughput (in TFLOP/s): 37.818
MLP duration (in seconds): 0.0093
MLP throughput (in TFLOP/s): 222.163
Transformer duration (in seconds): 0.0446
Transformer throughput (in TFLOP/s): 75.437
Transformer - MLP - Attention (in seconds): 0.0009
========================================================================================================================
num_attention_heads: 128, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 54.340
MLP duration (in seconds): 0.0098
MLP throughput (in TFLOP/s): 224.675
Transformer duration (in seconds): 0.0360
Transformer throughput (in TFLOP/s): 99.175
Transformer - MLP - Attention (in seconds): 0.0010
========================================================================================================================
num_attention_heads: 128, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0352
Attention throughput (in TFLOP/s): 41.316
MLP duration (in seconds): 0.0101
MLP throughput (in TFLOP/s): 232.674
Transformer duration (in seconds): 0.0465
Transformer throughput (in TFLOP/s): 81.456
Transformer - MLP - Attention (in seconds): 0.0013
========================================================================================================================
num_attention_heads: 128, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0347
Attention throughput (in TFLOP/s): 44.159
MLP duration (in seconds): 0.0108
MLP throughput (in TFLOP/s): 229.316
Transformer duration (in seconds): 0.0468
Transformer throughput (in TFLOP/s): 85.720
Transformer - MLP - Attention (in seconds): 0.0013
========================================================================================================================
num_attention_heads: 128, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0367
Attention throughput (in TFLOP/s): 44.090
MLP duration (in seconds): 0.0113
MLP throughput (in TFLOP/s): 233.505
Transformer duration (in seconds): 0.0493
Transformer throughput (in TFLOP/s): 86.131
Transformer - MLP - Attention (in seconds): 0.0014
========================================================================================================================
num_attention_heads: 128, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0361
Attention throughput (in TFLOP/s): 47.120
MLP duration (in seconds): 0.0118
MLP throughput (in TFLOP/s): 234.961
Transformer duration (in seconds): 0.0493
Transformer throughput (in TFLOP/s): 90.864
Transformer - MLP - Attention (in seconds): 0.0014
========================================================================================================================
num_attention_heads: 128, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0379
Attention throughput (in TFLOP/s): 47.190
MLP duration (in seconds): 0.0123
MLP throughput (in TFLOP/s): 238.766
Transformer duration (in seconds): 0.0518
Transformer throughput (in TFLOP/s): 91.279
Transformer - MLP - Attention (in seconds): 0.0016
========================================================================================================================
num_attention_heads: 128, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0376
Attention throughput (in TFLOP/s): 49.856
MLP duration (in seconds): 0.0133
MLP throughput (in TFLOP/s): 232.485
Transformer duration (in seconds): 0.0527
Transformer throughput (in TFLOP/s): 94.496
Transformer - MLP - Attention (in seconds): 0.0017
========================================================================================================================
num_attention_heads: 128, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0394
Attention throughput (in TFLOP/s): 50.005
MLP duration (in seconds): 0.0139
MLP throughput (in TFLOP/s): 234.999
Transformer duration (in seconds): 0.0550
Transformer throughput (in TFLOP/s): 95.120
Transformer - MLP - Attention (in seconds): 0.0018
========================================================================================================================
num_attention_heads: 128, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0288
Attention throughput (in TFLOP/s): 71.535
MLP duration (in seconds): 0.0144
MLP throughput (in TFLOP/s): 238.491
Transformer duration (in seconds): 0.0447
Transformer throughput (in TFLOP/s): 122.913
Transformer - MLP - Attention (in seconds): 0.0015
========================================================================================================================
num_attention_heads: 128, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0404
Attention throughput (in TFLOP/s): 53.362
MLP duration (in seconds): 0.0153
MLP throughput (in TFLOP/s): 236.085
Transformer duration (in seconds): 0.0576
Transformer throughput (in TFLOP/s): 100.084
Transformer - MLP - Attention (in seconds): 0.0019
========================================================================================================================
num_attention_heads: 128, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0398
Attention throughput (in TFLOP/s): 56.640
MLP duration (in seconds): 0.0158
MLP throughput (in TFLOP/s): 239.392
Transformer duration (in seconds): 0.0576
Transformer throughput (in TFLOP/s): 104.904
Transformer - MLP - Attention (in seconds): 0.0020
========================================================================================================================
num_attention_heads: 128, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0410
Attention throughput (in TFLOP/s): 57.411
MLP duration (in seconds): 0.0164
MLP throughput (in TFLOP/s): 242.767
Transformer duration (in seconds): 0.0596
Transformer throughput (in TFLOP/s): 106.146
Transformer - MLP - Attention (in seconds): 0.0022
========================================================================================================================
num_attention_heads: 128, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0405
Attention throughput (in TFLOP/s): 60.634
MLP duration (in seconds): 0.0176
MLP throughput (in TFLOP/s): 236.578
Transformer duration (in seconds): 0.0603
Transformer throughput (in TFLOP/s): 109.605
Transformer - MLP - Attention (in seconds): 0.0023
========================================================================================================================
num_attention_heads: 128, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0416
Attention throughput (in TFLOP/s): 61.555
MLP duration (in seconds): 0.0179
MLP throughput (in TFLOP/s): 243.402
Transformer duration (in seconds): 0.0613
Transformer throughput (in TFLOP/s): 112.782
Transformer - MLP - Attention (in seconds): 0.0018
========================================================================================================================
num_attention_heads: 128, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0409
Attention throughput (in TFLOP/s): 65.260
MLP duration (in seconds): 0.0186
MLP throughput (in TFLOP/s): 243.905
Transformer duration (in seconds): 0.0616
Transformer throughput (in TFLOP/s): 117.073
Transformer - MLP - Attention (in seconds): 0.0021
========================================================================================================================
num_attention_heads: 128, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0423
Attention throughput (in TFLOP/s): 65.579
MLP duration (in seconds): 0.0195
MLP throughput (in TFLOP/s): 242.918
Transformer duration (in seconds): 0.0643
Transformer throughput (in TFLOP/s): 116.962
Transformer - MLP - Attention (in seconds): 0.0024
========================================================================================================================
num_attention_heads: 128, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 91.495
MLP duration (in seconds): 0.0202
MLP throughput (in TFLOP/s): 245.062
Transformer duration (in seconds): 0.0537
Transformer throughput (in TFLOP/s): 145.831
Transformer - MLP - Attention (in seconds): 0.0020
========================================================================================================================
num_attention_heads: 128, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0431
Attention throughput (in TFLOP/s): 69.582
MLP duration (in seconds): 0.0217
MLP throughput (in TFLOP/s): 237.610
Transformer duration (in seconds): 0.0678
Transformer throughput (in TFLOP/s): 120.337
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0426
Attention throughput (in TFLOP/s): 73.089
MLP duration (in seconds): 0.0222
MLP throughput (in TFLOP/s): 241.853
Transformer duration (in seconds): 0.0673
Transformer throughput (in TFLOP/s): 126.035
Transformer - MLP - Attention (in seconds): 0.0025
========================================================================================================================
num_attention_heads: 128, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0441
Attention throughput (in TFLOP/s): 73.333
MLP duration (in seconds): 0.0234
MLP throughput (in TFLOP/s): 238.917
Transformer duration (in seconds): 0.0704
Transformer throughput (in TFLOP/s): 125.181
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0436
Attention throughput (in TFLOP/s): 76.857
MLP duration (in seconds): 0.0238
MLP throughput (in TFLOP/s): 243.925
Transformer duration (in seconds): 0.0700
Transformer throughput (in TFLOP/s): 130.818
Transformer - MLP - Attention (in seconds): 0.0026
========================================================================================================================
num_attention_heads: 128, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0450
Attention throughput (in TFLOP/s): 77.058
MLP duration (in seconds): 0.0247
MLP throughput (in TFLOP/s): 243.830
Transformer duration (in seconds): 0.0724
Transformer throughput (in TFLOP/s): 131.188
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0444
Attention throughput (in TFLOP/s): 81.035
MLP duration (in seconds): 0.0255
MLP throughput (in TFLOP/s): 245.229
Transformer duration (in seconds): 0.0724
Transformer throughput (in TFLOP/s): 136.095
Transformer - MLP - Attention (in seconds): 0.0025
========================================================================================================================
num_attention_heads: 128, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0461
Attention throughput (in TFLOP/s): 80.726
MLP duration (in seconds): 0.0273
MLP throughput (in TFLOP/s): 237.897
Transformer duration (in seconds): 0.0763
Transformer throughput (in TFLOP/s): 133.821
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0351
Attention throughput (in TFLOP/s): 109.553
MLP duration (in seconds): 0.0275
MLP throughput (in TFLOP/s): 244.731
Transformer duration (in seconds): 0.0647
Transformer throughput (in TFLOP/s): 163.536
Transformer - MLP - Attention (in seconds): 0.0021
========================================================================================================================
num_attention_heads: 128, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0468
Attention throughput (in TFLOP/s): 84.979
MLP duration (in seconds): 0.0293
MLP throughput (in TFLOP/s): 238.399
Transformer duration (in seconds): 0.0796
Transformer throughput (in TFLOP/s): 137.688
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0466
Attention throughput (in TFLOP/s): 88.175
MLP duration (in seconds): 0.0298
MLP throughput (in TFLOP/s): 242.740
Transformer duration (in seconds): 0.0791
Transformer throughput (in TFLOP/s): 143.313
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0481
Attention throughput (in TFLOP/s): 88.162
MLP duration (in seconds): 0.0315
MLP throughput (in TFLOP/s): 237.633
Transformer duration (in seconds): 0.0833
Transformer throughput (in TFLOP/s): 140.739
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0477
Attention throughput (in TFLOP/s): 91.852
MLP duration (in seconds): 0.0317
MLP throughput (in TFLOP/s): 243.822
Transformer duration (in seconds): 0.0820
Transformer throughput (in TFLOP/s): 147.786
Transformer - MLP - Attention (in seconds): 0.0026
========================================================================================================================
num_attention_heads: 128, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0494
Attention throughput (in TFLOP/s): 91.517
MLP duration (in seconds): 0.0334
MLP throughput (in TFLOP/s): 239.214
Transformer duration (in seconds): 0.0866
Transformer throughput (in TFLOP/s): 144.533
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0491
Attention throughput (in TFLOP/s): 94.995
MLP duration (in seconds): 0.0338
MLP throughput (in TFLOP/s): 243.906
Transformer duration (in seconds): 0.0856
Transformer throughput (in TFLOP/s): 150.826
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0504
Attention throughput (in TFLOP/s): 95.380
MLP duration (in seconds): 0.0359
MLP throughput (in TFLOP/s): 237.438
Transformer duration (in seconds): 0.0904
Transformer throughput (in TFLOP/s): 147.453
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
1.13.1 

[2023-10-22 21:42:59,120] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-10-22 21:42:59,971] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.149.177, master_port=6000
[2023-10-22 21:42:59,971] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-10-22 21:43:03,218] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 128, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 0.209
MLP duration (in seconds): 0.0001
MLP throughput (in TFLOP/s): 8.711
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 0.254
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 0.412
MLP duration (in seconds): 0.0001
MLP throughput (in TFLOP/s): 32.169
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 0.573
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 0.742
MLP duration (in seconds): 0.0002
MLP throughput (in TFLOP/s): 51.242
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.133
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 0.959
MLP duration (in seconds): 0.0003
MLP throughput (in TFLOP/s): 61.221
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.575
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 1.403
MLP duration (in seconds): 0.0004
MLP throughput (in TFLOP/s): 73.878
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 2.433
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 1.622
MLP duration (in seconds): 0.0005
MLP throughput (in TFLOP/s): 76.657
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 2.940
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2.201
MLP duration (in seconds): 0.0006
MLP throughput (in TFLOP/s): 83.463
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 4.120
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 3.324
MLP duration (in seconds): 0.0008
MLP throughput (in TFLOP/s): 89.015
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 6.346
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 2.971
MLP duration (in seconds): 0.0010
MLP throughput (in TFLOP/s): 89.147
Transformer duration (in seconds): 0.0288
Transformer throughput (in TFLOP/s): 5.863
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 3.159
MLP duration (in seconds): 0.0011
MLP throughput (in TFLOP/s): 98.075
Transformer duration (in seconds): 0.0320
Transformer throughput (in TFLOP/s): 6.382
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0280
Attention throughput (in TFLOP/s): 4.009
MLP duration (in seconds): 0.0014
MLP throughput (in TFLOP/s): 92.253
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 8.165
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 4.291
MLP duration (in seconds): 0.0017
MLP throughput (in TFLOP/s): 93.393
Transformer duration (in seconds): 0.0320
Transformer throughput (in TFLOP/s): 8.872
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 5.178
MLP duration (in seconds): 0.0018
MLP throughput (in TFLOP/s): 98.347
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 10.770
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 5.383
MLP duration (in seconds): 0.0021
MLP throughput (in TFLOP/s): 98.726
Transformer duration (in seconds): 0.0332
Transformer throughput (in TFLOP/s): 11.328
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0287
Attention throughput (in TFLOP/s): 6.444
MLP duration (in seconds): 0.0024
MLP throughput (in TFLOP/s): 100.837
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 13.548
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 9.439
MLP duration (in seconds): 0.0026
MLP throughput (in TFLOP/s): 104.945
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 19.359
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 7.406
MLP duration (in seconds): 0.0030
MLP throughput (in TFLOP/s): 102.897
Transformer duration (in seconds): 0.0343
Transformer throughput (in TFLOP/s): 15.714
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0287
Attention throughput (in TFLOP/s): 8.749
MLP duration (in seconds): 0.0033
MLP throughput (in TFLOP/s): 105.135
Transformer duration (in seconds): 0.0325
Transformer throughput (in TFLOP/s): 18.454
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 8.868
MLP duration (in seconds): 0.0037
MLP throughput (in TFLOP/s): 105.971
Transformer duration (in seconds): 0.0352
Transformer throughput (in TFLOP/s): 18.839
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 10.205
MLP duration (in seconds): 0.0040
MLP throughput (in TFLOP/s): 106.777
Transformer duration (in seconds): 0.0340
Transformer throughput (in TFLOP/s): 21.473
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0319
Attention throughput (in TFLOP/s): 10.238
MLP duration (in seconds): 0.0044
MLP throughput (in TFLOP/s): 106.521
Transformer duration (in seconds): 0.0369
Transformer throughput (in TFLOP/s): 21.697
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0296
Attention throughput (in TFLOP/s): 11.986
MLP duration (in seconds): 0.0049
MLP throughput (in TFLOP/s): 106.558
Transformer duration (in seconds): 0.0350
Transformer throughput (in TFLOP/s): 24.967
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0319
Attention throughput (in TFLOP/s): 12.016
MLP duration (in seconds): 0.0052
MLP throughput (in TFLOP/s): 108.820
Transformer duration (in seconds): 0.0376
Transformer throughput (in TFLOP/s): 25.261
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 17.556
MLP duration (in seconds): 0.0058
MLP throughput (in TFLOP/s): 106.454
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 34.508
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0328
Attention throughput (in TFLOP/s): 13.506
MLP duration (in seconds): 0.0061
MLP throughput (in TFLOP/s): 110.097
Transformer duration (in seconds): 0.0395
Transformer throughput (in TFLOP/s): 28.199
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 16.183
MLP duration (in seconds): 0.0065
MLP throughput (in TFLOP/s): 110.953
Transformer duration (in seconds): 0.0365
Transformer throughput (in TFLOP/s): 32.904
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0329
Attention throughput (in TFLOP/s): 15.439
MLP duration (in seconds): 0.0070
MLP throughput (in TFLOP/s): 112.251
Transformer duration (in seconds): 0.0404
Transformer throughput (in TFLOP/s): 31.908
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0298
Attention throughput (in TFLOP/s): 18.147
MLP duration (in seconds): 0.0076
MLP throughput (in TFLOP/s): 110.156
Transformer duration (in seconds): 0.0382
Transformer throughput (in TFLOP/s): 36.162
Transformer - MLP - Attention (in seconds): 0.0008
========================================================================================================================
num_attention_heads: 128, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0335
Attention throughput (in TFLOP/s): 17.218
MLP duration (in seconds): 0.0081
MLP throughput (in TFLOP/s): 111.819
Transformer duration (in seconds): 0.0422
Transformer throughput (in TFLOP/s): 35.080
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 20.114
MLP duration (in seconds): 0.0085
MLP throughput (in TFLOP/s): 113.798
Transformer duration (in seconds): 0.0398
Transformer throughput (in TFLOP/s): 39.640
Transformer - MLP - Attention (in seconds): 0.0009
========================================================================================================================
num_attention_heads: 128, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0344
Attention throughput (in TFLOP/s): 18.888
MLP duration (in seconds): 0.0092
MLP throughput (in TFLOP/s): 111.761
Transformer duration (in seconds): 0.0445
Transformer throughput (in TFLOP/s): 37.751
Transformer - MLP - Attention (in seconds): 0.0009
========================================================================================================================
num_attention_heads: 128, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 27.118
MLP duration (in seconds): 0.0097
MLP throughput (in TFLOP/s): 113.345
Transformer duration (in seconds): 0.0360
Transformer throughput (in TFLOP/s): 49.616
Transformer - MLP - Attention (in seconds): 0.0010
========================================================================================================================
num_attention_heads: 128, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0352
Attention throughput (in TFLOP/s): 20.628
MLP duration (in seconds): 0.0099
MLP throughput (in TFLOP/s): 118.062
Transformer duration (in seconds): 0.0464
Transformer throughput (in TFLOP/s): 40.889
Transformer - MLP - Attention (in seconds): 0.0012
========================================================================================================================
num_attention_heads: 128, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0348
Attention throughput (in TFLOP/s): 22.049
MLP duration (in seconds): 0.0106
MLP throughput (in TFLOP/s): 116.618
Transformer duration (in seconds): 0.0467
Transformer throughput (in TFLOP/s): 42.982
Transformer - MLP - Attention (in seconds): 0.0013
========================================================================================================================
num_attention_heads: 128, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0367
Attention throughput (in TFLOP/s): 22.012
MLP duration (in seconds): 0.0111
MLP throughput (in TFLOP/s): 117.984
Transformer duration (in seconds): 0.0491
Transformer throughput (in TFLOP/s): 43.202
Transformer - MLP - Attention (in seconds): 0.0013
========================================================================================================================
num_attention_heads: 128, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0362
Attention throughput (in TFLOP/s): 23.520
MLP duration (in seconds): 0.0116
MLP throughput (in TFLOP/s): 120.059
Transformer duration (in seconds): 0.0492
Transformer throughput (in TFLOP/s): 45.561
Transformer - MLP - Attention (in seconds): 0.0015
========================================================================================================================
num_attention_heads: 128, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0379
Attention throughput (in TFLOP/s): 23.566
MLP duration (in seconds): 0.0121
MLP throughput (in TFLOP/s): 121.036
Transformer duration (in seconds): 0.0517
Transformer throughput (in TFLOP/s): 45.766
Transformer - MLP - Attention (in seconds): 0.0016
========================================================================================================================
num_attention_heads: 128, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0377
Attention throughput (in TFLOP/s): 24.884
MLP duration (in seconds): 0.0131
MLP throughput (in TFLOP/s): 118.133
Transformer duration (in seconds): 0.0524
Transformer throughput (in TFLOP/s): 47.472
Transformer - MLP - Attention (in seconds): 0.0016
========================================================================================================================
num_attention_heads: 128, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0394
Attention throughput (in TFLOP/s): 24.961
MLP duration (in seconds): 0.0136
MLP throughput (in TFLOP/s): 120.044
Transformer duration (in seconds): 0.0547
Transformer throughput (in TFLOP/s): 47.806
Transformer - MLP - Attention (in seconds): 0.0017
========================================================================================================================
num_attention_heads: 128, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0289
Attention throughput (in TFLOP/s): 35.698
MLP duration (in seconds): 0.0142
MLP throughput (in TFLOP/s): 120.797
Transformer duration (in seconds): 0.0447
Transformer throughput (in TFLOP/s): 61.506
Transformer - MLP - Attention (in seconds): 0.0016
========================================================================================================================
num_attention_heads: 128, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0405
Attention throughput (in TFLOP/s): 26.616
MLP duration (in seconds): 0.0152
MLP throughput (in TFLOP/s): 119.133
Transformer duration (in seconds): 0.0579
Transformer throughput (in TFLOP/s): 49.768
Transformer - MLP - Attention (in seconds): 0.0023
========================================================================================================================
num_attention_heads: 128, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0399
Attention throughput (in TFLOP/s): 28.227
MLP duration (in seconds): 0.0158
MLP throughput (in TFLOP/s): 119.971
Transformer duration (in seconds): 0.0580
Transformer throughput (in TFLOP/s): 52.118
Transformer - MLP - Attention (in seconds): 0.0022
========================================================================================================================
num_attention_heads: 128, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0412
Attention throughput (in TFLOP/s): 28.609
MLP duration (in seconds): 0.0163
MLP throughput (in TFLOP/s): 121.557
Transformer duration (in seconds): 0.0600
Transformer throughput (in TFLOP/s): 52.689
Transformer - MLP - Attention (in seconds): 0.0025
========================================================================================================================
num_attention_heads: 128, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0407
Attention throughput (in TFLOP/s): 30.204
MLP duration (in seconds): 0.0174
MLP throughput (in TFLOP/s): 119.149
Transformer duration (in seconds): 0.0608
Transformer throughput (in TFLOP/s): 54.374
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0418
Attention throughput (in TFLOP/s): 30.666
MLP duration (in seconds): 0.0178
MLP throughput (in TFLOP/s): 122.139
Transformer duration (in seconds): 0.0618
Transformer throughput (in TFLOP/s): 55.914
Transformer - MLP - Attention (in seconds): 0.0022
========================================================================================================================
num_attention_heads: 128, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0410
Attention throughput (in TFLOP/s): 32.504
MLP duration (in seconds): 0.0185
MLP throughput (in TFLOP/s): 122.567
Transformer duration (in seconds): 0.0620
Transformer throughput (in TFLOP/s): 58.111
Transformer - MLP - Attention (in seconds): 0.0025
========================================================================================================================
num_attention_heads: 128, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0425
Attention throughput (in TFLOP/s): 32.669
MLP duration (in seconds): 0.0193
MLP throughput (in TFLOP/s): 122.854
Transformer duration (in seconds): 0.0646
Transformer throughput (in TFLOP/s): 58.172
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0317
Attention throughput (in TFLOP/s): 45.533
MLP duration (in seconds): 0.0201
MLP throughput (in TFLOP/s): 123.079
Transformer duration (in seconds): 0.0540
Transformer throughput (in TFLOP/s): 72.518
Transformer - MLP - Attention (in seconds): 0.0022
========================================================================================================================
num_attention_heads: 128, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0433
Attention throughput (in TFLOP/s): 34.670
MLP duration (in seconds): 0.0215
MLP throughput (in TFLOP/s): 119.866
Transformer duration (in seconds): 0.0681
Transformer throughput (in TFLOP/s): 59.856
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0427
Attention throughput (in TFLOP/s): 36.430
MLP duration (in seconds): 0.0219
MLP throughput (in TFLOP/s): 122.790
Transformer duration (in seconds): 0.0676
Transformer throughput (in TFLOP/s): 62.711
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0442
Attention throughput (in TFLOP/s): 36.535
MLP duration (in seconds): 0.0231
MLP throughput (in TFLOP/s): 120.673
Transformer duration (in seconds): 0.0711
Transformer throughput (in TFLOP/s): 62.027
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0438
Attention throughput (in TFLOP/s): 38.245
MLP duration (in seconds): 0.0236
MLP throughput (in TFLOP/s): 123.279
Transformer duration (in seconds): 0.0704
Transformer throughput (in TFLOP/s): 65.018
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0453
Attention throughput (in TFLOP/s): 38.339
MLP duration (in seconds): 0.0245
MLP throughput (in TFLOP/s): 123.177
Transformer duration (in seconds): 0.0729
Transformer throughput (in TFLOP/s): 65.166
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0446
Attention throughput (in TFLOP/s): 40.298
MLP duration (in seconds): 0.0251
MLP throughput (in TFLOP/s): 124.535
Transformer duration (in seconds): 0.0729
Transformer throughput (in TFLOP/s): 67.581
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0464
Attention throughput (in TFLOP/s): 40.134
MLP duration (in seconds): 0.0270
MLP throughput (in TFLOP/s): 120.161
Transformer duration (in seconds): 0.0769
Transformer throughput (in TFLOP/s): 66.391
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0353
Attention throughput (in TFLOP/s): 54.475
MLP duration (in seconds): 0.0273
MLP throughput (in TFLOP/s): 123.483
Transformer duration (in seconds): 0.0651
Transformer throughput (in TFLOP/s): 81.262
Transformer - MLP - Attention (in seconds): 0.0025
========================================================================================================================
num_attention_heads: 128, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0470
Attention throughput (in TFLOP/s): 42.317
MLP duration (in seconds): 0.0291
MLP throughput (in TFLOP/s): 119.702
Transformer duration (in seconds): 0.0801
Transformer throughput (in TFLOP/s): 68.410
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0468
Attention throughput (in TFLOP/s): 43.886
MLP duration (in seconds): 0.0293
MLP throughput (in TFLOP/s): 123.194
Transformer duration (in seconds): 0.0796
Transformer throughput (in TFLOP/s): 71.215
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0483
Attention throughput (in TFLOP/s): 43.899
MLP duration (in seconds): 0.0312
MLP throughput (in TFLOP/s): 119.931
Transformer duration (in seconds): 0.0837
Transformer throughput (in TFLOP/s): 70.032
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0478
Attention throughput (in TFLOP/s): 45.813
MLP duration (in seconds): 0.0311
MLP throughput (in TFLOP/s): 124.286
Transformer duration (in seconds): 0.0825
Transformer throughput (in TFLOP/s): 73.435
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0494
Attention throughput (in TFLOP/s): 45.736
MLP duration (in seconds): 0.0331
MLP throughput (in TFLOP/s): 120.683
Transformer duration (in seconds): 0.0867
Transformer throughput (in TFLOP/s): 72.171
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0490
Attention throughput (in TFLOP/s): 47.564
MLP duration (in seconds): 0.0335
MLP throughput (in TFLOP/s): 123.276
Transformer duration (in seconds): 0.0858
Transformer throughput (in TFLOP/s): 75.263
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0505
Attention throughput (in TFLOP/s): 47.526
MLP duration (in seconds): 0.0356
MLP throughput (in TFLOP/s): 119.828
Transformer duration (in seconds): 0.0900
Transformer throughput (in TFLOP/s): 74.005
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 0.104
MLP duration (in seconds): 0.0001
MLP throughput (in TFLOP/s): 4.549
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 0.127
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 0.206
MLP duration (in seconds): 0.0001
MLP throughput (in TFLOP/s): 15.970
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 0.286
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 0.371
MLP duration (in seconds): 0.0002
MLP throughput (in TFLOP/s): 25.817
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 0.567
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 0.480
MLP duration (in seconds): 0.0003
MLP throughput (in TFLOP/s): 30.353
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 0.788
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 0.702
MLP duration (in seconds): 0.0004
MLP throughput (in TFLOP/s): 37.061
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.217
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 0.811
MLP duration (in seconds): 0.0005
MLP throughput (in TFLOP/s): 38.166
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.470
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 1.101
MLP duration (in seconds): 0.0006
MLP throughput (in TFLOP/s): 41.732
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 2.061
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1.662
MLP duration (in seconds): 0.0008
MLP throughput (in TFLOP/s): 44.507
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 3.172
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 1.486
MLP duration (in seconds): 0.0010
MLP throughput (in TFLOP/s): 44.487
Transformer duration (in seconds): 0.0288
Transformer throughput (in TFLOP/s): 2.932
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 1.580
MLP duration (in seconds): 0.0011
MLP throughput (in TFLOP/s): 49.102
Transformer duration (in seconds): 0.0320
Transformer throughput (in TFLOP/s): 3.191
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0280
Attention throughput (in TFLOP/s): 2.005
MLP duration (in seconds): 0.0014
MLP throughput (in TFLOP/s): 46.291
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 4.083
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 2.146
MLP duration (in seconds): 0.0017
MLP throughput (in TFLOP/s): 46.777
Transformer duration (in seconds): 0.0319
Transformer throughput (in TFLOP/s): 4.437
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 2.590
MLP duration (in seconds): 0.0018
MLP throughput (in TFLOP/s): 49.205
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 5.386
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 2.692
MLP duration (in seconds): 0.0021
MLP throughput (in TFLOP/s): 49.401
Transformer duration (in seconds): 0.0332
Transformer throughput (in TFLOP/s): 5.665
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0288
Attention throughput (in TFLOP/s): 3.221
MLP duration (in seconds): 0.0024
MLP throughput (in TFLOP/s): 50.393
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 6.775
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 4.722
MLP duration (in seconds): 0.0026
MLP throughput (in TFLOP/s): 52.453
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 9.676
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 3.703
MLP duration (in seconds): 0.0030
MLP throughput (in TFLOP/s): 51.314
Transformer duration (in seconds): 0.0343
Transformer throughput (in TFLOP/s): 7.858
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0287
Attention throughput (in TFLOP/s): 4.375
MLP duration (in seconds): 0.0033
MLP throughput (in TFLOP/s): 52.613
Transformer duration (in seconds): 0.0325
Transformer throughput (in TFLOP/s): 9.228
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 4.435
MLP duration (in seconds): 0.0037
MLP throughput (in TFLOP/s): 53.041
Transformer duration (in seconds): 0.0352
Transformer throughput (in TFLOP/s): 9.421
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 5.104
MLP duration (in seconds): 0.0040
MLP throughput (in TFLOP/s): 53.376
Transformer duration (in seconds): 0.0340
Transformer throughput (in TFLOP/s): 10.738
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0319
Attention throughput (in TFLOP/s): 5.120
MLP duration (in seconds): 0.0044
MLP throughput (in TFLOP/s): 53.281
Transformer duration (in seconds): 0.0369
Transformer throughput (in TFLOP/s): 10.850
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0296
Attention throughput (in TFLOP/s): 5.994
MLP duration (in seconds): 0.0049
MLP throughput (in TFLOP/s): 53.294
Transformer duration (in seconds): 0.0350
Transformer throughput (in TFLOP/s): 12.485
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0318
Attention throughput (in TFLOP/s): 6.010
MLP duration (in seconds): 0.0052
MLP throughput (in TFLOP/s): 54.378
Transformer duration (in seconds): 0.0376
Transformer throughput (in TFLOP/s): 12.631
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 8.774
MLP duration (in seconds): 0.0058
MLP throughput (in TFLOP/s): 53.234
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 17.248
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0328
Attention throughput (in TFLOP/s): 6.754
MLP duration (in seconds): 0.0061
MLP throughput (in TFLOP/s): 55.038
Transformer duration (in seconds): 0.0395
Transformer throughput (in TFLOP/s): 14.101
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 8.091
MLP duration (in seconds): 0.0065
MLP throughput (in TFLOP/s): 55.472
Transformer duration (in seconds): 0.0365
Transformer throughput (in TFLOP/s): 16.454
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0329
Attention throughput (in TFLOP/s): 7.720
MLP duration (in seconds): 0.0069
MLP throughput (in TFLOP/s): 56.405
Transformer duration (in seconds): 0.0404
Transformer throughput (in TFLOP/s): 15.956
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0298
Attention throughput (in TFLOP/s): 9.074
MLP duration (in seconds): 0.0076
MLP throughput (in TFLOP/s): 55.099
Transformer duration (in seconds): 0.0382
Transformer throughput (in TFLOP/s): 18.081
Transformer - MLP - Attention (in seconds): 0.0008
========================================================================================================================
num_attention_heads: 128, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0335
Attention throughput (in TFLOP/s): 8.611
MLP duration (in seconds): 0.0081
MLP throughput (in TFLOP/s): 55.893
Transformer duration (in seconds): 0.0422
Transformer throughput (in TFLOP/s): 17.545
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 10.059
MLP duration (in seconds): 0.0085
MLP throughput (in TFLOP/s): 56.896
Transformer duration (in seconds): 0.0399
Transformer throughput (in TFLOP/s): 19.776
Transformer - MLP - Attention (in seconds): 0.0010
========================================================================================================================
num_attention_heads: 128, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0344
Attention throughput (in TFLOP/s): 9.445
MLP duration (in seconds): 0.0092
MLP throughput (in TFLOP/s): 55.889
Transformer duration (in seconds): 0.0445
Transformer throughput (in TFLOP/s): 18.877
Transformer - MLP - Attention (in seconds): 0.0009
========================================================================================================================
num_attention_heads: 128, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 13.558
MLP duration (in seconds): 0.0097
MLP throughput (in TFLOP/s): 56.703
Transformer duration (in seconds): 0.0360
Transformer throughput (in TFLOP/s): 24.820
Transformer - MLP - Attention (in seconds): 0.0010
========================================================================================================================
num_attention_heads: 128, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0352
Attention throughput (in TFLOP/s): 10.316
MLP duration (in seconds): 0.0099
MLP throughput (in TFLOP/s): 59.061
Transformer duration (in seconds): 0.0464
Transformer throughput (in TFLOP/s): 20.444
Transformer - MLP - Attention (in seconds): 0.0013
========================================================================================================================
num_attention_heads: 128, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0348
Attention throughput (in TFLOP/s): 11.026
MLP duration (in seconds): 0.0106
MLP throughput (in TFLOP/s): 58.336
Transformer duration (in seconds): 0.0467
Transformer throughput (in TFLOP/s): 21.495
Transformer - MLP - Attention (in seconds): 0.0013
========================================================================================================================
num_attention_heads: 128, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0367
Attention throughput (in TFLOP/s): 11.008
MLP duration (in seconds): 0.0111
MLP throughput (in TFLOP/s): 58.996
Transformer duration (in seconds): 0.0491
Transformer throughput (in TFLOP/s): 21.605
Transformer - MLP - Attention (in seconds): 0.0013
========================================================================================================================
num_attention_heads: 128, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0362
Attention throughput (in TFLOP/s): 11.760
MLP duration (in seconds): 0.0116
MLP throughput (in TFLOP/s): 60.042
Transformer duration (in seconds): 0.0492
Transformer throughput (in TFLOP/s): 22.789
Transformer - MLP - Attention (in seconds): 0.0014
========================================================================================================================
num_attention_heads: 128, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0379
Attention throughput (in TFLOP/s): 11.783
MLP duration (in seconds): 0.0121
MLP throughput (in TFLOP/s): 60.513
Transformer duration (in seconds): 0.0517
Transformer throughput (in TFLOP/s): 22.864
Transformer - MLP - Attention (in seconds): 0.0016
========================================================================================================================
num_attention_heads: 128, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0377
Attention throughput (in TFLOP/s): 12.443
MLP duration (in seconds): 0.0131
MLP throughput (in TFLOP/s): 59.076
Transformer duration (in seconds): 0.0524
Transformer throughput (in TFLOP/s): 23.739
Transformer - MLP - Attention (in seconds): 0.0016
========================================================================================================================
num_attention_heads: 128, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0394
Attention throughput (in TFLOP/s): 12.481
MLP duration (in seconds): 0.0136
MLP throughput (in TFLOP/s): 60.002
Transformer duration (in seconds): 0.0548
Transformer throughput (in TFLOP/s): 23.899
Transformer - MLP - Attention (in seconds): 0.0017
========================================================================================================================
num_attention_heads: 128, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0289
Attention throughput (in TFLOP/s): 17.852
MLP duration (in seconds): 0.0142
MLP throughput (in TFLOP/s): 60.437
Transformer duration (in seconds): 0.0446
Transformer throughput (in TFLOP/s): 30.848
Transformer - MLP - Attention (in seconds): 0.0015
========================================================================================================================
num_attention_heads: 128, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0405
Attention throughput (in TFLOP/s): 13.311
MLP duration (in seconds): 0.0151
MLP throughput (in TFLOP/s): 59.603
Transformer duration (in seconds): 0.0579
Transformer throughput (in TFLOP/s): 24.886
Transformer - MLP - Attention (in seconds): 0.0023
========================================================================================================================
num_attention_heads: 128, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0399
Attention throughput (in TFLOP/s): 14.116
MLP duration (in seconds): 0.0158
MLP throughput (in TFLOP/s): 59.986
Transformer duration (in seconds): 0.0580
Transformer throughput (in TFLOP/s): 26.070
Transformer - MLP - Attention (in seconds): 0.0022
========================================================================================================================
num_attention_heads: 128, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0411
Attention throughput (in TFLOP/s): 14.308
MLP duration (in seconds): 0.0163
MLP throughput (in TFLOP/s): 60.805
Transformer duration (in seconds): 0.0602
Transformer throughput (in TFLOP/s): 26.254
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0407
Attention throughput (in TFLOP/s): 15.104
MLP duration (in seconds): 0.0174
MLP throughput (in TFLOP/s): 59.591
Transformer duration (in seconds): 0.0608
Transformer throughput (in TFLOP/s): 27.193
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0418
Attention throughput (in TFLOP/s): 15.334
MLP duration (in seconds): 0.0178
MLP throughput (in TFLOP/s): 61.062
Transformer duration (in seconds): 0.0618
Transformer throughput (in TFLOP/s): 27.941
Transformer - MLP - Attention (in seconds): 0.0023
========================================================================================================================
num_attention_heads: 128, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0410
Attention throughput (in TFLOP/s): 16.255
MLP duration (in seconds): 0.0185
MLP throughput (in TFLOP/s): 61.279
Transformer duration (in seconds): 0.0620
Transformer throughput (in TFLOP/s): 29.060
Transformer - MLP - Attention (in seconds): 0.0025
========================================================================================================================
num_attention_heads: 128, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0425
Attention throughput (in TFLOP/s): 16.338
MLP duration (in seconds): 0.0193
MLP throughput (in TFLOP/s): 61.430
Transformer duration (in seconds): 0.0646
Transformer throughput (in TFLOP/s): 29.086
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 128, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0317
Attention throughput (in TFLOP/s): 22.760
MLP duration (in seconds): 0.0201
MLP throughput (in TFLOP/s): 61.549
Transformer duration (in seconds): 0.0540
Transformer throughput (in TFLOP/s): 36.240
Transformer - MLP - Attention (in seconds): 0.0022
========================================================================================================================
num_attention_heads: 128, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0432
Attention throughput (in TFLOP/s): 17.338
MLP duration (in seconds): 0.0215
MLP throughput (in TFLOP/s): 59.957
Transformer duration (in seconds): 0.0682
Transformer throughput (in TFLOP/s): 29.908
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0427
Attention throughput (in TFLOP/s): 18.217
MLP duration (in seconds): 0.0219
MLP throughput (in TFLOP/s): 61.400
Transformer duration (in seconds): 0.0676
Transformer throughput (in TFLOP/s): 31.355
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0442
Attention throughput (in TFLOP/s): 18.270
MLP duration (in seconds): 0.0232
MLP throughput (in TFLOP/s): 60.297
Transformer duration (in seconds): 0.0710
Transformer throughput (in TFLOP/s): 31.064
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0438
Attention throughput (in TFLOP/s): 19.135
MLP duration (in seconds): 0.0235
MLP throughput (in TFLOP/s): 61.649
Transformer duration (in seconds): 0.0703
Transformer throughput (in TFLOP/s): 32.544
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0453
Attention throughput (in TFLOP/s): 19.177
MLP duration (in seconds): 0.0245
MLP throughput (in TFLOP/s): 61.610
Transformer duration (in seconds): 0.0729
Transformer throughput (in TFLOP/s): 32.603
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0446
Attention throughput (in TFLOP/s): 20.154
MLP duration (in seconds): 0.0251
MLP throughput (in TFLOP/s): 62.288
Transformer duration (in seconds): 0.0729
Transformer throughput (in TFLOP/s): 33.792
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0463
Attention throughput (in TFLOP/s): 20.076
MLP duration (in seconds): 0.0270
MLP throughput (in TFLOP/s): 60.056
Transformer duration (in seconds): 0.0769
Transformer throughput (in TFLOP/s): 33.215
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0353
Attention throughput (in TFLOP/s): 27.233
MLP duration (in seconds): 0.0271
MLP throughput (in TFLOP/s): 62.110
Transformer duration (in seconds): 0.0651
Transformer throughput (in TFLOP/s): 40.654
Transformer - MLP - Attention (in seconds): 0.0026
========================================================================================================================
num_attention_heads: 128, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0470
Attention throughput (in TFLOP/s): 21.170
MLP duration (in seconds): 0.0291
MLP throughput (in TFLOP/s): 59.871
Transformer duration (in seconds): 0.0800
Transformer throughput (in TFLOP/s): 34.226
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0467
Attention throughput (in TFLOP/s): 21.990
MLP duration (in seconds): 0.0293
MLP throughput (in TFLOP/s): 61.665
Transformer duration (in seconds): 0.0796
Transformer throughput (in TFLOP/s): 35.614
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0482
Attention throughput (in TFLOP/s): 22.029
MLP duration (in seconds): 0.0312
MLP throughput (in TFLOP/s): 59.992
Transformer duration (in seconds): 0.0836
Transformer throughput (in TFLOP/s): 35.058
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0478
Attention throughput (in TFLOP/s): 22.917
MLP duration (in seconds): 0.0312
MLP throughput (in TFLOP/s): 61.904
Transformer duration (in seconds): 0.0824
Transformer throughput (in TFLOP/s): 36.725
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0494
Attention throughput (in TFLOP/s): 22.867
MLP duration (in seconds): 0.0331
MLP throughput (in TFLOP/s): 60.311
Transformer duration (in seconds): 0.0866
Transformer throughput (in TFLOP/s): 36.097
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0491
Attention throughput (in TFLOP/s): 23.741
MLP duration (in seconds): 0.0334
MLP throughput (in TFLOP/s): 61.704
Transformer duration (in seconds): 0.0857
Transformer throughput (in TFLOP/s): 37.668
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0505
Attention throughput (in TFLOP/s): 23.758
MLP duration (in seconds): 0.0355
MLP throughput (in TFLOP/s): 59.992
Transformer duration (in seconds): 0.0900
Transformer throughput (in TFLOP/s): 37.026
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 0.052
MLP duration (in seconds): 0.0001
MLP throughput (in TFLOP/s): 2.270
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 0.063
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 0.103
MLP duration (in seconds): 0.0001
MLP throughput (in TFLOP/s): 8.085
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 0.143
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 0.186
MLP duration (in seconds): 0.0002
MLP throughput (in TFLOP/s): 12.876
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 0.283
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 0.240
MLP duration (in seconds): 0.0003
MLP throughput (in TFLOP/s): 15.151
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 0.394
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 0.351
MLP duration (in seconds): 0.0004
MLP throughput (in TFLOP/s): 18.445
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 0.608
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 0.406
MLP duration (in seconds): 0.0005
MLP throughput (in TFLOP/s): 19.164
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 0.735
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 0.551
MLP duration (in seconds): 0.0006
MLP throughput (in TFLOP/s): 20.945
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.031
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 0.831
MLP duration (in seconds): 0.0008
MLP throughput (in TFLOP/s): 22.240
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.585
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 0.743
MLP duration (in seconds): 0.0010
MLP throughput (in TFLOP/s): 22.303
Transformer duration (in seconds): 0.0289
Transformer throughput (in TFLOP/s): 1.465
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 0.790
MLP duration (in seconds): 0.0011
MLP throughput (in TFLOP/s): 24.588
Transformer duration (in seconds): 0.0320
Transformer throughput (in TFLOP/s): 1.596
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0280
Attention throughput (in TFLOP/s): 1.002
MLP duration (in seconds): 0.0014
MLP throughput (in TFLOP/s): 23.110
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 2.042
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 1.073
MLP duration (in seconds): 0.0017
MLP throughput (in TFLOP/s): 23.382
Transformer duration (in seconds): 0.0319
Transformer throughput (in TFLOP/s): 2.219
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 1.295
MLP duration (in seconds): 0.0018
MLP throughput (in TFLOP/s): 24.612
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 2.693
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 1.346
MLP duration (in seconds): 0.0021
MLP throughput (in TFLOP/s): 24.706
Transformer duration (in seconds): 0.0332
Transformer throughput (in TFLOP/s): 2.833
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0288
Attention throughput (in TFLOP/s): 1.611
MLP duration (in seconds): 0.0024
MLP throughput (in TFLOP/s): 25.199
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 3.388
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2.360
MLP duration (in seconds): 0.0026
MLP throughput (in TFLOP/s): 26.258
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 4.839
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 1.850
MLP duration (in seconds): 0.0030
MLP throughput (in TFLOP/s): 25.742
Transformer duration (in seconds): 0.0343
Transformer throughput (in TFLOP/s): 3.929
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0287
Attention throughput (in TFLOP/s): 2.188
MLP duration (in seconds): 0.0033
MLP throughput (in TFLOP/s): 26.325
Transformer duration (in seconds): 0.0325
Transformer throughput (in TFLOP/s): 4.615
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0310
Attention throughput (in TFLOP/s): 2.218
MLP duration (in seconds): 0.0037
MLP throughput (in TFLOP/s): 26.520
Transformer duration (in seconds): 0.0352
Transformer throughput (in TFLOP/s): 4.711
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0294
Attention throughput (in TFLOP/s): 2.552
MLP duration (in seconds): 0.0040
MLP throughput (in TFLOP/s): 26.702
Transformer duration (in seconds): 0.0340
Transformer throughput (in TFLOP/s): 5.369
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0319
Attention throughput (in TFLOP/s): 2.561
MLP duration (in seconds): 0.0044
MLP throughput (in TFLOP/s): 26.647
Transformer duration (in seconds): 0.0369
Transformer throughput (in TFLOP/s): 5.425
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0296
Attention throughput (in TFLOP/s): 2.997
MLP duration (in seconds): 0.0049
MLP throughput (in TFLOP/s): 26.654
Transformer duration (in seconds): 0.0350
Transformer throughput (in TFLOP/s): 6.243
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0318
Attention throughput (in TFLOP/s): 3.005
MLP duration (in seconds): 0.0052
MLP throughput (in TFLOP/s): 27.194
Transformer duration (in seconds): 0.0376
Transformer throughput (in TFLOP/s): 6.316
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 4.387
MLP duration (in seconds): 0.0058
MLP throughput (in TFLOP/s): 26.770
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 8.624
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0328
Attention throughput (in TFLOP/s): 3.377
MLP duration (in seconds): 0.0061
MLP throughput (in TFLOP/s): 27.517
Transformer duration (in seconds): 0.0395
Transformer throughput (in TFLOP/s): 7.051
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 4.046
MLP duration (in seconds): 0.0065
MLP throughput (in TFLOP/s): 27.751
Transformer duration (in seconds): 0.0365
Transformer throughput (in TFLOP/s): 8.228
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0329
Attention throughput (in TFLOP/s): 3.861
MLP duration (in seconds): 0.0069
MLP throughput (in TFLOP/s): 28.231
Transformer duration (in seconds): 0.0404
Transformer throughput (in TFLOP/s): 7.979
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0298
Attention throughput (in TFLOP/s): 4.537
MLP duration (in seconds): 0.0076
MLP throughput (in TFLOP/s): 27.556
Transformer duration (in seconds): 0.0381
Transformer throughput (in TFLOP/s): 9.071
Transformer - MLP - Attention (in seconds): 0.0007
========================================================================================================================
num_attention_heads: 128, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0335
Attention throughput (in TFLOP/s): 4.305
MLP duration (in seconds): 0.0080
MLP throughput (in TFLOP/s): 28.143
Transformer duration (in seconds): 0.0422
Transformer throughput (in TFLOP/s): 8.772
Transformer - MLP - Attention (in seconds): 0.0007
========================================================================================================================
num_attention_heads: 128, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 5.029
MLP duration (in seconds): 0.0085
MLP throughput (in TFLOP/s): 28.459
Transformer duration (in seconds): 0.0398
Transformer throughput (in TFLOP/s): 9.909
Transformer - MLP - Attention (in seconds): 0.0009
========================================================================================================================
num_attention_heads: 128, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0344
Attention throughput (in TFLOP/s): 4.723
MLP duration (in seconds): 0.0092
MLP throughput (in TFLOP/s): 27.943
Transformer duration (in seconds): 0.0445
Transformer throughput (in TFLOP/s): 9.440
Transformer - MLP - Attention (in seconds): 0.0009
========================================================================================================================
num_attention_heads: 128, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 6.778
MLP duration (in seconds): 0.0096
MLP throughput (in TFLOP/s): 28.530
Transformer duration (in seconds): 0.0360
Transformer throughput (in TFLOP/s): 12.408
Transformer - MLP - Attention (in seconds): 0.0010
========================================================================================================================
num_attention_heads: 128, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0352
Attention throughput (in TFLOP/s): 5.158
MLP duration (in seconds): 0.0099
MLP throughput (in TFLOP/s): 29.534
Transformer duration (in seconds): 0.0463
Transformer throughput (in TFLOP/s): 10.230
Transformer - MLP - Attention (in seconds): 0.0012
========================================================================================================================
num_attention_heads: 128, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0348
Attention throughput (in TFLOP/s): 5.513
MLP duration (in seconds): 0.0106
MLP throughput (in TFLOP/s): 29.168
Transformer duration (in seconds): 0.0467
Transformer throughput (in TFLOP/s): 10.750
Transformer - MLP - Attention (in seconds): 0.0013
========================================================================================================================
num_attention_heads: 128, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0367
Attention throughput (in TFLOP/s): 5.504
MLP duration (in seconds): 0.0111
MLP throughput (in TFLOP/s): 29.510
Transformer duration (in seconds): 0.0491
Transformer throughput (in TFLOP/s): 10.803
Transformer - MLP - Attention (in seconds): 0.0013
========================================================================================================================
num_attention_heads: 128, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0362
Attention throughput (in TFLOP/s): 5.881
MLP duration (in seconds): 0.0116
MLP throughput (in TFLOP/s): 30.020
Transformer duration (in seconds): 0.0492
Transformer throughput (in TFLOP/s): 11.392
Transformer - MLP - Attention (in seconds): 0.0015
========================================================================================================================
num_attention_heads: 128, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0379
Attention throughput (in TFLOP/s): 5.892
MLP duration (in seconds): 0.0121
MLP throughput (in TFLOP/s): 30.246
Transformer duration (in seconds): 0.0517
Transformer throughput (in TFLOP/s): 11.436
Transformer - MLP - Attention (in seconds): 0.0016
========================================================================================================================
num_attention_heads: 128, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0377
Attention throughput (in TFLOP/s): 6.222
MLP duration (in seconds): 0.0131
MLP throughput (in TFLOP/s): 29.541
Transformer duration (in seconds): 0.0524
Transformer throughput (in TFLOP/s): 11.868
Transformer - MLP - Attention (in seconds): 0.0016
========================================================================================================================
num_attention_heads: 128, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0394
Attention throughput (in TFLOP/s): 6.242
MLP duration (in seconds): 0.0136
MLP throughput (in TFLOP/s): 30.011
Transformer duration (in seconds): 0.0548
Transformer throughput (in TFLOP/s): 11.950
Transformer - MLP - Attention (in seconds): 0.0017
========================================================================================================================
num_attention_heads: 128, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0289
Attention throughput (in TFLOP/s): 8.925
MLP duration (in seconds): 0.0142
MLP throughput (in TFLOP/s): 30.221
Transformer duration (in seconds): 0.0447
Transformer throughput (in TFLOP/s): 15.369
Transformer - MLP - Attention (in seconds): 0.0016
========================================================================================================================
num_attention_heads: 128, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0405
Attention throughput (in TFLOP/s): 6.655
MLP duration (in seconds): 0.0151
MLP throughput (in TFLOP/s): 29.785
Transformer duration (in seconds): 0.0579
Transformer throughput (in TFLOP/s): 12.442
Transformer - MLP - Attention (in seconds): 0.0023
========================================================================================================================
num_attention_heads: 128, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0399
Attention throughput (in TFLOP/s): 7.058
MLP duration (in seconds): 0.0158
MLP throughput (in TFLOP/s): 30.002
Transformer duration (in seconds): 0.0580
Transformer throughput (in TFLOP/s): 13.032
Transformer - MLP - Attention (in seconds): 0.0022
========================================================================================================================
num_attention_heads: 128, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0411
Attention throughput (in TFLOP/s): 7.154
MLP duration (in seconds): 0.0163
MLP throughput (in TFLOP/s): 30.391
Transformer duration (in seconds): 0.0602
Transformer throughput (in TFLOP/s): 13.139
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0407
Attention throughput (in TFLOP/s): 7.553
MLP duration (in seconds): 0.0174
MLP throughput (in TFLOP/s): 29.802
Transformer duration (in seconds): 0.0608
Transformer throughput (in TFLOP/s): 13.591
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0417
Attention throughput (in TFLOP/s): 7.667
MLP duration (in seconds): 0.0178
MLP throughput (in TFLOP/s): 30.521
Transformer duration (in seconds): 0.0618
Transformer throughput (in TFLOP/s): 13.975
Transformer - MLP - Attention (in seconds): 0.0022
========================================================================================================================
num_attention_heads: 128, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0410
Attention throughput (in TFLOP/s): 8.127
MLP duration (in seconds): 0.0185
MLP throughput (in TFLOP/s): 30.646
Transformer duration (in seconds): 0.0621
Transformer throughput (in TFLOP/s): 14.526
Transformer - MLP - Attention (in seconds): 0.0025
========================================================================================================================
num_attention_heads: 128, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0425
Attention throughput (in TFLOP/s): 8.168
MLP duration (in seconds): 0.0193
MLP throughput (in TFLOP/s): 30.710
Transformer duration (in seconds): 0.0647
Transformer throughput (in TFLOP/s): 14.538
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 128, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0317
Attention throughput (in TFLOP/s): 11.379
MLP duration (in seconds): 0.0201
MLP throughput (in TFLOP/s): 30.768
Transformer duration (in seconds): 0.0540
Transformer throughput (in TFLOP/s): 18.120
Transformer - MLP - Attention (in seconds): 0.0022
========================================================================================================================
num_attention_heads: 128, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0432
Attention throughput (in TFLOP/s): 8.669
MLP duration (in seconds): 0.0215
MLP throughput (in TFLOP/s): 29.982
Transformer duration (in seconds): 0.0678
Transformer throughput (in TFLOP/s): 15.033
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0427
Attention throughput (in TFLOP/s): 9.107
MLP duration (in seconds): 0.0219
MLP throughput (in TFLOP/s): 30.693
Transformer duration (in seconds): 0.0676
Transformer throughput (in TFLOP/s): 15.691
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0442
Attention throughput (in TFLOP/s): 9.137
MLP duration (in seconds): 0.0231
MLP throughput (in TFLOP/s): 30.162
Transformer duration (in seconds): 0.0709
Transformer throughput (in TFLOP/s): 15.533
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0438
Attention throughput (in TFLOP/s): 9.564
MLP duration (in seconds): 0.0236
MLP throughput (in TFLOP/s): 30.818
Transformer duration (in seconds): 0.0704
Transformer throughput (in TFLOP/s): 16.268
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0452
Attention throughput (in TFLOP/s): 9.591
MLP duration (in seconds): 0.0245
MLP throughput (in TFLOP/s): 30.799
Transformer duration (in seconds): 0.0729
Transformer throughput (in TFLOP/s): 16.297
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0446
Attention throughput (in TFLOP/s): 10.079
MLP duration (in seconds): 0.0251
MLP throughput (in TFLOP/s): 31.143
Transformer duration (in seconds): 0.0729
Transformer throughput (in TFLOP/s): 16.905
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0463
Attention throughput (in TFLOP/s): 10.038
MLP duration (in seconds): 0.0270
MLP throughput (in TFLOP/s): 30.030
Transformer duration (in seconds): 0.0769
Transformer throughput (in TFLOP/s): 16.610
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0353
Attention throughput (in TFLOP/s): 13.612
MLP duration (in seconds): 0.0274
MLP throughput (in TFLOP/s): 30.748
Transformer duration (in seconds): 0.0651
Transformer throughput (in TFLOP/s): 20.324
Transformer - MLP - Attention (in seconds): 0.0024
========================================================================================================================
num_attention_heads: 128, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0470
Attention throughput (in TFLOP/s): 10.583
MLP duration (in seconds): 0.0291
MLP throughput (in TFLOP/s): 29.944
Transformer duration (in seconds): 0.0801
Transformer throughput (in TFLOP/s): 17.104
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0468
Attention throughput (in TFLOP/s): 10.972
MLP duration (in seconds): 0.0294
MLP throughput (in TFLOP/s): 30.720
Transformer duration (in seconds): 0.0796
Transformer throughput (in TFLOP/s): 17.801
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0482
Attention throughput (in TFLOP/s): 11.013
MLP duration (in seconds): 0.0311
MLP throughput (in TFLOP/s): 30.000
Transformer duration (in seconds): 0.0836
Transformer throughput (in TFLOP/s): 17.515
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0478
Attention throughput (in TFLOP/s): 11.457
MLP duration (in seconds): 0.0314
MLP throughput (in TFLOP/s): 30.764
Transformer duration (in seconds): 0.0825
Transformer throughput (in TFLOP/s): 18.360
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0494
Attention throughput (in TFLOP/s): 11.431
MLP duration (in seconds): 0.0331
MLP throughput (in TFLOP/s): 30.160
Transformer duration (in seconds): 0.0867
Transformer throughput (in TFLOP/s): 18.039
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0490
Attention throughput (in TFLOP/s): 11.900
MLP duration (in seconds): 0.0335
MLP throughput (in TFLOP/s): 30.832
Transformer duration (in seconds): 0.0857
Transformer throughput (in TFLOP/s): 18.828
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0506
Attention throughput (in TFLOP/s): 11.876
MLP duration (in seconds): 0.0355
MLP throughput (in TFLOP/s): 30.026
Transformer duration (in seconds): 0.0901
Transformer throughput (in TFLOP/s): 18.490
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
1.13.1 

[2023-10-23 02:42:56,247] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-10-23 02:42:57,111] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.159.13, master_port=6000
[2023-10-23 02:42:57,111] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-10-23 02:43:00,340] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 128, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0388
Attention throughput (in TFLOP/s): 31.890
MLP duration (in seconds): 0.0364
MLP throughput (in TFLOP/s): 60.408
Transformer duration (in seconds): 0.0781
Transformer throughput (in TFLOP/s): 43.982
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 128, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0530
Attention throughput (in TFLOP/s): 24.016
MLP duration (in seconds): 0.0377
MLP throughput (in TFLOP/s): 60.127
Transformer duration (in seconds): 0.0948
Transformer throughput (in TFLOP/s): 37.377
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0542
Attention throughput (in TFLOP/s): 24.179
MLP duration (in seconds): 0.0389
MLP throughput (in TFLOP/s): 60.076
Transformer duration (in seconds): 0.0971
Transformer throughput (in TFLOP/s): 37.587
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0548
Attention throughput (in TFLOP/s): 24.601
MLP duration (in seconds): 0.0402
MLP throughput (in TFLOP/s): 59.965
Transformer duration (in seconds): 0.0990
Transformer throughput (in TFLOP/s): 37.957
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0565
Attention throughput (in TFLOP/s): 24.572
MLP duration (in seconds): 0.0410
MLP throughput (in TFLOP/s): 60.511
Transformer duration (in seconds): 0.1013
Transformer throughput (in TFLOP/s): 38.212
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0570
Attention throughput (in TFLOP/s): 25.028
MLP duration (in seconds): 0.0423
MLP throughput (in TFLOP/s): 60.463
Transformer duration (in seconds): 0.1032
Transformer throughput (in TFLOP/s): 38.571
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0588
Attention throughput (in TFLOP/s): 24.927
MLP duration (in seconds): 0.0422
MLP throughput (in TFLOP/s): 62.384
Transformer duration (in seconds): 0.1043
Transformer throughput (in TFLOP/s): 39.284
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0595
Attention throughput (in TFLOP/s): 25.325
MLP duration (in seconds): 0.0436
MLP throughput (in TFLOP/s): 62.080
Transformer duration (in seconds): 0.1061
Transformer throughput (in TFLOP/s): 39.682
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0448
Attention throughput (in TFLOP/s): 34.507
MLP duration (in seconds): 0.0460
MLP throughput (in TFLOP/s): 60.492
Transformer duration (in seconds): 0.0942
Transformer throughput (in TFLOP/s): 45.971
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0609
Attention throughput (in TFLOP/s): 26.061
MLP duration (in seconds): 0.0473
MLP throughput (in TFLOP/s): 60.443
Transformer duration (in seconds): 0.1124
Transformer throughput (in TFLOP/s): 39.578
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0488
Attention throughput (in TFLOP/s): 33.383
MLP duration (in seconds): 0.0494
MLP throughput (in TFLOP/s): 59.486
Transformer duration (in seconds): 0.1014
Transformer throughput (in TFLOP/s): 45.058
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0621
Attention throughput (in TFLOP/s): 26.910
MLP duration (in seconds): 0.0507
MLP throughput (in TFLOP/s): 59.571
Transformer duration (in seconds): 0.1175
Transformer throughput (in TFLOP/s): 39.929
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0501
Attention throughput (in TFLOP/s): 34.212
MLP duration (in seconds): 0.0504
MLP throughput (in TFLOP/s): 61.478
Transformer duration (in seconds): 0.1032
Transformer throughput (in TFLOP/s): 46.661
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0635
Attention throughput (in TFLOP/s): 27.661
MLP duration (in seconds): 0.0535
MLP throughput (in TFLOP/s): 59.469
Transformer duration (in seconds): 0.1217
Transformer throughput (in TFLOP/s): 40.576
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0514
Attention throughput (in TFLOP/s): 35.032
MLP duration (in seconds): 0.0534
MLP throughput (in TFLOP/s): 61.160
Transformer duration (in seconds): 0.1073
Transformer throughput (in TFLOP/s): 47.205
Transformer - MLP - Attention (in seconds): 0.0025
========================================================================================================================
num_attention_heads: 128, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0651
Attention throughput (in TFLOP/s): 28.327
MLP duration (in seconds): 0.0562
MLP throughput (in TFLOP/s): 59.629
Transformer duration (in seconds): 0.1256
Transformer throughput (in TFLOP/s): 41.381
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0506
Attention throughput (in TFLOP/s): 37.364
MLP duration (in seconds): 0.0561
MLP throughput (in TFLOP/s): 61.295
Transformer duration (in seconds): 0.1092
Transformer throughput (in TFLOP/s): 48.791
Transformer - MLP - Attention (in seconds): 0.0025
========================================================================================================================
num_attention_heads: 128, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0666
Attention throughput (in TFLOP/s): 29.042
MLP duration (in seconds): 0.0589
MLP throughput (in TFLOP/s): 59.775
Transformer duration (in seconds): 0.1298
Transformer throughput (in TFLOP/s): 42.036
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0545
Attention throughput (in TFLOP/s): 36.350
MLP duration (in seconds): 0.0606
MLP throughput (in TFLOP/s): 59.535
Transformer duration (in seconds): 0.1188
Transformer throughput (in TFLOP/s): 47.077
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0682
Attention throughput (in TFLOP/s): 29.729
MLP duration (in seconds): 0.0619
MLP throughput (in TFLOP/s): 59.733
Transformer duration (in seconds): 0.1346
Transformer throughput (in TFLOP/s): 42.532
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0557
Attention throughput (in TFLOP/s): 37.224
MLP duration (in seconds): 0.0619
MLP throughput (in TFLOP/s): 61.200
Transformer duration (in seconds): 0.1206
Transformer throughput (in TFLOP/s): 48.619
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0701
Attention throughput (in TFLOP/s): 30.270
MLP duration (in seconds): 0.0649
MLP throughput (in TFLOP/s): 59.776
Transformer duration (in seconds): 0.1394
Transformer throughput (in TFLOP/s): 43.046
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0573
Attention throughput (in TFLOP/s): 37.867
MLP duration (in seconds): 0.0648
MLP throughput (in TFLOP/s): 61.261
Transformer duration (in seconds): 0.1250
Transformer throughput (in TFLOP/s): 49.113
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 128, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0716
Attention throughput (in TFLOP/s): 30.986
MLP duration (in seconds): 0.0680
MLP throughput (in TFLOP/s): 59.717
Transformer duration (in seconds): 0.1436
Transformer throughput (in TFLOP/s): 43.742
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0570
Attention throughput (in TFLOP/s): 39.809
MLP duration (in seconds): 0.0679
MLP throughput (in TFLOP/s): 61.197
Transformer duration (in seconds): 0.1276
Transformer throughput (in TFLOP/s): 50.344
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0733
Attention throughput (in TFLOP/s): 31.635
MLP duration (in seconds): 0.0696
MLP throughput (in TFLOP/s): 61.133
Transformer duration (in seconds): 0.1459
Transformer throughput (in TFLOP/s): 45.022
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0610
Attention throughput (in TFLOP/s): 38.805
MLP duration (in seconds): 0.0727
MLP throughput (in TFLOP/s): 59.807
Transformer duration (in seconds): 0.1372
Transformer throughput (in TFLOP/s): 48.936
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0747
Attention throughput (in TFLOP/s): 32.366
MLP duration (in seconds): 0.0743
MLP throughput (in TFLOP/s): 59.857
Transformer duration (in seconds): 0.1531
Transformer throughput (in TFLOP/s): 44.842
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0620
Attention throughput (in TFLOP/s): 39.812
MLP duration (in seconds): 0.0739
MLP throughput (in TFLOP/s): 61.518
Transformer duration (in seconds): 0.1392
Transformer throughput (in TFLOP/s): 50.384
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0764
Attention throughput (in TFLOP/s): 33.014
MLP duration (in seconds): 0.0776
MLP throughput (in TFLOP/s): 59.845
Transformer duration (in seconds): 0.1582
Transformer throughput (in TFLOP/s): 45.292
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0633
Attention throughput (in TFLOP/s): 40.664
MLP duration (in seconds): 0.0759
MLP throughput (in TFLOP/s): 62.460
Transformer duration (in seconds): 0.1429
Transformer throughput (in TFLOP/s): 51.205
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0782
Attention throughput (in TFLOP/s): 33.581
MLP duration (in seconds): 0.0797
MLP throughput (in TFLOP/s): 60.792
Transformer duration (in seconds): 0.1621
Transformer throughput (in TFLOP/s): 46.093
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0624
Attention throughput (in TFLOP/s): 42.930
MLP duration (in seconds): 0.0795
MLP throughput (in TFLOP/s): 62.211
Transformer duration (in seconds): 0.1456
Transformer throughput (in TFLOP/s): 52.378
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0813
Attention throughput (in TFLOP/s): 33.612
MLP duration (in seconds): 0.0830
MLP throughput (in TFLOP/s): 60.844
Transformer duration (in seconds): 0.1692
Transformer throughput (in TFLOP/s): 46.025
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0680
Attention throughput (in TFLOP/s): 41.014
MLP duration (in seconds): 0.0826
MLP throughput (in TFLOP/s): 62.417
Transformer duration (in seconds): 0.1541
Transformer throughput (in TFLOP/s): 51.563
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0839
Attention throughput (in TFLOP/s): 33.900
MLP duration (in seconds): 0.0865
MLP throughput (in TFLOP/s): 60.836
Transformer duration (in seconds): 0.1752
Transformer throughput (in TFLOP/s): 46.265
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0692
Attention throughput (in TFLOP/s): 41.884
MLP duration (in seconds): 0.0861
MLP throughput (in TFLOP/s): 62.381
Transformer duration (in seconds): 0.1593
Transformer throughput (in TFLOP/s): 51.915
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0864
Attention throughput (in TFLOP/s): 34.210
MLP duration (in seconds): 0.0876
MLP throughput (in TFLOP/s): 62.530
Transformer duration (in seconds): 0.1781
Transformer throughput (in TFLOP/s): 47.345
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0712
Attention throughput (in TFLOP/s): 42.296
MLP duration (in seconds): 0.0895
MLP throughput (in TFLOP/s): 62.409
Transformer duration (in seconds): 0.1644
Transformer throughput (in TFLOP/s): 52.304
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0890
Attention throughput (in TFLOP/s): 34.487
MLP duration (in seconds): 0.0914
MLP throughput (in TFLOP/s): 62.339
Transformer duration (in seconds): 0.1845
Transformer throughput (in TFLOP/s): 47.507
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0713
Attention throughput (in TFLOP/s): 43.875
MLP duration (in seconds): 0.0926
MLP throughput (in TFLOP/s): 62.675
Transformer duration (in seconds): 0.1686
Transformer throughput (in TFLOP/s): 52.982
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0916
Attention throughput (in TFLOP/s): 34.788
MLP duration (in seconds): 0.0946
MLP throughput (in TFLOP/s): 62.549
Transformer duration (in seconds): 0.1910
Transformer throughput (in TFLOP/s): 47.675
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0755
Attention throughput (in TFLOP/s): 42.958
MLP duration (in seconds): 0.0963
MLP throughput (in TFLOP/s): 62.620
Transformer duration (in seconds): 0.1764
Transformer throughput (in TFLOP/s): 52.591
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0932
Attention throughput (in TFLOP/s): 35.454
MLP duration (in seconds): 0.0983
MLP throughput (in TFLOP/s): 62.516
Transformer duration (in seconds): 0.1962
Transformer throughput (in TFLOP/s): 48.166
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0766
Attention throughput (in TFLOP/s): 43.912
MLP duration (in seconds): 0.0995
MLP throughput (in TFLOP/s): 62.956
Transformer duration (in seconds): 0.1810
Transformer throughput (in TFLOP/s): 53.169
Transformer - MLP - Attention (in seconds): 0.0050
========================================================================================================================
num_attention_heads: 128, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0948
Attention throughput (in TFLOP/s): 36.096
MLP duration (in seconds): 0.1025
MLP throughput (in TFLOP/s): 62.235
Transformer duration (in seconds): 0.2018
Transformer throughput (in TFLOP/s): 48.575
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0786
Attention throughput (in TFLOP/s): 44.356
MLP duration (in seconds): 0.1043
MLP throughput (in TFLOP/s): 62.311
Transformer duration (in seconds): 0.1875
Transformer throughput (in TFLOP/s): 53.225
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0974
Attention throughput (in TFLOP/s): 36.388
MLP duration (in seconds): 0.1058
MLP throughput (in TFLOP/s): 62.536
Transformer duration (in seconds): 0.2075
Transformer throughput (in TFLOP/s): 48.965
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0791
Attention throughput (in TFLOP/s): 45.615
MLP duration (in seconds): 0.1077
MLP throughput (in TFLOP/s): 62.525
Transformer duration (in seconds): 0.1920
Transformer throughput (in TFLOP/s): 53.863
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 128, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0996
Attention throughput (in TFLOP/s): 36.859
MLP duration (in seconds): 0.1100
MLP throughput (in TFLOP/s): 62.309
Transformer duration (in seconds): 0.2135
Transformer throughput (in TFLOP/s): 49.304
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0827
Attention throughput (in TFLOP/s): 45.133
MLP duration (in seconds): 0.1123
MLP throughput (in TFLOP/s): 62.124
Transformer duration (in seconds): 0.1998
Transformer throughput (in TFLOP/s): 53.609
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1015
Attention throughput (in TFLOP/s): 37.415
MLP duration (in seconds): 0.1139
MLP throughput (in TFLOP/s): 62.320
Transformer duration (in seconds): 0.2193
Transformer throughput (in TFLOP/s): 49.698
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0845
Attention throughput (in TFLOP/s): 45.709
MLP duration (in seconds): 0.1161
MLP throughput (in TFLOP/s): 62.223
Transformer duration (in seconds): 0.2049
Transformer throughput (in TFLOP/s): 54.106
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1029
Attention throughput (in TFLOP/s): 38.165
MLP duration (in seconds): 0.1179
MLP throughput (in TFLOP/s): 62.318
Transformer duration (in seconds): 0.2254
Transformer throughput (in TFLOP/s): 50.022
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0864
Attention throughput (in TFLOP/s): 46.207
MLP duration (in seconds): 0.1200
MLP throughput (in TFLOP/s): 62.316
Transformer duration (in seconds): 0.2108
Transformer throughput (in TFLOP/s): 54.398
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1059
Attention throughput (in TFLOP/s): 38.319
MLP duration (in seconds): 0.1222
MLP throughput (in TFLOP/s): 62.216
Transformer duration (in seconds): 0.2321
Transformer throughput (in TFLOP/s): 50.244
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0869
Attention throughput (in TFLOP/s): 47.431
MLP duration (in seconds): 0.1244
MLP throughput (in TFLOP/s): 62.132
Transformer duration (in seconds): 0.2159
Transformer throughput (in TFLOP/s): 54.900
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1078
Attention throughput (in TFLOP/s): 38.881
MLP duration (in seconds): 0.1265
MLP throughput (in TFLOP/s): 62.133
Transformer duration (in seconds): 0.2380
Transformer throughput (in TFLOP/s): 50.641
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0911
Attention throughput (in TFLOP/s): 46.746
MLP duration (in seconds): 0.1281
MLP throughput (in TFLOP/s): 62.373
Transformer duration (in seconds): 0.2241
Transformer throughput (in TFLOP/s): 54.649
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1100
Attention throughput (in TFLOP/s): 39.306
MLP duration (in seconds): 0.1311
MLP throughput (in TFLOP/s): 61.947
Transformer duration (in seconds): 0.2439
Transformer throughput (in TFLOP/s): 51.030
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0933
Attention throughput (in TFLOP/s): 47.117
MLP duration (in seconds): 0.1328
MLP throughput (in TFLOP/s): 62.138
Transformer duration (in seconds): 0.2299
Transformer throughput (in TFLOP/s): 55.019
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1123
Attention throughput (in TFLOP/s): 39.727
MLP duration (in seconds): 0.1348
MLP throughput (in TFLOP/s): 62.250
Transformer duration (in seconds): 0.2508
Transformer throughput (in TFLOP/s): 51.238
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0953
Attention throughput (in TFLOP/s): 47.553
MLP duration (in seconds): 0.1370
MLP throughput (in TFLOP/s): 62.207
Transformer duration (in seconds): 0.2364
Transformer throughput (in TFLOP/s): 55.231
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1147
Attention throughput (in TFLOP/s): 40.134
MLP duration (in seconds): 0.1393
MLP throughput (in TFLOP/s): 62.174
Transformer duration (in seconds): 0.2574
Transformer throughput (in TFLOP/s): 51.516
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0951
Attention throughput (in TFLOP/s): 49.153
MLP duration (in seconds): 0.1418
MLP throughput (in TFLOP/s): 62.016
Transformer duration (in seconds): 0.2417
Transformer throughput (in TFLOP/s): 55.736
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1219
Attention throughput (in TFLOP/s): 38.919
MLP duration (in seconds): 0.1437
MLP throughput (in TFLOP/s): 62.160
Transformer duration (in seconds): 0.2693
Transformer throughput (in TFLOP/s): 50.784
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1023
Attention throughput (in TFLOP/s): 47.092
MLP duration (in seconds): 0.1461
MLP throughput (in TFLOP/s): 62.087
Transformer duration (in seconds): 0.2524
Transformer throughput (in TFLOP/s): 55.019
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1250
Attention throughput (in TFLOP/s): 39.104
MLP duration (in seconds): 0.1484
MLP throughput (in TFLOP/s): 62.070
Transformer duration (in seconds): 0.2767
Transformer throughput (in TFLOP/s): 50.956
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1047
Attention throughput (in TFLOP/s): 47.389
MLP duration (in seconds): 0.1506
MLP throughput (in TFLOP/s): 62.121
Transformer duration (in seconds): 0.2592
Transformer throughput (in TFLOP/s): 55.233
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1283
Attention throughput (in TFLOP/s): 39.223
MLP duration (in seconds): 0.1531
MLP throughput (in TFLOP/s): 62.021
Transformer duration (in seconds): 0.2842
Transformer throughput (in TFLOP/s): 51.131
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1071
Attention throughput (in TFLOP/s): 47.699
MLP duration (in seconds): 0.1559
MLP throughput (in TFLOP/s): 61.827
Transformer duration (in seconds): 0.2660
Transformer throughput (in TFLOP/s): 55.450
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1312
Attention throughput (in TFLOP/s): 39.510
MLP duration (in seconds): 0.1575
MLP throughput (in TFLOP/s): 62.123
Transformer duration (in seconds): 0.2916
Transformer throughput (in TFLOP/s): 51.331
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 128, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1070
Attention throughput (in TFLOP/s): 49.119
MLP duration (in seconds): 0.1602
MLP throughput (in TFLOP/s): 62.003
Transformer duration (in seconds): 0.2709
Transformer throughput (in TFLOP/s): 56.069
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1339
Attention throughput (in TFLOP/s): 39.819
MLP duration (in seconds): 0.1626
MLP throughput (in TFLOP/s): 61.981
Transformer duration (in seconds): 0.2998
Transformer throughput (in TFLOP/s): 51.396
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1124
Attention throughput (in TFLOP/s): 48.134
MLP duration (in seconds): 0.1650
MLP throughput (in TFLOP/s): 61.958
Transformer duration (in seconds): 0.2813
Transformer throughput (in TFLOP/s): 55.563
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1372
Attention throughput (in TFLOP/s): 39.978
MLP duration (in seconds): 0.1672
MLP throughput (in TFLOP/s): 62.043
Transformer duration (in seconds): 0.3072
Transformer throughput (in TFLOP/s): 51.613
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 128, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1139
Attention throughput (in TFLOP/s): 48.819
MLP duration (in seconds): 0.1694
MLP throughput (in TFLOP/s): 62.103
Transformer duration (in seconds): 0.2878
Transformer throughput (in TFLOP/s): 55.897
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1386
Attention throughput (in TFLOP/s): 40.704
MLP duration (in seconds): 0.1722
MLP throughput (in TFLOP/s): 61.969
Transformer duration (in seconds): 0.3142
Transformer throughput (in TFLOP/s): 51.925
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1164
Attention throughput (in TFLOP/s): 49.122
MLP duration (in seconds): 0.1745
MLP throughput (in TFLOP/s): 62.037
Transformer duration (in seconds): 0.2956
Transformer throughput (in TFLOP/s): 55.974
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1418
Attention throughput (in TFLOP/s): 40.875
MLP duration (in seconds): 0.1770
MLP throughput (in TFLOP/s): 62.012
Transformer duration (in seconds): 0.3219
Transformer throughput (in TFLOP/s): 52.111
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1159
Attention throughput (in TFLOP/s): 50.696
MLP duration (in seconds): 0.1795
MLP throughput (in TFLOP/s): 62.033
Transformer duration (in seconds): 0.3003
Transformer throughput (in TFLOP/s): 56.632
Transformer - MLP - Attention (in seconds): 0.0050
========================================================================================================================
num_attention_heads: 128, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1308
Attention throughput (in TFLOP/s): 45.520
MLP duration (in seconds): 0.1818
MLP throughput (in TFLOP/s): 62.093
Transformer duration (in seconds): 0.3165
Transformer throughput (in TFLOP/s): 54.474
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1219
Attention throughput (in TFLOP/s): 49.493
MLP duration (in seconds): 0.1844
MLP throughput (in TFLOP/s): 62.062
Transformer duration (in seconds): 0.3106
Transformer throughput (in TFLOP/s): 56.285
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1332
Attention throughput (in TFLOP/s): 45.911
MLP duration (in seconds): 0.1875
MLP throughput (in TFLOP/s): 61.863
Transformer duration (in seconds): 0.3244
Transformer throughput (in TFLOP/s): 54.616
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1231
Attention throughput (in TFLOP/s): 50.327
MLP duration (in seconds): 0.1889
MLP throughput (in TFLOP/s): 62.257
Transformer duration (in seconds): 0.3168
Transformer throughput (in TFLOP/s): 56.690
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1357
Attention throughput (in TFLOP/s): 46.284
MLP duration (in seconds): 0.1915
MLP throughput (in TFLOP/s): 62.238
Transformer duration (in seconds): 0.3318
Transformer throughput (in TFLOP/s): 54.850
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1260
Attention throughput (in TFLOP/s): 50.506
MLP duration (in seconds): 0.1934
MLP throughput (in TFLOP/s): 62.450
Transformer duration (in seconds): 0.3243
Transformer throughput (in TFLOP/s): 56.870
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1380
Attention throughput (in TFLOP/s): 46.707
MLP duration (in seconds): 0.1961
MLP throughput (in TFLOP/s): 62.438
Transformer duration (in seconds): 0.3393
Transformer throughput (in TFLOP/s): 55.071
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 128, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1267
Attention throughput (in TFLOP/s): 51.520
MLP duration (in seconds): 0.1999
MLP throughput (in TFLOP/s): 62.040
Transformer duration (in seconds): 0.3304
Transformer throughput (in TFLOP/s): 57.309
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1412
Attention throughput (in TFLOP/s): 46.820
MLP duration (in seconds): 0.2030
MLP throughput (in TFLOP/s): 61.915
Transformer duration (in seconds): 0.3465
Transformer throughput (in TFLOP/s): 55.347
Transformer - MLP - Attention (in seconds): 0.0023
========================================================================================================================
num_attention_heads: 128, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1314
Attention throughput (in TFLOP/s): 50.977
MLP duration (in seconds): 0.2055
MLP throughput (in TFLOP/s): 61.969
Transformer duration (in seconds): 0.3401
Transformer throughput (in TFLOP/s): 57.132
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1419
Attention throughput (in TFLOP/s): 47.786
MLP duration (in seconds): 0.2072
MLP throughput (in TFLOP/s): 62.238
Transformer duration (in seconds): 0.3552
Transformer throughput (in TFLOP/s): 55.399
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 128, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1333
Attention throughput (in TFLOP/s): 51.536
MLP duration (in seconds): 0.2094
MLP throughput (in TFLOP/s): 62.393
Transformer duration (in seconds): 0.3475
Transformer throughput (in TFLOP/s): 57.356
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1452
Attention throughput (in TFLOP/s): 47.880
MLP duration (in seconds): 0.2133
MLP throughput (in TFLOP/s): 62.034
Transformer duration (in seconds): 0.3635
Transformer throughput (in TFLOP/s): 55.539
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1358
Attention throughput (in TFLOP/s): 51.831
MLP duration (in seconds): 0.2143
MLP throughput (in TFLOP/s): 62.542
Transformer duration (in seconds): 0.3564
Transformer throughput (in TFLOP/s): 57.362
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 128, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1484
Attention throughput (in TFLOP/s): 48.025
MLP duration (in seconds): 0.2177
MLP throughput (in TFLOP/s): 62.332
Transformer duration (in seconds): 0.3708
Transformer throughput (in TFLOP/s): 55.820
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1362
Attention throughput (in TFLOP/s): 52.970
MLP duration (in seconds): 0.2200
MLP throughput (in TFLOP/s): 62.466
Transformer duration (in seconds): 0.3617
Transformer throughput (in TFLOP/s): 57.942
Transformer - MLP - Attention (in seconds): 0.0055
========================================================================================================================
num_attention_heads: 128, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1518
Attention throughput (in TFLOP/s): 48.124
MLP duration (in seconds): 0.2864
MLP throughput (in TFLOP/s): 48.592
Transformer duration (in seconds): 0.4452
Transformer throughput (in TFLOP/s): 47.667
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 128, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1420
Attention throughput (in TFLOP/s): 52.062
MLP duration (in seconds): 0.2810
MLP throughput (in TFLOP/s): 50.133
Transformer duration (in seconds): 0.4343
Transformer throughput (in TFLOP/s): 49.461
Transformer - MLP - Attention (in seconds): 0.0113
========================================================================================================================
num_attention_heads: 128, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1546
Attention throughput (in TFLOP/s): 48.396
MLP duration (in seconds): 0.2932
MLP throughput (in TFLOP/s): 48.648
Transformer duration (in seconds): 0.4543
Transformer throughput (in TFLOP/s): 47.863
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 128, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1448
Attention throughput (in TFLOP/s): 52.296
MLP duration (in seconds): 0.2990
MLP throughput (in TFLOP/s): 48.294
Transformer duration (in seconds): 0.4532
Transformer throughput (in TFLOP/s): 48.571
Transformer - MLP - Attention (in seconds): 0.0094
========================================================================================================================
num_attention_heads: 128, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1577
Attention throughput (in TFLOP/s): 48.594
MLP duration (in seconds): 0.3091
MLP throughput (in TFLOP/s): 47.286
Transformer duration (in seconds): 0.4710
Transformer throughput (in TFLOP/s): 47.297
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1479
Attention throughput (in TFLOP/s): 52.412
MLP duration (in seconds): 0.3071
MLP throughput (in TFLOP/s): 48.172
Transformer duration (in seconds): 0.4601
Transformer throughput (in TFLOP/s): 49.005
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 128, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1608
Attention throughput (in TFLOP/s): 48.791
MLP duration (in seconds): 0.3083
MLP throughput (in TFLOP/s): 48.564
Transformer duration (in seconds): 0.4795
Transformer throughput (in TFLOP/s): 47.586
Transformer - MLP - Attention (in seconds): 0.0104
========================================================================================================================
num_attention_heads: 128, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1494
Attention throughput (in TFLOP/s): 53.115
MLP duration (in seconds): 0.3122
MLP throughput (in TFLOP/s): 48.542
Transformer duration (in seconds): 0.4688
Transformer throughput (in TFLOP/s): 49.252
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 128, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1638
Attention throughput (in TFLOP/s): 49.010
MLP duration (in seconds): 0.3172
MLP throughput (in TFLOP/s): 48.336
Transformer duration (in seconds): 0.4855
Transformer throughput (in TFLOP/s): 48.119
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1544
Attention throughput (in TFLOP/s): 52.619
MLP duration (in seconds): 0.3162
MLP throughput (in TFLOP/s): 49.070
Transformer duration (in seconds): 0.4754
Transformer throughput (in TFLOP/s): 49.720
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1664
Attention throughput (in TFLOP/s): 49.389
MLP duration (in seconds): 0.3244
MLP throughput (in TFLOP/s): 48.400
Transformer duration (in seconds): 0.5000
Transformer throughput (in TFLOP/s): 47.831
Transformer - MLP - Attention (in seconds): 0.0093
========================================================================================================================
num_attention_heads: 128, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1566
Attention throughput (in TFLOP/s): 53.064
MLP duration (in seconds): 0.3325
MLP throughput (in TFLOP/s): 47.772
Transformer duration (in seconds): 0.4937
Transformer throughput (in TFLOP/s): 49.006
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1696
Attention throughput (in TFLOP/s): 49.564
MLP duration (in seconds): 0.3332
MLP throughput (in TFLOP/s): 48.222
Transformer duration (in seconds): 0.5107
Transformer throughput (in TFLOP/s): 47.920
Transformer - MLP - Attention (in seconds): 0.0079
========================================================================================================================
num_attention_heads: 128, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1599
Attention throughput (in TFLOP/s): 53.168
MLP duration (in seconds): 0.3353
MLP throughput (in TFLOP/s): 48.470
Transformer duration (in seconds): 0.5025
Transformer throughput (in TFLOP/s): 49.263
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 128, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1731
Attention throughput (in TFLOP/s): 49.674
MLP duration (in seconds): 0.2710
MLP throughput (in TFLOP/s): 60.675
Transformer duration (in seconds): 0.4483
Transformer throughput (in TFLOP/s): 55.846
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1614
Attention throughput (in TFLOP/s): 53.850
MLP duration (in seconds): 0.2742
MLP throughput (in TFLOP/s): 60.641
Transformer duration (in seconds): 0.4408
Transformer throughput (in TFLOP/s): 57.447
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 128, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1761
Attention throughput (in TFLOP/s): 49.911
MLP duration (in seconds): 0.2770
MLP throughput (in TFLOP/s): 60.718
Transformer duration (in seconds): 0.4570
Transformer throughput (in TFLOP/s): 56.036
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1661
Attention throughput (in TFLOP/s): 53.517
MLP duration (in seconds): 0.2809
MLP throughput (in TFLOP/s): 60.549
Transformer duration (in seconds): 0.4500
Transformer throughput (in TFLOP/s): 57.552
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0386
Attention throughput (in TFLOP/s): 16.006
MLP duration (in seconds): 0.0362
MLP throughput (in TFLOP/s): 30.378
Transformer duration (in seconds): 0.0778
Transformer throughput (in TFLOP/s): 22.075
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0529
Attention throughput (in TFLOP/s): 12.045
MLP duration (in seconds): 0.0377
MLP throughput (in TFLOP/s): 30.096
Transformer duration (in seconds): 0.0946
Transformer throughput (in TFLOP/s): 18.730
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0542
Attention throughput (in TFLOP/s): 12.094
MLP duration (in seconds): 0.0389
MLP throughput (in TFLOP/s): 30.021
Transformer duration (in seconds): 0.0968
Transformer throughput (in TFLOP/s): 18.855
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0548
Attention throughput (in TFLOP/s): 12.308
MLP duration (in seconds): 0.0402
MLP throughput (in TFLOP/s): 29.968
Transformer duration (in seconds): 0.0990
Transformer throughput (in TFLOP/s): 18.984
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0564
Attention throughput (in TFLOP/s): 12.308
MLP duration (in seconds): 0.0412
MLP throughput (in TFLOP/s): 30.126
Transformer duration (in seconds): 0.1013
Transformer throughput (in TFLOP/s): 19.095
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0569
Attention throughput (in TFLOP/s): 12.524
MLP duration (in seconds): 0.0426
MLP throughput (in TFLOP/s): 30.021
Transformer duration (in seconds): 0.1035
Transformer throughput (in TFLOP/s): 19.233
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0590
Attention throughput (in TFLOP/s): 12.426
MLP duration (in seconds): 0.0427
MLP throughput (in TFLOP/s): 30.832
Transformer duration (in seconds): 0.1048
Transformer throughput (in TFLOP/s): 19.549
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0598
Attention throughput (in TFLOP/s): 12.599
MLP duration (in seconds): 0.0440
MLP throughput (in TFLOP/s): 30.732
Transformer duration (in seconds): 0.1066
Transformer throughput (in TFLOP/s): 19.755
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0449
Attention throughput (in TFLOP/s): 17.204
MLP duration (in seconds): 0.0465
MLP throughput (in TFLOP/s): 29.935
Transformer duration (in seconds): 0.0950
Transformer throughput (in TFLOP/s): 22.791
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0610
Attention throughput (in TFLOP/s): 13.014
MLP duration (in seconds): 0.0479
MLP throughput (in TFLOP/s): 29.860
Transformer duration (in seconds): 0.1132
Transformer throughput (in TFLOP/s): 19.645
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0491
Attention throughput (in TFLOP/s): 16.573
MLP duration (in seconds): 0.0500
MLP throughput (in TFLOP/s): 29.415
Transformer duration (in seconds): 0.1024
Transformer throughput (in TFLOP/s): 22.316
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0625
Attention throughput (in TFLOP/s): 13.377
MLP duration (in seconds): 0.0514
MLP throughput (in TFLOP/s): 29.385
Transformer duration (in seconds): 0.1181
Transformer throughput (in TFLOP/s): 19.857
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0504
Attention throughput (in TFLOP/s): 16.987
MLP duration (in seconds): 0.0510
MLP throughput (in TFLOP/s): 30.397
Transformer duration (in seconds): 0.1042
Transformer throughput (in TFLOP/s): 23.106
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0639
Attention throughput (in TFLOP/s): 13.740
MLP duration (in seconds): 0.0541
MLP throughput (in TFLOP/s): 29.411
Transformer duration (in seconds): 0.1220
Transformer throughput (in TFLOP/s): 20.240
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0518
Attention throughput (in TFLOP/s): 17.379
MLP duration (in seconds): 0.0538
MLP throughput (in TFLOP/s): 30.384
Transformer duration (in seconds): 0.1083
Transformer throughput (in TFLOP/s): 23.391
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0656
Attention throughput (in TFLOP/s): 14.071
MLP duration (in seconds): 0.0570
MLP throughput (in TFLOP/s): 29.402
Transformer duration (in seconds): 0.1268
Transformer throughput (in TFLOP/s): 20.494
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0510
Attention throughput (in TFLOP/s): 18.544
MLP duration (in seconds): 0.0567
MLP throughput (in TFLOP/s): 30.324
Transformer duration (in seconds): 0.1103
Transformer throughput (in TFLOP/s): 24.150
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0671
Attention throughput (in TFLOP/s): 14.430
MLP duration (in seconds): 0.0597
MLP throughput (in TFLOP/s): 29.504
Transformer duration (in seconds): 0.1309
Transformer throughput (in TFLOP/s): 20.841
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0549
Attention throughput (in TFLOP/s): 18.039
MLP duration (in seconds): 0.0612
MLP throughput (in TFLOP/s): 29.490
Transformer duration (in seconds): 0.1199
Transformer throughput (in TFLOP/s): 23.307
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0686
Attention throughput (in TFLOP/s): 14.777
MLP duration (in seconds): 0.0627
MLP throughput (in TFLOP/s): 29.484
Transformer duration (in seconds): 0.1357
Transformer throughput (in TFLOP/s): 21.104
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0563
Attention throughput (in TFLOP/s): 18.438
MLP duration (in seconds): 0.0624
MLP throughput (in TFLOP/s): 30.338
Transformer duration (in seconds): 0.1213
Transformer throughput (in TFLOP/s): 24.172
Transformer - MLP - Attention (in seconds): 0.0026
========================================================================================================================
num_attention_heads: 128, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0700
Attention throughput (in TFLOP/s): 15.165
MLP duration (in seconds): 0.0650
MLP throughput (in TFLOP/s): 29.835
Transformer duration (in seconds): 0.1388
Transformer throughput (in TFLOP/s): 21.613
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0570
Attention throughput (in TFLOP/s): 19.052
MLP duration (in seconds): 0.0641
MLP throughput (in TFLOP/s): 30.957
Transformer duration (in seconds): 0.1245
Transformer throughput (in TFLOP/s): 24.667
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0708
Attention throughput (in TFLOP/s): 15.670
MLP duration (in seconds): 0.0681
MLP throughput (in TFLOP/s): 29.851
Transformer duration (in seconds): 0.1431
Transformer throughput (in TFLOP/s): 21.956
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0567
Attention throughput (in TFLOP/s): 20.002
MLP duration (in seconds): 0.0675
MLP throughput (in TFLOP/s): 30.793
Transformer duration (in seconds): 0.1276
Transformer throughput (in TFLOP/s): 25.183
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0729
Attention throughput (in TFLOP/s): 15.891
MLP duration (in seconds): 0.0695
MLP throughput (in TFLOP/s): 30.595
Transformer duration (in seconds): 0.1462
Transformer throughput (in TFLOP/s): 22.471
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0610
Attention throughput (in TFLOP/s): 19.402
MLP duration (in seconds): 0.0729
MLP throughput (in TFLOP/s): 29.831
Transformer duration (in seconds): 0.1366
Transformer throughput (in TFLOP/s): 24.579
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0740
Attention throughput (in TFLOP/s): 16.331
MLP duration (in seconds): 0.0741
MLP throughput (in TFLOP/s): 29.980
Transformer duration (in seconds): 0.1521
Transformer throughput (in TFLOP/s): 22.557
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0615
Attention throughput (in TFLOP/s): 20.075
MLP duration (in seconds): 0.0733
MLP throughput (in TFLOP/s): 30.981
Transformer duration (in seconds): 0.1384
Transformer throughput (in TFLOP/s): 25.331
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0759
Attention throughput (in TFLOP/s): 16.603
MLP duration (in seconds): 0.0776
MLP throughput (in TFLOP/s): 29.934
Transformer duration (in seconds): 0.1580
Transformer throughput (in TFLOP/s): 22.668
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0630
Attention throughput (in TFLOP/s): 20.427
MLP duration (in seconds): 0.0757
MLP throughput (in TFLOP/s): 31.315
Transformer duration (in seconds): 0.1426
Transformer throughput (in TFLOP/s): 25.661
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0779
Attention throughput (in TFLOP/s): 16.858
MLP duration (in seconds): 0.0801
MLP throughput (in TFLOP/s): 30.247
Transformer duration (in seconds): 0.1622
Transformer throughput (in TFLOP/s): 23.027
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0626
Attention throughput (in TFLOP/s): 21.411
MLP duration (in seconds): 0.0800
MLP throughput (in TFLOP/s): 30.933
Transformer duration (in seconds): 0.1463
Transformer throughput (in TFLOP/s): 26.072
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0816
Attention throughput (in TFLOP/s): 16.746
MLP duration (in seconds): 0.0833
MLP throughput (in TFLOP/s): 30.307
Transformer duration (in seconds): 0.1685
Transformer throughput (in TFLOP/s): 23.099
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0674
Attention throughput (in TFLOP/s): 20.672
MLP duration (in seconds): 0.0819
MLP throughput (in TFLOP/s): 31.485
Transformer duration (in seconds): 0.1528
Transformer throughput (in TFLOP/s): 26.001
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0831
Attention throughput (in TFLOP/s): 17.099
MLP duration (in seconds): 0.0866
MLP throughput (in TFLOP/s): 30.378
Transformer duration (in seconds): 0.1743
Transformer throughput (in TFLOP/s): 23.254
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0689
Attention throughput (in TFLOP/s): 21.045
MLP duration (in seconds): 0.0853
MLP throughput (in TFLOP/s): 31.453
Transformer duration (in seconds): 0.1582
Transformer throughput (in TFLOP/s): 26.132
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0861
Attention throughput (in TFLOP/s): 17.157
MLP duration (in seconds): 0.0876
MLP throughput (in TFLOP/s): 31.266
Transformer duration (in seconds): 0.1780
Transformer throughput (in TFLOP/s): 23.689
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0710
Attention throughput (in TFLOP/s): 21.197
MLP duration (in seconds): 0.0896
MLP throughput (in TFLOP/s): 31.170
Transformer duration (in seconds): 0.1652
Transformer throughput (in TFLOP/s): 26.028
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0892
Attention throughput (in TFLOP/s): 17.204
MLP duration (in seconds): 0.0919
MLP throughput (in TFLOP/s): 30.978
Transformer duration (in seconds): 0.1850
Transformer throughput (in TFLOP/s): 23.693
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0711
Attention throughput (in TFLOP/s): 21.996
MLP duration (in seconds): 0.0923
MLP throughput (in TFLOP/s): 31.443
Transformer duration (in seconds): 0.1670
Transformer throughput (in TFLOP/s): 26.748
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0909
Attention throughput (in TFLOP/s): 17.515
MLP duration (in seconds): 0.0943
MLP throughput (in TFLOP/s): 31.370
Transformer duration (in seconds): 0.1888
Transformer throughput (in TFLOP/s): 24.112
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0749
Attention throughput (in TFLOP/s): 21.649
MLP duration (in seconds): 0.0959
MLP throughput (in TFLOP/s): 31.448
Transformer duration (in seconds): 0.1746
Transformer throughput (in TFLOP/s): 26.567
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0925
Attention throughput (in TFLOP/s): 17.857
MLP duration (in seconds): 0.0979
MLP throughput (in TFLOP/s): 31.381
Transformer duration (in seconds): 0.1948
Transformer throughput (in TFLOP/s): 24.250
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0763
Attention throughput (in TFLOP/s): 22.024
MLP duration (in seconds): 0.1000
MLP throughput (in TFLOP/s): 31.307
Transformer duration (in seconds): 0.1812
Transformer throughput (in TFLOP/s): 26.559
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0954
Attention throughput (in TFLOP/s): 17.935
MLP duration (in seconds): 0.1030
MLP throughput (in TFLOP/s): 30.966
Transformer duration (in seconds): 0.2026
Transformer throughput (in TFLOP/s): 24.190
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0790
Attention throughput (in TFLOP/s): 22.066
MLP duration (in seconds): 0.1045
MLP throughput (in TFLOP/s): 31.090
Transformer duration (in seconds): 0.1875
Transformer throughput (in TFLOP/s): 26.619
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0964
Attention throughput (in TFLOP/s): 18.393
MLP duration (in seconds): 0.1053
MLP throughput (in TFLOP/s): 31.398
Transformer duration (in seconds): 0.2059
Transformer throughput (in TFLOP/s): 24.673
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0786
Attention throughput (in TFLOP/s): 22.945
MLP duration (in seconds): 0.1070
MLP throughput (in TFLOP/s): 31.465
Transformer duration (in seconds): 0.1906
Transformer throughput (in TFLOP/s): 27.129
Transformer - MLP - Attention (in seconds): 0.0050
========================================================================================================================
num_attention_heads: 128, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0985
Attention throughput (in TFLOP/s): 18.634
MLP duration (in seconds): 0.1098
MLP throughput (in TFLOP/s): 31.228
Transformer duration (in seconds): 0.2129
Transformer throughput (in TFLOP/s): 24.719
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0826
Attention throughput (in TFLOP/s): 22.610
MLP duration (in seconds): 0.1119
MLP throughput (in TFLOP/s): 31.188
Transformer duration (in seconds): 0.1990
Transformer throughput (in TFLOP/s): 26.906
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1016
Attention throughput (in TFLOP/s): 18.683
MLP duration (in seconds): 0.1146
MLP throughput (in TFLOP/s): 30.978
Transformer duration (in seconds): 0.2202
Transformer throughput (in TFLOP/s): 24.741
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0847
Attention throughput (in TFLOP/s): 22.780
MLP duration (in seconds): 0.1165
MLP throughput (in TFLOP/s): 30.998
Transformer duration (in seconds): 0.2050
Transformer throughput (in TFLOP/s): 27.039
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1027
Attention throughput (in TFLOP/s): 19.114
MLP duration (in seconds): 0.1188
MLP throughput (in TFLOP/s): 30.924
Transformer duration (in seconds): 0.2263
Transformer throughput (in TFLOP/s): 24.914
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0864
Attention throughput (in TFLOP/s): 23.095
MLP duration (in seconds): 0.1209
MLP throughput (in TFLOP/s): 30.923
Transformer duration (in seconds): 0.2121
Transformer throughput (in TFLOP/s): 27.034
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1057
Attention throughput (in TFLOP/s): 19.182
MLP duration (in seconds): 0.1231
MLP throughput (in TFLOP/s): 30.891
Transformer duration (in seconds): 0.2334
Transformer throughput (in TFLOP/s): 24.975
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0872
Attention throughput (in TFLOP/s): 23.631
MLP duration (in seconds): 0.1250
MLP throughput (in TFLOP/s): 30.932
Transformer duration (in seconds): 0.2162
Transformer throughput (in TFLOP/s): 27.415
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1071
Attention throughput (in TFLOP/s): 19.566
MLP duration (in seconds): 0.1274
MLP throughput (in TFLOP/s): 30.843
Transformer duration (in seconds): 0.2388
Transformer throughput (in TFLOP/s): 25.235
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0914
Attention throughput (in TFLOP/s): 23.301
MLP duration (in seconds): 0.1292
MLP throughput (in TFLOP/s): 30.913
Transformer duration (in seconds): 0.2247
Transformer throughput (in TFLOP/s): 27.249
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1094
Attention throughput (in TFLOP/s): 19.776
MLP duration (in seconds): 0.1314
MLP throughput (in TFLOP/s): 30.899
Transformer duration (in seconds): 0.2455
Transformer throughput (in TFLOP/s): 25.351
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0930
Attention throughput (in TFLOP/s): 23.633
MLP duration (in seconds): 0.1330
MLP throughput (in TFLOP/s): 31.044
Transformer duration (in seconds): 0.2292
Transformer throughput (in TFLOP/s): 27.594
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1103
Attention throughput (in TFLOP/s): 20.229
MLP duration (in seconds): 0.1337
MLP throughput (in TFLOP/s): 31.375
Transformer duration (in seconds): 0.2493
Transformer throughput (in TFLOP/s): 25.776
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 128, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0940
Attention throughput (in TFLOP/s): 24.100
MLP duration (in seconds): 0.1357
MLP throughput (in TFLOP/s): 31.403
Transformer duration (in seconds): 0.2340
Transformer throughput (in TFLOP/s): 27.897
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1128
Attention throughput (in TFLOP/s): 20.406
MLP duration (in seconds): 0.1381
MLP throughput (in TFLOP/s): 31.344
Transformer duration (in seconds): 0.2560
Transformer throughput (in TFLOP/s): 25.903
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 128, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0946
Attention throughput (in TFLOP/s): 24.689
MLP duration (in seconds): 0.1408
MLP throughput (in TFLOP/s): 31.231
Transformer duration (in seconds): 0.2403
Transformer throughput (in TFLOP/s): 28.029
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1202
Attention throughput (in TFLOP/s): 19.740
MLP duration (in seconds): 0.1423
MLP throughput (in TFLOP/s): 31.401
Transformer duration (in seconds): 0.2675
Transformer throughput (in TFLOP/s): 25.562
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 128, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1014
Attention throughput (in TFLOP/s): 23.750
MLP duration (in seconds): 0.1447
MLP throughput (in TFLOP/s): 31.357
Transformer duration (in seconds): 0.2517
Transformer throughput (in TFLOP/s): 27.590
Transformer - MLP - Attention (in seconds): 0.0056
========================================================================================================================
num_attention_heads: 128, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1239
Attention throughput (in TFLOP/s): 19.731
MLP duration (in seconds): 0.1481
MLP throughput (in TFLOP/s): 31.105
Transformer duration (in seconds): 0.2779
Transformer throughput (in TFLOP/s): 25.372
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 128, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1042
Attention throughput (in TFLOP/s): 23.808
MLP duration (in seconds): 0.1494
MLP throughput (in TFLOP/s): 31.314
Transformer duration (in seconds): 0.2565
Transformer throughput (in TFLOP/s): 27.901
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1262
Attention throughput (in TFLOP/s): 19.937
MLP duration (in seconds): 0.1516
MLP throughput (in TFLOP/s): 31.319
Transformer duration (in seconds): 0.2828
Transformer throughput (in TFLOP/s): 25.687
Transformer - MLP - Attention (in seconds): 0.0050
========================================================================================================================
num_attention_heads: 128, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1061
Attention throughput (in TFLOP/s): 24.072
MLP duration (in seconds): 0.1548
MLP throughput (in TFLOP/s): 31.144
Transformer duration (in seconds): 0.2663
Transformer throughput (in TFLOP/s): 27.694
Transformer - MLP - Attention (in seconds): 0.0054
========================================================================================================================
num_attention_heads: 128, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1298
Attention throughput (in TFLOP/s): 19.966
MLP duration (in seconds): 0.1554
MLP throughput (in TFLOP/s): 31.491
Transformer duration (in seconds): 0.2897
Transformer throughput (in TFLOP/s): 25.831
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1065
Attention throughput (in TFLOP/s): 24.682
MLP duration (in seconds): 0.1583
MLP throughput (in TFLOP/s): 31.361
Transformer duration (in seconds): 0.2697
Transformer throughput (in TFLOP/s): 28.153
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1327
Attention throughput (in TFLOP/s): 20.089
MLP duration (in seconds): 0.1617
MLP throughput (in TFLOP/s): 31.149
Transformer duration (in seconds): 0.3002
Transformer throughput (in TFLOP/s): 25.663
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 128, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1121
Attention throughput (in TFLOP/s): 24.133
MLP duration (in seconds): 0.1648
MLP throughput (in TFLOP/s): 31.011
Transformer duration (in seconds): 0.2796
Transformer throughput (in TFLOP/s): 27.960
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1349
Attention throughput (in TFLOP/s): 20.328
MLP duration (in seconds): 0.1652
MLP throughput (in TFLOP/s): 31.392
Transformer duration (in seconds): 0.3050
Transformer throughput (in TFLOP/s): 25.993
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1133
Attention throughput (in TFLOP/s): 24.540
MLP duration (in seconds): 0.1680
MLP throughput (in TFLOP/s): 31.318
Transformer duration (in seconds): 0.2875
Transformer throughput (in TFLOP/s): 27.971
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 128, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1378
Attention throughput (in TFLOP/s): 20.470
MLP duration (in seconds): 0.1713
MLP throughput (in TFLOP/s): 31.155
Transformer duration (in seconds): 0.3146
Transformer throughput (in TFLOP/s): 25.929
Transformer - MLP - Attention (in seconds): 0.0055
========================================================================================================================
num_attention_heads: 128, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1169
Attention throughput (in TFLOP/s): 24.452
MLP duration (in seconds): 0.1752
MLP throughput (in TFLOP/s): 30.895
Transformer duration (in seconds): 0.2968
Transformer throughput (in TFLOP/s): 27.865
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1413
Attention throughput (in TFLOP/s): 20.508
MLP duration (in seconds): 0.1774
MLP throughput (in TFLOP/s): 30.935
Transformer duration (in seconds): 0.3234
Transformer throughput (in TFLOP/s): 25.935
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1167
Attention throughput (in TFLOP/s): 25.182
MLP duration (in seconds): 0.1796
MLP throughput (in TFLOP/s): 31.000
Transformer duration (in seconds): 0.3027
Transformer throughput (in TFLOP/s): 28.096
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 128, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1321
Attention throughput (in TFLOP/s): 22.539
MLP duration (in seconds): 0.1828
MLP throughput (in TFLOP/s): 30.877
Transformer duration (in seconds): 0.3196
Transformer throughput (in TFLOP/s): 26.974
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1223
Attention throughput (in TFLOP/s): 24.673
MLP duration (in seconds): 0.1839
MLP throughput (in TFLOP/s): 31.108
Transformer duration (in seconds): 0.3118
Transformer throughput (in TFLOP/s): 28.033
Transformer - MLP - Attention (in seconds): 0.0055
========================================================================================================================
num_attention_heads: 128, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1342
Attention throughput (in TFLOP/s): 22.792
MLP duration (in seconds): 0.1873
MLP throughput (in TFLOP/s): 30.977
Transformer duration (in seconds): 0.3253
Transformer throughput (in TFLOP/s): 27.231
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1239
Attention throughput (in TFLOP/s): 25.015
MLP duration (in seconds): 0.1883
MLP throughput (in TFLOP/s): 31.225
Transformer duration (in seconds): 0.3183
Transformer throughput (in TFLOP/s): 28.209
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 128, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1363
Attention throughput (in TFLOP/s): 23.031
MLP duration (in seconds): 0.1911
MLP throughput (in TFLOP/s): 31.182
Transformer duration (in seconds): 0.3324
Transformer throughput (in TFLOP/s): 27.377
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1265
Attention throughput (in TFLOP/s): 25.145
MLP duration (in seconds): 0.1921
MLP throughput (in TFLOP/s): 31.446
Transformer duration (in seconds): 0.3240
Transformer throughput (in TFLOP/s): 28.463
Transformer - MLP - Attention (in seconds): 0.0054
========================================================================================================================
num_attention_heads: 128, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1390
Attention throughput (in TFLOP/s): 23.182
MLP duration (in seconds): 0.1958
MLP throughput (in TFLOP/s): 31.263
Transformer duration (in seconds): 0.3405
Transformer throughput (in TFLOP/s): 27.440
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 128, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1273
Attention throughput (in TFLOP/s): 25.632
MLP duration (in seconds): 0.1987
MLP throughput (in TFLOP/s): 31.205
Transformer duration (in seconds): 0.3325
Transformer throughput (in TFLOP/s): 28.465
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 128, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1417
Attention throughput (in TFLOP/s): 23.329
MLP duration (in seconds): 0.2008
MLP throughput (in TFLOP/s): 31.290
Transformer duration (in seconds): 0.3487
Transformer throughput (in TFLOP/s): 27.502
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 128, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1326
Attention throughput (in TFLOP/s): 25.258
MLP duration (in seconds): 0.2035
MLP throughput (in TFLOP/s): 31.290
Transformer duration (in seconds): 0.3396
Transformer throughput (in TFLOP/s): 28.610
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1438
Attention throughput (in TFLOP/s): 23.587
MLP duration (in seconds): 0.2044
MLP throughput (in TFLOP/s): 31.552
Transformer duration (in seconds): 0.3541
Transformer throughput (in TFLOP/s): 27.788
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 128, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1346
Attention throughput (in TFLOP/s): 25.515
MLP duration (in seconds): 0.2081
MLP throughput (in TFLOP/s): 31.398
Transformer duration (in seconds): 0.3491
Transformer throughput (in TFLOP/s): 28.552
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 128, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1472
Attention throughput (in TFLOP/s): 23.623
MLP duration (in seconds): 0.2120
MLP throughput (in TFLOP/s): 31.209
Transformer duration (in seconds): 0.3649
Transformer throughput (in TFLOP/s): 27.659
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 128, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1367
Attention throughput (in TFLOP/s): 25.750
MLP duration (in seconds): 0.2125
MLP throughput (in TFLOP/s): 31.542
Transformer duration (in seconds): 0.3553
Transformer throughput (in TFLOP/s): 28.771
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 128, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1493
Attention throughput (in TFLOP/s): 23.873
MLP duration (in seconds): 0.2168
MLP throughput (in TFLOP/s): 31.300
Transformer duration (in seconds): 0.3690
Transformer throughput (in TFLOP/s): 28.048
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 128, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1367
Attention throughput (in TFLOP/s): 26.390
MLP duration (in seconds): 0.2176
MLP throughput (in TFLOP/s): 31.585
Transformer duration (in seconds): 0.3613
Transformer throughput (in TFLOP/s): 29.007
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 128, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1526
Attention throughput (in TFLOP/s): 23.934
MLP duration (in seconds): 0.2862
MLP throughput (in TFLOP/s): 24.312
Transformer duration (in seconds): 0.4469
Transformer throughput (in TFLOP/s): 23.742
Transformer - MLP - Attention (in seconds): 0.0081
========================================================================================================================
num_attention_heads: 128, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1420
Attention throughput (in TFLOP/s): 26.030
MLP duration (in seconds): 0.2828
MLP throughput (in TFLOP/s): 24.911
Transformer duration (in seconds): 0.4355
Transformer throughput (in TFLOP/s): 24.667
Transformer - MLP - Attention (in seconds): 0.0106
========================================================================================================================
num_attention_heads: 128, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1550
Attention throughput (in TFLOP/s): 24.139
MLP duration (in seconds): 0.2938
MLP throughput (in TFLOP/s): 24.279
Transformer duration (in seconds): 0.4573
Transformer throughput (in TFLOP/s): 23.777
Transformer - MLP - Attention (in seconds): 0.0086
========================================================================================================================
num_attention_heads: 128, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1455
Attention throughput (in TFLOP/s): 26.028
MLP duration (in seconds): 0.2980
MLP throughput (in TFLOP/s): 24.231
Transformer duration (in seconds): 0.4537
Transformer throughput (in TFLOP/s): 24.258
Transformer - MLP - Attention (in seconds): 0.0103
========================================================================================================================
num_attention_heads: 128, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1584
Attention throughput (in TFLOP/s): 24.182
MLP duration (in seconds): 0.3084
MLP throughput (in TFLOP/s): 23.698
Transformer duration (in seconds): 0.4736
Transformer throughput (in TFLOP/s): 23.520
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 128, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1490
Attention throughput (in TFLOP/s): 26.026
MLP duration (in seconds): 0.3073
MLP throughput (in TFLOP/s): 24.067
Transformer duration (in seconds): 0.4621
Transformer throughput (in TFLOP/s): 24.399
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 128, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1618
Attention throughput (in TFLOP/s): 24.247
MLP duration (in seconds): 0.3075
MLP throughput (in TFLOP/s): 24.348
Transformer duration (in seconds): 0.4796
Transformer throughput (in TFLOP/s): 23.786
Transformer - MLP - Attention (in seconds): 0.0104
========================================================================================================================
num_attention_heads: 128, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1506
Attention throughput (in TFLOP/s): 26.352
MLP duration (in seconds): 0.3117
MLP throughput (in TFLOP/s): 24.307
Transformer duration (in seconds): 0.4709
Transformer throughput (in TFLOP/s): 24.516
Transformer - MLP - Attention (in seconds): 0.0086
========================================================================================================================
num_attention_heads: 128, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1656
Attention throughput (in TFLOP/s): 24.246
MLP duration (in seconds): 0.3173
MLP throughput (in TFLOP/s): 24.166
Transformer duration (in seconds): 0.4862
Transformer throughput (in TFLOP/s): 24.026
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1549
Attention throughput (in TFLOP/s): 26.215
MLP duration (in seconds): 0.3150
MLP throughput (in TFLOP/s): 24.630
Transformer duration (in seconds): 0.4761
Transformer throughput (in TFLOP/s): 24.826
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 128, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1677
Attention throughput (in TFLOP/s): 24.494
MLP duration (in seconds): 0.3235
MLP throughput (in TFLOP/s): 24.265
Transformer duration (in seconds): 0.5022
Transformer throughput (in TFLOP/s): 23.809
Transformer - MLP - Attention (in seconds): 0.0110
========================================================================================================================
num_attention_heads: 128, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1567
Attention throughput (in TFLOP/s): 26.526
MLP duration (in seconds): 0.3314
MLP throughput (in TFLOP/s): 23.964
Transformer duration (in seconds): 0.4952
Transformer throughput (in TFLOP/s): 24.427
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 128, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1708
Attention throughput (in TFLOP/s): 24.609
MLP duration (in seconds): 0.3326
MLP throughput (in TFLOP/s): 24.154
Transformer duration (in seconds): 0.5113
Transformer throughput (in TFLOP/s): 23.933
Transformer - MLP - Attention (in seconds): 0.0079
========================================================================================================================
num_attention_heads: 128, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1613
Attention throughput (in TFLOP/s): 26.345
MLP duration (in seconds): 0.3343
MLP throughput (in TFLOP/s): 24.308
Transformer duration (in seconds): 0.5016
Transformer throughput (in TFLOP/s): 24.674
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 128, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1726
Attention throughput (in TFLOP/s): 24.903
MLP duration (in seconds): 0.2685
MLP throughput (in TFLOP/s): 30.619
Transformer duration (in seconds): 0.4484
Transformer throughput (in TFLOP/s): 27.919
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 128, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1619
Attention throughput (in TFLOP/s): 26.841
MLP duration (in seconds): 0.2734
MLP throughput (in TFLOP/s): 30.415
Transformer duration (in seconds): 0.4444
Transformer throughput (in TFLOP/s): 28.489
Transformer - MLP - Attention (in seconds): 0.0091
========================================================================================================================
num_attention_heads: 128, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1759
Attention throughput (in TFLOP/s): 24.982
MLP duration (in seconds): 0.2736
MLP throughput (in TFLOP/s): 30.743
Transformer duration (in seconds): 0.4555
Transformer throughput (in TFLOP/s): 28.110
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 128, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1656
Attention throughput (in TFLOP/s): 26.837
MLP duration (in seconds): 0.2781
MLP throughput (in TFLOP/s): 30.581
Transformer duration (in seconds): 0.4525
Transformer throughput (in TFLOP/s): 28.616
Transformer - MLP - Attention (in seconds): 0.0088
========================================================================================================================
